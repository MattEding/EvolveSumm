{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalized Google Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\n",
    "\\large\n",
    "\\text{NGD} ( t_k, t_l ) = \\dfrac \n",
    "    { \\text{max} \\big\\{\n",
    "        \\text{log} ( f_k ), \n",
    "        \\text{log} ( f_l )\n",
    "    \\big\\}\n",
    "    - \\text{log} ( f_{ lk } ) }\n",
    "    { \\text{log} ( n ) \n",
    "    - \\text{min} \\big\\{\n",
    "        \\text{log} ( f_k ), \n",
    "        \\text{log} ( f_l )\n",
    "    \\big\\} }\n",
    "$\n",
    "\n",
    "where:\n",
    "- $ t_k $ and $ t_l $ are terms \n",
    "- $ f_k $ is the number of sentences containing $ t_k $\n",
    "- $ f_{ kl } $ is the number of sentences containing both $ t_k $ and $ t_l $\n",
    "- $ n $ is the total number of sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\n",
    "\\large \n",
    "sim_{ \\text{NGD} } ( t_k, t_l ) = \\text{exp} \n",
    "    \\big( - \\text{NGD} ( t_k, t_l ) \\big)\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\n",
    "\\large \n",
    "sim_{ \\text{NGD} } ( S_i, S_j ) = \n",
    "\\dfrac\n",
    "    { \\sum\\limits\n",
    "        _{ \\small t_k \\in S_i } \n",
    "        \\sum\\limits\n",
    "            _{ \\small t_l \\in S_j } \n",
    "            sim_{ \\text{NGD} } ( t_k, t_l ) }\n",
    "    { m_i m_j }\n",
    "$  \n",
    "\n",
    "where:\n",
    "- $ S_i $ and $ S_j $ are sentences\n",
    "- $ m_i $ is the number of words in $ S_i $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NormalizedGoogle:\n",
    "    def __init__(self, document):\n",
    "        self.sentence_words = tuple(distinct_words(sent) for sent in tokenize.sent_tokenize(document))\n",
    "        \n",
    "    # double check scientific paper's handling of \"bad\" log values\n",
    "    def distance(self, term_k, term_l):\n",
    "        freq_k = sum(term_k in sent for sent in self.sentence_words)\n",
    "        freq_l = sum(term_l in sent for sent in self.sentence_words)\n",
    "        if not (freq_k and freq_l):\n",
    "            raise ValueError('terms must be in document')\n",
    "\n",
    "        freq_kl = sum((term_k in sent) and (term_l in sent) for sent in self.sentence_words)\n",
    "        if (freq_k > 0) and (freq_l > 0) and (freq_kl == 0):\n",
    "            return 1.0\n",
    "\n",
    "        logs_k_l = (math.log(freq_k), math.log(freq_l))\n",
    "        n = len(self.sentence_words)\n",
    "\n",
    "        numerator = max(logs_k_l) - math.log(freq_kl)\n",
    "        denominator = math.log(n) - min(logs_k_l)\n",
    "        return numerator / denominator\n",
    "    \n",
    "    def term_similarity(self, term_k, term_l):\n",
    "        dist = self.distance(term_k, term_l)\n",
    "        return math.exp(-dist)\n",
    "    \n",
    "    def sentence_similarity(self, sent_i, sent_j):\n",
    "        total = sum(self.term_similarity(term_k, term_l)\n",
    "                    for term_k, term_l in itertools.product(sent_i, sent_j))\n",
    "        return total / len(sent_i) / len(sent_j)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
