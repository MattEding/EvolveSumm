{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "import calendar\n",
    "import pickle\n",
    "import json\n",
    "import logging\n",
    "import time\n",
    "\n",
    "import bs4\n",
    "import numpy as np\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://www.npr.org/sections/news/archive'\n",
    "\n",
    "# month number (no pad) / day (no pad) / year (4 digits)\n",
    "params = {'date': '3-6-2019'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "day_resp = requests.get(url, params=params)\n",
    "day_resp.raise_for_status()\n",
    "day_soup = bs4.BeautifulSoup(day_resp.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles = day_soup.find_all('article')\n",
    "article = articles[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delay(seconds, lam=1.0):\n",
    "    \"\"\"Sleep for at least a given number of seconds plus random amount of seconds from Poisson distribution.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    seconds : float\n",
    "        Minimum delay execution for a given number of seconds.\n",
    "    lam : float\n",
    "        Expectation of interval, should be >= 0.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    None\n",
    "    \"\"\"\n",
    "    \n",
    "    time.sleep(seconds + np.random.rand() + np.random.poisson(lam=lam))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_article(article, *, seconds=3, path=None):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    article : bs4.element.Tag\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    info : dict\n",
    "        Keys -- date, summary, story, author\n",
    "    \"\"\"\n",
    "    \n",
    "    teaser = article.find(class_='teaser')\n",
    "    date, summary = teaser.text.split('\\x95')\n",
    "    \n",
    "    link = article.a['href']\n",
    "    resp = requests.get(link)\n",
    "    \n",
    "    try:\n",
    "        resp.raise_for_status()\n",
    "    except requests.exceptions.HTTPError as exc:\n",
    "        logging.exception(exc)\n",
    "        raise\n",
    "    \n",
    "    soup = bs4.BeautifulSoup(resp.text).article\n",
    "    \n",
    "    author = soup.find(class_='byline__name').text.strip()\n",
    "    paragraphs = soup.find(id='storytext').find_all('p')\n",
    "    story = '\\n\\n'.join(p.text.strip() for p in paragraphs if p.parent.get('id') == 'storytext')\n",
    "    title = soup.find(class_='storytitle').text.strip()\n",
    "    \n",
    "    info = dict(date=date, title=title, author=author, summary=summary, story=story)\n",
    "    \n",
    "    if path:\n",
    "        with open(path) as fp:\n",
    "            json.dump(info, fp)\n",
    "        logging.info(f'Saved to: {path}')\n",
    "    \n",
    "    delay(seconds)\n",
    "    return info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "extract = extract_article(article)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
