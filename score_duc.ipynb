{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import json\n",
    "import logging\n",
    "import pathlib\n",
    "\n",
    "import gensim\n",
    "from nltk import tokenize\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_distances\n",
    "from sumy.parsers.plaintext import PlaintextParser\n",
    "from sumy.summarizers.lex_rank import LexRankSummarizer\n",
    "from sumy.summarizers.text_rank import TextRankSummarizer\n",
    "from sumy.nlp.tokenizers import Tokenizer\n",
    "\n",
    "import dde\n",
    "import process_duc\n",
    "from rouge import rouge_n\n",
    "import similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "cwd = pathlib.Path.cwd()\n",
    "data = cwd / 'data'\n",
    "duc = data / 'duc'\n",
    "files = sorted(duc.iterdir())\n",
    "\n",
    "logfile = cwd / f'articles.log'\n",
    "logfile.touch()\n",
    "fmt = '{name} - {asctime} - {levelname} : {message}'\n",
    "logging.basicConfig(filename=logfile, level=logging.INFO, style='{', format=fmt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fitness(chromosome):\n",
    "    return similarity.cohesion_separation(chromosome, similarity.jaccard_similarity, doc)\n",
    "\n",
    "\n",
    "dde_scores = []\n",
    "gs_scores = []\n",
    "tr_scores = []\n",
    "lr_scores = []\n",
    "for i, file in enumerate(files):\n",
    "    np.random.seed(i)\n",
    "    logging.info(f'article {i}')\n",
    "    try:\n",
    "        abstract, original = process_duc.extract(file)\n",
    "        if len(abstract.split()) < 10 or len(original.split()) < 10:\n",
    "            logging.info(f'skipping article {i}')\n",
    "            continue\n",
    "    except StopIteration:\n",
    "        logging.info(f'skipping article {i}')\n",
    "        continue\n",
    "    \n",
    "    original = original.replace('\\n', ' ')\n",
    "    abstract = abstract.replace('\\n', ' ')\n",
    "    \n",
    "    cv = CountVectorizer(stop_words='english')\n",
    "    sents_lower = tokenize.sent_tokenize(original.lower())\n",
    "    sents_lower = (sent.split('\\n') for sent in sents_lower)\n",
    "    sents_lower = tuple(itertools.chain.from_iterable(sents_lower))\n",
    "    vec = cv.fit_transform(sents_lower)\n",
    "    doc = vec.toarray().astype(bool).astype(int)\n",
    "\n",
    "    # dde\n",
    "    best = dde.main(population_size=100, summary_length=0.1, sentence_count=len(doc), fitness=fitness, lamba_=0.5,\n",
    "                    crossover_rate=0.5, iterations=1000)\n",
    "    \n",
    "    dde_summ = dde.construct_summary(best, doc, original, sents_lower, cosine_distances, fitness)\n",
    "    dde_summ.replace('\\n', ' ')\n",
    "    dde_rouge = rouge_n(1, dde_summ, abstract), rouge_n(2, dde_summ, abstract), rouge_n(3, dde_summ, abstract)\n",
    "    dde_scores.append(dde_rouge)\n",
    "    \n",
    "    # gensim\n",
    "    gs_summ = gensim.summarization.summarize(original, ratio=0.1)\n",
    "    gs_rouge = rouge_n(1, gs_summ, abstract), rouge_n(2, gs_summ, abstract), rouge_n(3, gs_summ, abstract)\n",
    "    gs_scores.append(gs_rouge)\n",
    "    \n",
    "    # sumy\n",
    "    num_sents = 0.1 * len(doc) or 1\n",
    "    parser = PlaintextParser(original, Tokenizer('english'))\n",
    "    \n",
    "    text_rank = TextRankSummarizer()\n",
    "    tr_summ = text_rank(parser.document, num_sents)\n",
    "    tr_summ = ' '.join(str(s) for s in tr_summ)\n",
    "    tr_rouge = rouge_n(1, tr_summ, abstract), rouge_n(2, tr_summ, abstract), rouge_n(3, tr_summ, abstract)\n",
    "    tr_scores.append(tr_rouge)\n",
    "    \n",
    "    lex_rank = LexRankSummarizer()\n",
    "    lr_summ = lex_rank(parser.document, num_sents)\n",
    "    lr_summ = ' '.join(str(s) for s in lr_summ)\n",
    "    lr_rouge = rouge_n(1, lr_summ, abstract), rouge_n(2, lr_summ, abstract), rouge_n(3, lr_summ, abstract)\n",
    "    lr_scores.append(lr_rouge)\n",
    "    \n",
    "with open('dde_scores.json', 'w') as fp:\n",
    "    json.dump(dde_scores, fp)\n",
    "\n",
    "with open('gs_scores.json', 'w') as fp:\n",
    "    json.dump(gs_scores, fp)\n",
    "\n",
    "with open('tr_scores.json', 'w') as fp:\n",
    "    json.dump(tr_scores, fp)\n",
    "    \n",
    "with open('lr_scores.json', 'w') as fp:\n",
    "    json.dump(lr_scores, fp)\n",
    "\n",
    "logging.info('saved dde, gensim, textrank, lexrank scores')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('dde_scores.json') as fp:\n",
    "    dde_scores = np.array(json.load(fp))\n",
    "\n",
    "with open('gs_scores.json') as fp:\n",
    "    gs_scores = np.array(json.load(fp))\n",
    "\n",
    "with open('tr_scores.json') as fp:\n",
    "    tr_scores = np.array(json.load(fp))\n",
    "    \n",
    "with open('lr_scores.json') as fp:\n",
    "    lr_scores = np.array(json.load(fp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0.80190613, 0.64122802, 0.39380937]), 7)"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tr_scores[tr_scores == 0] = np.nan\n",
    "np.nanmean(tr_scores, 0), np.isnan(tr_scores.sum(1)).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0.76380854, 0.5868909 , 0.35080269]), 7)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_scores[lr_scores == 0] = np.nan\n",
    "np.nanmean(lr_scores, 0), np.isnan(lr_scores.sum(1)).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0.80958321, 0.65230658, 0.41725812]), 9)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs_scores[gs_scores == 0] = np.nan\n",
    "np.nanmean(gs_scores, 0), np.isnan(gs_scores.sum(1)).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0.8183164 , 0.6495861 , 0.40700852]), 0)"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dde_scores[dde_scores == 0] = np.nan\n",
    "np.nanmean(dde_scores, 0), np.isnan(dde_scores.sum(1)).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
