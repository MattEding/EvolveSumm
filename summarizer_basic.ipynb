{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import json\n",
    "import pathlib\n",
    "from pprint import pprint\n",
    "import string\n",
    "\n",
    "import gensim as gs\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from nltk import tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand_contractions(text):\n",
    "    pass\n",
    "    # *can't --> cannot\n",
    "    # shouldn't, hasn't, wouldn't --> (...)n't --> (...) not\n",
    "    # I'd, he'd, we'd --> (...)'d --> (...) would\n",
    "    # should've --> (...)'ve --> (...) have"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize(text, top, *, vectorizer=TfidfVectorizer, stop_words=None, metric=cosine_similarity):\n",
    "    sentences = tokenize.sent_tokenize(text)\n",
    "    corpus = sentences + [text]\n",
    "    vector = vectorizer(stop_words=stop_words).fit_transform(corpus)\n",
    "    \n",
    "    sent_slice = slice(0, -1)\n",
    "    text_index = -1\n",
    "    measures = metric(vector[sent_slice], vector[text_index])\n",
    "    meas_sent = zip(measures, sentences)\n",
    "    top = sorted(meas_sent, reverse=True)[:top]\n",
    "    top_sents = [s for _, s in top]\n",
    "    return top_sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def id2token(dctnry, bow):\n",
    "    id2token = {v: k for k, v in dctnry.token2id.items()}\n",
    "    return [(id2token[id], token) for id, token in bow]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "year = str(2010)\n",
    "jsons = pathlib.Path.cwd() / 'data' / 'jsons'\n",
    "json_year = jsons / year / f'{year}.json'\n",
    "\n",
    "with open(json_year) as fp:\n",
    "    dict_year = json.load(fp)[year]\n",
    "    \n",
    "article = dict_year[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Life begins to return to normal in Kyrgyzstan after two days of violent protests led the country's president to flee the capital, Bishkek. But President Kurmanbek Bakiyev has not resigned, and the self-declared interim government warns there could be more violence.\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "story = article['story']\n",
    "summary = article['summary'].strip()\n",
    "summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = story\n",
    "tokenizer = tokenize.word_tokenize\n",
    "stop_words = set(stopwords.words('english'))\n",
    "# stop_words.update(string.punctuation)\n",
    "\n",
    "def seive(word):\n",
    "    return (word not in stop_words) and (len(word) > 2) and (not word.startswith(\"'\"))\n",
    "\n",
    "summary_sentences = tokenize.sent_tokenize(summary.lower())\n",
    "summary_tokens = [[word for word in tokenizer(sent) if seive(word)] \n",
    "                  for sent in summary_sentences]\n",
    "\n",
    "story_paragraphs = story.lower().split('\\n\\n')\n",
    "story_tokens = [[word for word in tokenizer(para) if seive(word)]\n",
    "                for para in story_paragraphs]\n",
    "\n",
    "####: for some reason the tfidf only really works with a corpus of more than 1 item\n",
    "# story_paragraphs = story.lower().replace('\\n\\n', ' ')\n",
    "# story_tokens = [[word for word in tokenizer(story_paragraphs) if word not in stop_words]]\n",
    "\n",
    "story_dctnry = gs.corpora.Dictionary(story_tokens)\n",
    "corpus = [story_dctnry.doc2bow(tkn) for tkn in story_tokens]\n",
    "\n",
    "tfidf = gs.models.TfidfModel(corpus)\n",
    "corpus_tfidf = tfidf[corpus]\n",
    "\n",
    "summary_bows = [story_dctnry.doc2bow(doc) for doc in summary_tokens]\n",
    "summary_tfidf = tfidf[summary_bows]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "lsi = gs.models.LsiModel(corpus_tfidf, id2word=story_dctnry, num_topics=2)  # initialize an LSI transformation\n",
    "corpus_lsi = lsi[corpus_tfidf]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.236*\"said\" + 0.213*\"feel\" + 0.187*\"trying\" + 0.165*\"bishkek\" + 0.165*\"beginning\" + 0.150*\"kerimbayeva\" + 0.149*\"president\" + 0.149*\"saryganbayev\" + 0.146*\"worst\" + 0.146*\"optimistic\"'),\n",
       " (1,\n",
       "  '-0.285*\"feel\" + 0.222*\"among\" + 0.222*\"friday\" + 0.222*\"main\" + 0.220*\"square\" + -0.203*\"trying\" + -0.196*\"said\" + -0.196*\"confident\" + -0.196*\"calm\" + -0.179*\"optimistic\"')]"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lsi.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('bishkek', 1),\n",
       " ('capital', 1),\n",
       " ('country', 1),\n",
       " ('days', 1),\n",
       " ('flee', 1),\n",
       " ('kyrgyzstan', 1),\n",
       " ('led', 1),\n",
       " ('life', 1),\n",
       " ('normal', 1),\n",
       " ('president', 1),\n",
       " ('protests', 1),\n",
       " ('return', 1),\n",
       " ('two', 1),\n",
       " ('violent', 1)]"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id2token(story_dctnry, summary_bows[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('president', 1),\n",
       " ('bakiyev', 1),\n",
       " ('could', 1),\n",
       " ('government', 1),\n",
       " ('interim', 1),\n",
       " ('kurmanbek', 1),\n",
       " ('self-declared', 1),\n",
       " ('violence', 1),\n",
       " ('warns', 1)]"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id2token(story_dctnry, summary_bows[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
