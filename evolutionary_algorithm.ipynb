{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import itertools\n",
    "import functools\n",
    "import math\n",
    "import operator\n",
    "import string\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from nltk import tokenize\n",
    "from nltk.util import ngrams\n",
    "\n",
    "from scipy.spatial.distance import jaccard\n",
    "from scipy.special import expit as sigmoid\n",
    "\n",
    "from sklearn.metrics import jaccard_similarity_score\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pathlib\n",
    "\n",
    "cwd = pathlib.Path.cwd()\n",
    "data = cwd / 'data'\n",
    "jsons = data / 'jsons'\n",
    "json_2018 = jsons / '2018'\n",
    "\n",
    "json_2018 = list(json_2018.iterdir())[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(json_2018) as fp:\n",
    "    articles_2018 = json.load(fp)['2018']\n",
    "\n",
    "article = articles_2018[4]\n",
    "text = article['story'].lower()\n",
    "# print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evolutionary Algorithms\n",
    "- [Discrete Differential Evolution for Text Summarization](https://www.researchgate.net/publication/281662415_Discrete_Differential_Evolution_for_Text_Summarization)\n",
    "- [Evolutionary Algorithm for Extractive Text Summarization](https://www.researchgate.net/profile/Ramiz_Aliguliyev/publication/220518077_Evolutionary_Algorithm_for_Extractive_Text_Summarization/links/09e4151356fc2caab6000000.pdf)\n",
    "- [An Improved Evolutionary Algorithm for Extractive Text Summarization](https://link.springer.com/chapter/10.1007/978-3-642-36543-0_9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentence Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distinct_words(text):\n",
    "    no_punctuation = ''.join(t for t in text if t not in string.punctuation)\n",
    "    return frozenset(tokenize.word_tokenize(no_punctuation))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let document $ D $ be decomposed into a set of $ n $ sentences.  \n",
    "$\n",
    "D = \\{ S_1, S_2, ..., S_n \\}\n",
    "$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "D = document_sentences = set(tokenize.sent_tokenize(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "sents_words_df = pd.DataFrame(sents_words_arr, columns=cv.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let terms $ T $ be the set of all $ m $ distinct words in $ D $.  \n",
    "$\n",
    "T = \\{ t_1, t_2, ..., t_m \\}\n",
    "$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "T = document_distinct_words = distinct_words(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let $ S_i $ represent the set of distinct terms in sentence $ S_i $ with \n",
    "$ m_i $ distinct terms.  \n",
    "$\n",
    "S_i = \\{ t_1, t_2, ..., t_{ m_i } \\}\n",
    "$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "S = sentence_distinct_words = {distinct_words(ds) for ds in document_sentences}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Similarity Measures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Jaccard Coefficient Similarity Measure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\n",
    "\\large\n",
    "sim_{ jaccard } ( S_i, S_j ) = \n",
    "\\dfrac\n",
    "    { | S_i \\cap S_j | }\n",
    "    { | S_i \\cup S_j | }\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalized Google Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\n",
    "\\large\n",
    "\\text{NGD} ( t_k, t_l ) = \\dfrac \n",
    "    { \\text{max} \\big\\{\n",
    "        \\text{log} ( f_k ), \n",
    "        \\text{log} ( f_l )\n",
    "    \\big\\}\n",
    "    - \\text{log} ( f_{ lk } ) }\n",
    "    { \\text{log} ( n ) \n",
    "    - \\text{min} \\big\\{\n",
    "        \\text{log} ( f_k ), \n",
    "        \\text{log} ( f_l )\n",
    "    \\big\\} }\n",
    "$\n",
    "\n",
    "where:\n",
    "- $ t_k $ and $ t_l $ are terms \n",
    "- $ f_k $ is the number of sentences containing $ t_k $\n",
    "- $ f_{ kl } $ is the number of sentences containing both $ t_k $ and $ t_l $\n",
    "- $ n $ is the total number of sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\n",
    "\\large \n",
    "sim_{ \\text{NGD} } ( t_k, t_l ) = \\text{exp} \n",
    "    \\big( - \\text{NGD} ( t_k, t_l ) \\big)\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\n",
    "\\large \n",
    "sim_{ \\text{NGD} } ( S_i, S_j ) = \n",
    "\\dfrac\n",
    "    { \\sum\\limits\n",
    "        _{ \\small t_k \\in S_i } \n",
    "        \\sum\\limits\n",
    "            _{ \\small t_l \\in S_j } \n",
    "            sim_{ \\text{NGD} } ( t_k, t_l ) }\n",
    "    { m_i m_j }\n",
    "$  \n",
    "\n",
    "where:\n",
    "- $ S_i $ and $ S_j $ are sentences\n",
    "- $ m_i $ is the number of words in $ S_i $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NormalizedGoogle:\n",
    "    def __init__(self, document):\n",
    "        self.sentence_words = tuple(distinct_words(sent) for sent in tokenize.sent_tokenize(document))\n",
    "        \n",
    "    # double check scientific paper's handling of \"bad\" log values\n",
    "    def distance(self, term_k, term_l):\n",
    "        freq_k = sum(term_k in sent for sent in self.sentence_words)\n",
    "        freq_l = sum(term_l in sent for sent in self.sentence_words)\n",
    "        if not (freq_k and freq_l):\n",
    "            raise ValueError('terms must be in document')\n",
    "\n",
    "        freq_kl = sum((term_k in sent) and (term_l in sent) for sent in self.sentence_words)\n",
    "        if (freq_k > 0) and (freq_l > 0) and (freq_kl == 0):\n",
    "            return 1.0\n",
    "\n",
    "        logs_k_l = (math.log(freq_k), math.log(freq_l))\n",
    "        n = len(self.sentence_words)\n",
    "\n",
    "        numerator = max(logs_k_l) - math.log(freq_kl)\n",
    "        denominator = math.log(n) - min(logs_k_l)\n",
    "        return numerator / denominator\n",
    "    \n",
    "    def term_similarity(self, term_k, term_l):\n",
    "        dist = self.distance(term_k, term_l)\n",
    "        return math.exp(-dist)\n",
    "    \n",
    "    def sentence_similarity(self, sent_i, sent_j):\n",
    "        total = sum(self.term_similarity(term_k, term_l)\n",
    "                    for term_k, term_l in itertools.product(sent_i, sent_j))\n",
    "        return total / len(sent_i) / len(sent_j)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objective Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let $ C $ be a partition of $ D $ with $ k $ clusters.  \n",
    "$ C = \\{ C_1, C_2, ..., C_k \\} $  \n",
    "\n",
    "where:\n",
    "- $ C_p \\cap C_q = \\emptyset, \n",
    "        \\forall p \\ne q \\in \\{ 1, 2, ..., k \\} \n",
    "  $\n",
    "- $ \\bigcup\\limits\n",
    "    _{ p = 1 }\n",
    "    ^k C_p = D \n",
    "  $\n",
    "- $ C_p \\ne \\emptyset, \n",
    "        \\forall p \\in \\{ 1, 2, ..., k \\}\n",
    "  $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def verify_clusters(clusters, document):\n",
    "    disjoint = all(not cluster_i & cluster_j for cluster_i, cluster_j in itertools.combinations(clusters, r=2))\n",
    "    union = functools.reduce(operator.or_, clusters) == document\n",
    "    nonempty = all(cluster for cluster in clusters)\n",
    "    if not (disjoint and union and nonempty):\n",
    "        raise ValueError('clusters do not form a partition')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clusterize(chromosome):\n",
    "    partition = collections.defaultdict(list)\n",
    "    for i, cluster in enumerate(chromosome):\n",
    "        partition[cluster].append(i)\n",
    "    return partition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sigmoid Function\n",
    "$\n",
    "\\large\n",
    "sigm ( x ) = \n",
    "\\dfrac\n",
    "    { 1 }\n",
    "    { 1 + \\text{exp} ( -x ) }\n",
    "$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.special import expit as sigmoid\n",
    "# interesting this is 3x faster than my implimentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Intra-Cluster Similiarity (Cohesion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\n",
    "\\large\n",
    "F_1 = \n",
    "\\sum\\limits\n",
    "    _{ \\small p = 1 }\n",
    "    ^{ \\small k } \n",
    "\\sum\\limits\n",
    "    _{ \\small S_i, S_j \\in C_p } \n",
    "\\dfrac \n",
    "    { sim ( S_i, S_j ) } \n",
    "    { | C_p | } \n",
    "\\rightarrow \\text{max}\n",
    "$\n",
    "\n",
    "*(__Note:__ Evol Alg for Ext Txt Summ doesn't show division of $|C_p|$ but I believe this to be a typo since the paragraph detailing it says \"the average sum\". Additionally the Disc Diff Evol for Txt Summ shows it as such.)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inter-Cluster Dissimilarity (Separation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\n",
    "\\large\n",
    "F_2 = \n",
    "\\sum\\limits\n",
    "    _{ \\small p = 1 }\n",
    "    ^{ \\small k - 1 }\n",
    "\\sum\\limits\n",
    "    _{ \\small q = p + 1 }\n",
    "    ^{ \\small k } \n",
    "\\sum\\limits\n",
    "    _{ \\small S_i \\in C_p } \n",
    "\\sum\\limits\n",
    "    _{ \\small S_j \\in C_q } \n",
    "\\dfrac \n",
    "    { sim ( S_i, S_j ) } \n",
    "    { | C_p | \\cdot | C_q | }\n",
    "\\rightarrow \\text{min}\n",
    "$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numba\n",
    "from sklearn.metrics import jaccard_similarity_score\n",
    "\n",
    "\n",
    "cv = CountVectorizer()\n",
    "vec = cv.fit_transform(tokenize.sent_tokenize(text))\n",
    "#: convert all nonzero counts to 1;just need as like a giant set\n",
    "sents_words_arr = vec.toarray().astype(bool).astype(int)\n",
    "\n",
    "\n",
    "jaccard_similarity_score(sents_words_arr[0], sents_words_arr[1])  #: works\n",
    "\n",
    "\n",
    "chrom = np.array([0, 2, 1, 1, 3, 2, 4, 1])\n",
    "\n",
    "\n",
    "def cohesion(chromosome, sim):\n",
    "    total = 0\n",
    "    for p in np.unique(chromosome):\n",
    "        cluster_p = sents_words_arr[np.where(chromosome == p)]   #: using global sent_words_arr\n",
    "        for sent_i, sent_j in itertools.combinations(cluster_p, r=2):\n",
    "            total += sim(sent_i, sent_j) / len(cluster_p)\n",
    "    return total\n",
    "\n",
    "\n",
    "def separation(chromosome, sim):\n",
    "    total = 0\n",
    "    for p, q in itertools.combinations(np.unique(chromosome), r=2):\n",
    "        cluster_p = sents_words_arr[np.where(chromosome == p)]   #: using global sent_words_arr\n",
    "        cluster_q = sents_words_arr[np.where(chromosome == q)]   #: using global sent_words_arr\n",
    "        for sent_i, sent_j in itertools.product(cluster_p, cluster_q):\n",
    "            total += sim(sent_i, sent_j) / len(cluster_p) / len(cluster_q)\n",
    "    return total\n",
    "\n",
    "def cohesion_separation(chromosome, sim):\n",
    "    coh = cohesion(chromosome, sim)\n",
    "    sep = separation(chromosome, sim)\n",
    "    return pow(1 + sigmoid(coh), sep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1848953118802626"
      ]
     },
     "execution_count": 328,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chrom = np.array([0, 2, 1, 1, 3, 2, 4, 1])\n",
    "\n",
    "@numba.jit(nopython=True)\n",
    "def combinations(a):\n",
    "    k = len(a)\n",
    "    for i in range(k-1):\n",
    "        for j in range(i+1, k):\n",
    "            yield a[i], a[j]\n",
    "\n",
    "@numba.jit(nopython=True)\n",
    "def jaccard_sim(a, b):\n",
    "    #: assume union is non-empty since each sentence >= 1 word\n",
    "    return np.sum(a & b) / np.sum(a | b)\n",
    "\n",
    "@numba.jit(nopython=True)\n",
    "def cohesion2(chromosome, sim):\n",
    "    total = 0\n",
    "    for p in np.unique(chromosome):\n",
    "        cluster_p = sents_words_arr[np.where(chromosome == p)]   #: using global sent_words_arr\n",
    "        #: cannot do itertools.combinations with numba\n",
    "        for sent_i, sent_j in combinations(cluster_p):\n",
    "            total += sim(sent_i, sent_j) / len(cluster_p)\n",
    "    return total\n",
    "\n",
    "@numba.jit(nopython=True)\n",
    "def test():\n",
    "    yield 1\n",
    "\n",
    "cohesion2(chrom, jaccard_sim)\n",
    "# jaccard_sim(a[0], b[0])\n",
    "# list(combinations('abcd')) == list(itertools.combinations('abcd', r=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37.7 µs ± 316 ns per loop (mean ± std. dev. of 7 runs, 10000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit list(combinations('abcdefghijklmn'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.46 µs ± 13.7 ns per loop (mean ± std. dev. of 7 runs, 100000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit list(itertools.combinations('abcdefghijklmn', r=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.19 µs ± 4.18 ns per loop (mean ± std. dev. of 7 runs, 1000000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit jaccard_sim(a[0], b[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "89.7 µs ± 630 ns per loop (mean ± std. dev. of 7 runs, 10000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit jaccard_similarity_score(a[0], b[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "from numba import jit\n",
    "import numpy as np\n",
    "\n",
    "x = np.arange(100).reshape(10, 10)\n",
    "\n",
    "# @jit(nopython=True) # Set \"nopython\" mode for best performance, equivalent to @njit\n",
    "@numba.njit()\n",
    "def go_fast(a): # Function is compiled to machine code when called the first time\n",
    "    trace = 0\n",
    "    sents_words_arr\n",
    "    for i in range(a.shape[0]):   # Numba likes loops\n",
    "        trace += np.tanh(a[i, i]) # Numba likes NumPy functions\n",
    "    return a + trace              # Numba likes NumPy broadcasting\n",
    "\n",
    "print(go_fast(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inter/Intra-Cluster Balance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\n",
    "\\large\n",
    "F = \\big( 1 + sigm ( F_1 ) \\big)\n",
    "    ^{ F_2 } \\rightarrow \\text{max}\n",
    "$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cohesion_separation(chromosome, sentences, sim):\n",
    "    coh = cohesion(chromosome, sentences, sim)\n",
    "    sep = separation(chromosome, sentences, sim)\n",
    "    return pow(1 + sigmoid(coh), sep)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitness"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\n",
    "\\large\n",
    "fitness_1 \\big( X_a ( t ) \\big) = F_1 \\big( X_a ( t ) \\big)\n",
    "$\n",
    "\n",
    "$\n",
    "\\large\n",
    "fitness_2 \\big( X_a ( t ) \\big) = \\dfrac{ 1 }{ F_2 ( X_a ( t ) ) }\n",
    "$\n",
    "\n",
    "$\n",
    "\\large\n",
    "fitness \\big( X_a ( t ) \\big) = F \\big( X_a ( t ) \\big)\n",
    "$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fitness_1(chromosome, sentences, sim):\n",
    "    return cohesion(chromosome, sentences, sim)\n",
    "\n",
    "def fitness_2(chromosome, sentences, sim):\n",
    "    return 1 / separation(chromosome, sentences, sim)\n",
    "\n",
    "def fitness(chromosome, sentences, sim):\n",
    "    return cohesion_separation(chromosome, sentences, sim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modified Discrete Differential Evolution Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize the population with $ N $ chromosomes each composed of $ n $ random integers from \\[1, k\\].  \n",
    "\n",
    "$\n",
    "X_r ( t ) = [ x_{ r, 1 } ( t ), x_{ r, 2 } ( t ), ..., x_{ r, n } ( t ) ]\n",
    "$  \n",
    "\n",
    "where:\n",
    "- $ x_{ r, s } ( t ) \\in \\{ 1, 2, ..., k \\} $\n",
    "- $ r = 1, 2, ..., N $\n",
    "- $ s = 1, 2, ..., n $\n",
    "- $ N $ is the population size\n",
    "- $ n $ is the number of sentences _(in the document)_\n",
    "- $ k $ is the number of clusters _(number of sentences for summary)_\n",
    "- $ t $ is the iteration step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\n",
    "\\large\n",
    "y_{ r, s } ( t + 1 ) = \n",
    "\\begin{cases}\n",
    "    x_{ r1, s } ( t ) + \\lambda \\big( x_{ r2, s } ( t ) - x_{ r3, s } ( t ) \\big) \n",
    "        & \\text{if } rand_s < \\text{CR} \\\\\n",
    "    x_{ r, s } ( t ) & \\text{otherwise}\n",
    "\\end{cases}\n",
    "$\n",
    "\n",
    "where  \n",
    "- For each $ X_r $, randomly sample $ X_{ r1 } ( t ), X_{ r2 } ( t ), X_{ r3 } ( t ) $ \n",
    "  from the same generation _(each distinct)_\n",
    "- $ rand_s $ is uniformally distributed random numbers from $ [ 0, 1 ] $ \n",
    "  chosen once for each $ s \\in \\{ 1, 2, ..., n \\} $\n",
    "\n",
    "hyper-parameters \n",
    "- $ \\lambda $ is a scale factor from $ [ 0, 1 ] $\n",
    "- $ \\text{CR} $ is the crossover rate from $ [ 0, 1 ] $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def jaccard_similarity(a, b):\n",
    "#     intersection = np.intersect1d(a, b)\n",
    "#     union = np.union1d(a, b)\n",
    "#     try:\n",
    "#         return len(intersection) / len(union)\n",
    "#     except ZeroDivisionError:\n",
    "#         return 1\n",
    "\n",
    "#TODO: vectorize this entire cell if possible and remove np.vectorizer(func) from next_generation\n",
    "def jaccard_similarity(a, b):\n",
    "    a, b = set(a), set(b)\n",
    "    try:\n",
    "        return len(a & b) / len(a | b)\n",
    "    except ZeroDivisionError:\n",
    "        return 1.0\n",
    "\n",
    "\n",
    "def clusterize(chromosome):\n",
    "    partition = collections.defaultdict(list)\n",
    "    for i, cluster in enumerate(chromosome):\n",
    "        partition[cluster].append(i)\n",
    "    return partition\n",
    "\n",
    "\n",
    "def cohesion(chromosome, sentences, sim):\n",
    "    total = 0\n",
    "    clusters = clusterize(chromosome)\n",
    "    for cluster in clusters.values():\n",
    "        for i, j in itertools.combinations(cluster, r=2):\n",
    "            sent_i, sent_j = sentences[[i,j]]\n",
    "            total += sim(sent_i, sent_j) / len(cluster)\n",
    "    return total\n",
    "\n",
    "\n",
    "def separation(chromosome, sentences, sim):\n",
    "    total = 0\n",
    "    clusters = clusterize(chromosome)\n",
    "    for cluster_p, cluster_q in itertools.combinations(clusters.values(), r=2):\n",
    "        for i, j in itertools.product(cluster_p, cluster_q):\n",
    "            sent_i, sent_j = sentences[[i,j]]\n",
    "            total += sim(sent_i, sent_j) / len(cluster_p) / len(cluster_q)\n",
    "    return total\n",
    "\n",
    "\n",
    "def cohesion_separation(chromosome, sentences, sim):\n",
    "    coh = cohesion(chromosome, sentences, sim)\n",
    "    sep = separation(chromosome, sentences, sim)\n",
    "    return pow(1 + sigmoid(coh), sep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(x))\n",
    "\n",
    "\n",
    "def get_chromosome(choices, length):\n",
    "    chrom = np.full(length, -1)\n",
    "    #: ensure that each choice is accounted for at least once\n",
    "    idxs = np.random.choice(np.arange(length), len(choices), replace=False)\n",
    "    chrom[idxs] = np.random.permutation(choices)\n",
    "    idxs = np.where(chrom == -1)[0]\n",
    "    chrom[idxs] = np.random.choice(choices, len(idxs))\n",
    "    return chrom\n",
    "\n",
    "\n",
    "def init_population(pop_size, clust_amt, chrom_len):\n",
    "    clusts = np.arange(clust_amt)\n",
    "    chroms = [get_chromosome(clusts, chrom_len) for _ in range(pop_size)]\n",
    "    pop = np.vstack(chroms)\n",
    "    return pop\n",
    "    \n",
    "\n",
    "def get_offspring(pop, rnd, lam, cr):\n",
    "    #: For computation time, relax requirement that X_r, X_r1, X_r2, X_r3 are distinct. \n",
    "    #: With large population size, this is unlikely to occur, and if it does, it doesn't\n",
    "    #: seem that detrimental. Also is this mitigated with appropriate lam choice?\n",
    "    n = len(pop)\n",
    "    idxs = np.random.choice(np.arange(n), size=(n, 3))\n",
    "    chrom_1, chrom_2, chrom_3 = map(np.squeeze, np.split(pop[idxs], 3, axis=1))\n",
    "    offspr = chrom_1 + lam * (chrom_2 - chrom_3)\n",
    "    mask = rnd < cr\n",
    "    offspr[mask] = pop[mask]\n",
    "    return offspr\n",
    "\n",
    "\n",
    "def next_generation(pop, offspr, func):\n",
    "    fit_off = np.array(list(map(func, offspr)))\n",
    "    fit_pop = np.array(list(map(func, pop)))\n",
    "    mask = fit_off > fit_pop\n",
    "    pop[mask] = offspr[mask]\n",
    "    return\n",
    "\n",
    "\n",
    "def mutate(pop, rnd):\n",
    "    mask = rnd < sigmoid(pop)\n",
    "    idxs = np.nonzero(mask)\n",
    "    rev = np.vstack(idxs).T\n",
    "    rev = np.array(sorted(rev, key=lambda x: (x[0], -x[1])))\n",
    "    rev = tuple(np.split(rev.T, 2))\n",
    "    pop[idxs] = pop[rev]\n",
    "    return\n",
    "\n",
    "\n",
    "def run_iterations(pop_size, summ_len, num_sents, func, lam, cr, iterations):\n",
    "    pop = init_population(pop_size, summ_len, num_sents)\n",
    "    shape = pop.shape\n",
    "    for i in range(iterations):\n",
    "        print(i)\n",
    "        rnd = np.random.random_sample(shape)\n",
    "        offspr = get_offspring(pop, rnd, lam, cr)\n",
    "        next_generation(pop, offspr, func)\n",
    "        mutate(pop, rnd)\n",
    "    return pop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-92-a79cd29cfc77>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m run_iterations(pop_size=100, summ_len=5, num_sents=len(document_sentences), \n\u001b[0;32m----> 7\u001b[0;31m                func=criterion, lam=0.9, cr=0.5, iterations=1000)\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# population = init_population(pop_size=100, summ_len=5, num_sents=len(document_sentences))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-91-c89866f8fdf1>\u001b[0m in \u001b[0;36mrun_iterations\u001b[0;34m(pop_size, summ_len, num_sents, func, lam, cr, iterations)\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[0mrnd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom_sample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0moffspr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_offspring\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrnd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m         \u001b[0mnext_generation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moffspr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m         \u001b[0mmutate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrnd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mpop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-91-c89866f8fdf1>\u001b[0m in \u001b[0;36mnext_generation\u001b[0;34m(pop, offspr, func)\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mnext_generation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moffspr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m     \u001b[0mfit_off\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moffspr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m     \u001b[0mfit_pop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0mmask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfit_off\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mfit_pop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-90-fda62cb71395>\u001b[0m in \u001b[0;36mcohesion_separation\u001b[0;34m(chromosome, sentences, sim)\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcohesion_separation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchromosome\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[0mcoh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcohesion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchromosome\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m     \u001b[0msep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mseparation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchromosome\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mpow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcoh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-90-fda62cb71395>\u001b[0m in \u001b[0;36mseparation\u001b[0;34m(chromosome, sentences, sim)\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mitertools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mproduct\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcluster_p\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcluster_q\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m             \u001b[0msent_i\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msent_j\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msentences\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m             \u001b[0mtotal\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0msim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msent_i\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msent_j\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcluster_p\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcluster_q\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtotal\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-90-fda62cb71395>\u001b[0m in \u001b[0;36mjaccard_similarity\u001b[0;34m(a, b)\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m \u001b[0;34m&\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mZeroDivisionError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "scaling_factor = 0.9\n",
    "crossover_rate = 0.5\n",
    "criterion = functools.partial(cohesion_separation, sentences=np.array(list(document_sentences)), sim=jaccard_similarity)\n",
    "\n",
    "\n",
    "run_iterations(pop_size=100, summ_len=5, num_sents=len(document_sentences), \n",
    "               func=criterion, lam=0.9, cr=0.5, iterations=1000)\n",
    "\n",
    "# population = init_population(pop_size=100, summ_len=5, num_sents=len(document_sentences))\n",
    "# evolved = evolve_generations(population, criterion, scaling_factor, crossover_rate, iterations=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\n",
    "\\large\n",
    "X_r(t+1) = \n",
    "\\begin{cases}\n",
    "    Y_r ( t + 1 ) & \\text{if } f \\big( Y_r ( t + 1 ) \\big) > f \\big( X_r ( t ) \\big) \\\\\n",
    "    X_r ( t ) & \\text{otherwise}\n",
    "\\end{cases}\n",
    "$\n",
    "\n",
    "where \n",
    "- $ f ( \\cdot ) $ is the objective function to be maximized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mutation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At each iteration $ t + 1 $ for each $ X_r ( t ) $ creates\n",
    "$ m_r ( t + 1 ) = [ m_{ r, 1 } ( t ), m_{ r, 2 } ( t ), ..., m_{ r, n } ( t ) ] $.  \n",
    "For each gene, 1 indicates no mutation and 0 means mutate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\n",
    "\\large\n",
    "m_{ r, s } ( t + 1 ) = \n",
    "\\begin{cases}\n",
    "    1 & \\text{if } rand_s < sigm \\big( y_{ r, s } ( t + 1 ) \\big) \\\\\n",
    "    0 & \\text{otherwise}\n",
    "\\end{cases}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inversion Operator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Figure 1 - Psuedo-code\n",
    "<img src=\"./data/pngs/fig1_-_inversion_operator_psuedo_code.png\" alt=\"Fig 1. Inverse operator psuedo-code\" width=\"33%\" align=\"left\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Figure 2 - Example\n",
    "<img src=\"./data/pngs/fig2_-_inversion_operator_diagram.png\" alt=\"Fig 2. Inverse operator example\" width=\"33%\" align=\"left\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Iterations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ROUGE-N\n",
    "_(Recall Oriented Understudy for Gisting Evaluation)_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\n",
    "\\large\n",
    "\\text{ROUGE-N} = \n",
    "\\dfrac\n",
    "    { \\sum \\limits\n",
    "        _{ \\small S \\in Summ_{ ref }} \n",
    "        \\sum \\limits\n",
    "        _{ \\small \\text{N-gram} \\in S } \n",
    "            Count_{ \\small match } ( \\text{N-gram} ) }\n",
    "    { \\sum \\limits\n",
    "        _{ \\small S \\in Summ_{ ref } } \n",
    "        \\sum \\limits\n",
    "            _{ \\small \\text{N-gram} \\in S } \n",
    "            Count( \\text{N-gram} ) }\n",
    "$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rouge_n(n, y_pred, y_true):\n",
    "    n_gram_pred = set(ngrams(y_pred, n))\n",
    "    n_gram_true = set(ngrams(y_true, n))\n",
    "    return len(n_gram_pred & n_gram_true) / len(n_gram_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
