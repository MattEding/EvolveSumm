{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import functools\n",
    "import math\n",
    "import operator\n",
    "import string\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from nltk import tokenize\n",
    "from nltk.util import ngrams\n",
    "\n",
    "from scipy.special import expit as sigmoid\n",
    "# from scipy.stats import wasserstein_distance as earth_movers_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import contextlib\n",
    "import io\n",
    "\n",
    "with io.StringIO() as str_io, contextlib.redirect_stdout(str_io):\n",
    "    import this\n",
    "    zen = str_io.getvalue()\n",
    "\n",
    "del str_io, this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"the zen of python, by tim peters\\n\\nbeautiful is better than ugly.\\nexplicit is better than implicit.\\nsimple is better than complex.\\ncomplex is better than complicated.\\nflat is better than nested.\\nsparse is better than dense.\\nreadability counts.\\nspecial cases aren't special enough to break the rules.\\nalthough practicality beats purity.\\nerrors should never pass silently.\\nunless explicitly silenced.\\nin the face of ambiguity, refuse the temptation to guess.\\nthere should be one-- and preferably only one --obvious way to do it.\\nalthough that way may not be obvious at first unless you're dutch.\\nnow is better than never.\\nalthough never is often better than *right* now.\\nif the implementation is hard to explain, it's a bad idea.\\nif the implementation is easy to explain, it may be a good idea.\\nnamespaces are one honking great idea -- let's do more of those!\\n\""
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def distinct_words(text):\n",
    "    no_punctuation = ''.join(t for t in text if t not in string.punctuation)\n",
    "    return frozenset(tokenize.word_tokenize(no_punctuation))\n",
    "\n",
    "text = zen.lower()\n",
    "text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evolutionary Algorithms\n",
    "- [Discrete Differential Evolution for Text Summarization](https://www.researchgate.net/publication/281662415_Discrete_Differential_Evolution_for_Text_Summarization)\n",
    "- [Evolutionary Algorithm for Extractive Text Summarization](https://www.researchgate.net/profile/Ramiz_Aliguliyev/publication/220518077_Evolutionary_Algorithm_for_Extractive_Text_Summarization/links/09e4151356fc2caab6000000.pdf)\n",
    "- [An Improved Evolutionary Algorithm for Extractive Text Summarization](https://link.springer.com/chapter/10.1007/978-3-642-36543-0_9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentence Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let document $D$ be decomposed into a set of $n$ sentences.  \n",
    "$\n",
    "D = \\{S_1, S_2, ..., S_n\\}\n",
    "$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "D = document_sentences = set(tokenize.sent_tokenize(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let terms $T$ be the set of all $m$ distinct words in D.  \n",
    "$\n",
    "T = \\{t_1, t_2, ..., t_m\\}\n",
    "$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "T = document_distinct_words = distinct_words(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let $S_i$ represent the set of distinct terms in sentence $S_i$ with \n",
    "$m_i$ distinct terms.  \n",
    "$\n",
    "S_i = \\{t_1, t_2, ..., t_{m_i}\\}\n",
    "$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "S = sentence_distinct_words = {distinct_words(ds) for ds in document_sentences}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seed = None  # need reproducibility option\n",
    "\n",
    "# N = len(population)\n",
    "# n = len(sentences)\n",
    "# k = len(clusters)\n",
    "\n",
    "# r = range(N)\n",
    "# s = range(n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Similarity Measures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Jaccard Coefficient Similarity Measure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\n",
    "\\large\n",
    "sim_{jaccard}(S_i, S_j) = \\frac{|S_i \\cap S_j|}{|S_i \\cup S_j|}\n",
    "$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jaccard_similarity(a, b):\n",
    "    a, b = set(a), set(b)\n",
    "    if not a and not b:\n",
    "        return 1.0\n",
    "    return len(a & b) / len(a | b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalized Google Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\n",
    "\\large\n",
    "NGD(t_k, t_l) = \\frac{max\\{log(f_k), log(f_l)\\} - log(f_{lk})} {log(n) - min\\{log(f_k), log(f_l)\\}}\n",
    "$  \n",
    "\n",
    "where:\n",
    "- $t_k$ and $t_l$ are terms \n",
    "- $f_k$ is the number of sentences containing $t_k$\n",
    "- $f_{kl}$ is the number of sentences containing both $t_k$ and $t_l$\n",
    "- $n$ is the total number of sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\n",
    "\\large \n",
    "sim_{NGD}(t_k, t_l) = e^{-NGD(t_k, t_l)}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\n",
    "\\large \n",
    "sim_{NGD}(S_i, S_j) = \\frac{ \\sum\\limits_{t_k \\in S_i} \\sum\\limits_{t_l \\in S_j} sim_{NGD}(t_k, t_l) }{ m_i m_j }\n",
    "$  \n",
    "\n",
    "where:\n",
    "- $S_i$ and $S_j$ are sentences\n",
    "- $m_i$ is the number of words in $S_i$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NormalizedGoogle:\n",
    "    def __init__(self, document):\n",
    "        self.sentence_words = tuple(distinct_words(sent) for sent in tokenize.sent_tokenize(document))\n",
    "        \n",
    "    # double check scientific paper's handling of \"bad\" log values\n",
    "    def distance(self, term_k, term_l):\n",
    "        freq_k = sum(term_k in sent for sent in self.sentence_words)\n",
    "        freq_l = sum(term_l in sent for sent in self.sentence_words)\n",
    "        if not (freq_k and freq_l):\n",
    "            raise ValueError('terms must be in document')\n",
    "\n",
    "        freq_kl = sum((term_k in sent) and (term_l in sent) for sent in self.sentence_words)\n",
    "        if (freq_k > 0) and (freq_l > 0) and (freq_kl == 0):\n",
    "            return 1.0\n",
    "\n",
    "        logs_k_l = (math.log(freq_k), math.log(freq_l))\n",
    "        n = len(self.sentence_words)\n",
    "\n",
    "        numerator = max(logs_k_l) - math.log(freq_kl)\n",
    "        denominator = math.log(n) - min(logs_k_l)\n",
    "        return numerator / denominator\n",
    "    \n",
    "    def term_similarity(self, term_k, term_l):\n",
    "        dist = self.distance(term_k, term_l)\n",
    "        return math.exp(-dist)\n",
    "    \n",
    "    def sentence_similarity(self, sent_i, sent_j):\n",
    "        total = sum(self.term_similarity(term_k, term_l)\n",
    "                    for term_k, term_l in itertools.product(sent_i, sent_j))\n",
    "        return total / len(sent_i) / len(sent_j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_google = NormalizedGoogle(text)\n",
    "\n",
    "# if tₖ == tₗ --> 1\n",
    "assert norm_google.term_similarity('better', 'better') == 1\n",
    "\n",
    "# if (tₖ != tₗ) and (fₖ == fₗ == fₖₗ > 0) --> 1\n",
    "assert norm_google.term_similarity('explicit', 'implicit') == 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objective Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let $C$ be a partition of D with $k$ clusters.  \n",
    "$\n",
    "C = \\{C_1, C_2, ..., C_k\\}\n",
    "$  \n",
    "\n",
    "where:\n",
    "- $ C_p \\cap C_q = \\emptyset, \\forall p \\ne q \\in \\{1, 2, ..., k\\} $\n",
    "- $ \\bigcup\\limits_{p=1}^k C_p = D $\n",
    "- $ C_p \\ne \\emptyset, \\forall p \\in \\{1, 2, ..., k\\} $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C = k_clusters = {frozenset(), ...}\n",
    "\n",
    "def verify_cluster(C, D):\n",
    "    disjoint = all(not C_i & C_j for C_i, C_j in itertools.combinations(C, C))\n",
    "    union = functools.reduce(operator.or_, C) == D\n",
    "    nonempty = all(C_p for C_p in C)\n",
    "    if not (disjoint and union and nonempty):\n",
    "        raise ValueError('cluster is not a partition')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sigmoid Function\n",
    "$\n",
    "\\large\n",
    "sigm(x) = \\frac{ 1 }{ 1 + e^{-x} }\n",
    "$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.special import expit as sigmoid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Intra-Cluster Similiarity (Cohesion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\n",
    "\\large\n",
    "F_1 = \n",
    "\\sum\\limits_{p=1}^{k} \n",
    "\\sum\\limits_{S_i, S_j \\in C_p} \n",
    "\\frac {sim(S_i, S_j)} {|C_p|} \n",
    "\\rightarrow max\n",
    "$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one paper shows 1/|Cp| and another shows 1/|Cp|^2. Both that show just |Cp| are the two orig ones i found\n",
    "\n",
    "def cohesion(clusters, sim):\n",
    "    total = 0\n",
    "    for cluster in clusters:\n",
    "        for sent_i, sent_j in itertools.product(cluster, repeat=2):  # use itertools.combinations? sim is symmetric\n",
    "            total += sim(sent_i, sent_j) / len(cluster)\n",
    "    return total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inter-Cluster Dissimilarity (Separation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\n",
    "\\large\n",
    "F_2 = \n",
    "\\sum\\limits_{p=1}^{k-1}\n",
    "\\sum\\limits_{q=p+1}^{k} \n",
    "\\sum\\limits_{S_i \\in C_p} \n",
    "\\sum\\limits_{S_l=j \\in C_q} \\frac {sim(S_i, S_j)} {|C_p| \\cdot |C_q|}\n",
    "\\rightarrow min\n",
    "$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "def separation(clusters, sim):\n",
    "    total = 0\n",
    "    for cluster_p, cluster_q in itertools.combinations(clusters, r=2):  # this is Σ(p=1 to k-1) and Σ(q=p+1 to k)\n",
    "        for sent_i, sent_j in itertools.product(cluster_p, cluster_q):    # should i use itertools.combinations?\n",
    "            total += sim(sent_i, sent_j) / len(cluster_p) / len(cluster_q)\n",
    "    return total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Criterion Function\n",
    "$\n",
    "\\large\n",
    "F = (1 + sigm(F_1))^{F_2} \\rightarrow max\n",
    "$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cohesion_separation(clusters, sim):\n",
    "    coh = cohesion(clusters, sim)\n",
    "    sep = separation(clusters, sim)\n",
    "    return pow(1 + sigmoid(coh), sep)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modified Discrete Differential Evolution Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize the population with $N$ chromosomes each composed of $n$ random integers from \\[1, k\\]. __NOTE:__ $t$ is the iteration step.\n",
    "\n",
    "$\n",
    "X_r(t) = [x_{r,1}(t), x_{r,2}(t), ... x_{r,n}(t)]\n",
    "$  \n",
    "\n",
    "where:\n",
    "- $ x_{r,s}(t) \\in \\{1, 2, ..., k\\} $\n",
    "- $ r = 1, 2, ..., N $\n",
    "- $ s = 1, 2, ..., n $\n",
    "- $N$ is the population size\n",
    "- $n$ is the number of sentences (in the document)\n",
    "- $k$ is the number of clusters (number of sentences for summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2, 0, 3, 1, 2, 2, 2, 3],\n",
       "       [0, 0, 2, 2, 3, 3, 2, 1],\n",
       "       [3, 0, 1, 0, 3, 0, 2, 0],\n",
       "       [1, 2, 1, 3, 0, 2, 0, 0],\n",
       "       [0, 3, 1, 3, 2, 2, 1, 1],\n",
       "       [0, 0, 2, 0, 1, 3, 0, 0]])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def assign_clusters(k, n):  #TODO: create more robust algorithm; however this is only run once for initialization\n",
    "    X_r = []\n",
    "    while not all(i in X_r for i in range(k)):\n",
    "        X_r = np.random.choice(range(k), n)\n",
    "    return X_r\n",
    "    \n",
    "\n",
    "def initialize_population(N, k, n):\n",
    "    if k > n:\n",
    "        raise ValueError('k cannot be greater than n')\n",
    "    X = np.array([assign_clusters(k, n) for _ in range(N)])\n",
    "    return X\n",
    "\n",
    "\n",
    "population = initialize_population(6, 4, 8)\n",
    "population"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\n",
    "\\large\n",
    "y_{r, s}(t+1) = \n",
    "\\begin{cases}\n",
    "    x_{r1,s}(t) + \\lambda[x_{r2,s}(t) - x_{r3,s}(t)] & \\text{if}\\ rand_s < CR \\\\\n",
    "    x_{r,s}(t) & \\text{otherwise}\n",
    "\\end{cases}\n",
    "$\n",
    "\n",
    "where  \n",
    "- For each $X_r$, randomly sample $X_{r1}(t), X_{r2}(t), X_{r3}(t)$ from the same generation (each distinct)\n",
    "- $rand_s$ is uniformally distributed random numbers from \\[0, 1\\] chosen once for each $s \\in \\{1, 2, ..., n\\}$\n",
    "\n",
    "hyper-parameters \n",
    "- $\\lambda$ is a scale factor from \\[0, 1\\]\n",
    "- $CR$ is the crossover rate from \\[0, 1\\]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2,  0,  1,  1,  4,  2,  2,  3],\n",
       "       [ 0,  0,  2,  2,  3,  0,  2,  1],\n",
       "       [ 3,  0,  1,  0,  3,  0,  2,  0],\n",
       "       [ 1,  2,  1, -1,  0,  1,  0,  0],\n",
       "       [ 0,  3,  1,  3,  2,  2,  1,  1],\n",
       "       [ 2,  0,  2,  0,  1,  3,  0,  0]])"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#: X - population\n",
    "#: Y - offspring\n",
    "#: M - mutation\n",
    "#: X_r - chromosome\n",
    "#: X_r_s - gene\n",
    "#: (t) - current generation\n",
    "#: (t+1) - next generation\n",
    "#: f(*) - fitness function for determining whether X_r(t) or X_r(t+1) survives\n",
    "\n",
    "\n",
    "#TODO: how to handle new X_r(t+1) clusters to be in [1,...,k]?\n",
    "def generate_offspring(population, scaling_factor, crossover_rate):\n",
    "    r, s = population.shape\n",
    "    offspring = np.empty_like(population)\n",
    "    for i, chrom in enumerate(population):\n",
    "        #: must select chrom != chrom_l != chrom_m != chrom_n from population\n",
    "        choices = tuple(set(range(r)) - {i})\n",
    "        l, m, n = np.random.choice(choices, size=3, replace=False)\n",
    "        chrom_l, chrom_m, chrom_n = population[l], population[m], population[n]\n",
    "        \n",
    "        #TODO: pull out rand so it can be used in mutate()\n",
    "        rand = np.random.random_sample(s)\n",
    "        for j, (gene, gene_l, gene_m, gene_n, rand_s) in enumerate(zip(chrom, chrom_l, chrom_m, chrom_n, rand)):\n",
    "            if rand_s < crossover_rate:\n",
    "                new_gene = gene_l + scaling_factor * (gene_m - gene_n)\n",
    "            else:\n",
    "                new_gene = gene\n",
    "            offspring[i, j] = new_gene\n",
    "    return offspring\n",
    "\n",
    "offspring = generate_offspring(population, scaling_factor=0.6, crossover_rate=0.2)\n",
    "offspring"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\n",
    "\\large\n",
    "X_r(t+1) = \n",
    "\\begin{cases}\n",
    "    Y_r(t+1) & \\text{if}\\ f(Y_r(t+1)) > f(X_r(t)) \\\\\n",
    "    X_{r}(t) & \\text{otherwise}\n",
    "\\end{cases}\n",
    "$\n",
    "\n",
    "where \n",
    "- $f(\\cdot)$ is the objective function to be maximized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "def next_generation(X, Y, func):\n",
    "    next_gen = X.copy()\n",
    "    Y = offspring(X, scaling_factor, crossover_rate)\n",
    "    for i, X_r, Y_r in enumerate(zip(X, Y)):\n",
    "        if func(Y_r) > func(X_r):\n",
    "            Y[i] = Y_r\n",
    "    return next_gen\n",
    "\n",
    "next_generation(X, Y, func=fitness(...))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitness\n",
    "<img src=\"./data/pngs/fitness.png\" alt=\"inverse operator psuedo-code\" width=\"33%\" align=\"left\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fitness_1(chromosome):\n",
    "    return cohesion(chromosome)\n",
    "\n",
    "def fitness_2(chromosome):\n",
    "    return 1 / separation(chromosome)\n",
    "\n",
    "def fitness(X_a):\n",
    "    return cohesion_separation(chromosome)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mutation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At each iteration $t+1$ for each $X_r(t)$ creates\n",
    "$ m_r(t+1) = [m_{r,1}(t), m_{r,2}(t), ..., m_{r,n}(t)] $.  \n",
    "For each gene, 1 indicates no mutation and 0 means mutate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\n",
    "\\large\n",
    "m_{r,s}(t+1) = \n",
    "\\begin{cases}\n",
    "    1 & \\text{if}\\ rand_s < sigm(y_{r,s}(t+1)) \\\\\n",
    "    0 & \\text{otherwise}\n",
    "\\end{cases}\n",
    "$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mutate(Y, rand):\n",
    "    M = np.empty_like(Y)\n",
    "    for i, (y, rand_s) in enumerate(zip(Y.ravel(), rand.ravel())):\n",
    "        M.ravel()[i] = rand_s < sigmoid(y)\n",
    "\n",
    "mutate(Y, rand)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inversion Operator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Figure 1 - Psuedo-code\n",
    "<img src=\"./data/pngs/fig1_-_inversion_operator_psuedo_code.png\" alt=\"inverse operator psuedo-code\" width=\"33%\" align=\"left\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inversion_operator(X_r, m_r_p1):\n",
    "    X_r_p1 = X_r.copy()\n",
    "    S = set(i for i, mutate in enumerate(m_r_p1) if not mutate)\n",
    "    \n",
    "    while len(S):\n",
    "        s_min, s_max = min(S), max(S)\n",
    "        X_r_p1[s_min] = X_r[s_max]\n",
    "        X_r_p1[s_max] = X_r[s_min]\n",
    "        S -= {s_max, s_min}\n",
    "    \n",
    "    return X_r_p1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Figure 2 - Example\n",
    "<img src=\"./data/pngs/fig2_-_inversion_operator_diagram.png\" alt=\"inverse operator example\" width=\"33%\" align=\"left\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_r = [3, 2, 4, 2, 3, 1, 4, 1]\n",
    "m_r_p1 = [0, 1, 1, 0, 1, 0, 0, 1]\n",
    "X_r_p1 = [4, 2, 4, 1, 3, 2, 3, 1]\n",
    "\n",
    "assert inversion_operator(X_r, m_r_p1) == X_r_p1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ROUGE-N\n",
    "(Recall Oriented Understudy for Gisting Evaluation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\n",
    "\\large\n",
    "ROUGE-N = \\frac\n",
    "{ \\sum\\limits_{S \\in Summ_{ref}} \\sum\\limits_{N-gram \\in S} Count_{match}(N-gram) }\n",
    "{ \\sum\\limits_{S \\in Summ_{ref}} \\sum\\limits_{N-gram \\in S} Count(N-gram) }\n",
    "$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rouge_n(n, y_pred, y_true):\n",
    "    n_gram_pred = set(ngrams(y_pred, n))\n",
    "    n_gram_true = set(ngrams(y_true, n))\n",
    "    return len(n_gram_pred & n_gram_true) / len(n_gram_true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### testing rouge_n\n",
    "https://rare-technologies.com/text-summarization-in-python-extractive-vs-abstractive-techniques-revisited/#how_to_evaluate_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = 'a good diet must have apples and bananas'.split()\n",
    "y_pred = 'apples and bananas are must for a good diet'.split()\n",
    "\n",
    "assert rouge_n(1, y_pred, y_true) == 7 / 8\n",
    "assert rouge_n(2, y_pred, y_true) == 4 / 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "set()"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = \"however doctors learned that long inactivity did more harm than good\".split()\n",
    "b = \"patients got out of shape developed blood clots and became demoralized\".split()\n",
    "\n",
    "set(a) & set(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jaccard_similarity(a, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocess Steps:  \n",
    " - stop words\n",
    " - lemmatize"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
