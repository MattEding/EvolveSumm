{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 454,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import itertools\n",
    "import functools\n",
    "import math\n",
    "import multiprocessing\n",
    "import operator\n",
    "import string\n",
    "import random\n",
    "\n",
    "import numba\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from nltk import tokenize\n",
    "from nltk.util import ngrams\n",
    "\n",
    "from scipy.spatial.distance import jaccard\n",
    "from scipy.special import expit as sigmoid\n",
    "\n",
    "from sklearn.metrics import jaccard_similarity_score\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pathlib\n",
    "\n",
    "cwd = pathlib.Path.cwd()\n",
    "data = cwd / 'data'\n",
    "jsons = data / 'jsons'\n",
    "json_2018 = jsons / '2018'\n",
    "\n",
    "json_2018 = list(json_2018.iterdir())[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(json_2018) as fp:\n",
    "    articles_2018 = json.load(fp)['2018']\n",
    "\n",
    "article = articles_2018[4]\n",
    "text = article['story'].lower()\n",
    "# print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evolutionary Algorithms\n",
    "- [Discrete Differential Evolution for Text Summarization](https://www.researchgate.net/publication/281662415_Discrete_Differential_Evolution_for_Text_Summarization)\n",
    "- [Evolutionary Algorithm for Extractive Text Summarization](https://www.researchgate.net/profile/Ramiz_Aliguliyev/publication/220518077_Evolutionary_Algorithm_for_Extractive_Text_Summarization/links/09e4151356fc2caab6000000.pdf)\n",
    "- [An Improved Evolutionary Algorithm for Extractive Text Summarization](https://link.springer.com/chapter/10.1007/978-3-642-36543-0_9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentence Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distinct_words(text):\n",
    "    no_punctuation = ''.join(t for t in text if t not in string.punctuation)\n",
    "    return frozenset(tokenize.word_tokenize(no_punctuation))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let document $ D $ be decomposed into a set of $ n $ sentences.  \n",
    "$\n",
    "D = \\{ S_1, S_2, ..., S_n \\}\n",
    "$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "D = document_sentences = set(tokenize.sent_tokenize(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "sents_words_df = pd.DataFrame(sents_words_arr, columns=cv.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let terms $ T $ be the set of all $ m $ distinct words in $ D $.  \n",
    "$\n",
    "T = \\{ t_1, t_2, ..., t_m \\}\n",
    "$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "T = document_distinct_words = distinct_words(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let $ S_i $ represent the set of distinct terms in sentence $ S_i $ with \n",
    "$ m_i $ distinct terms.  \n",
    "$\n",
    "S_i = \\{ t_1, t_2, ..., t_{ m_i } \\}\n",
    "$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "S = sentence_distinct_words = {distinct_words(ds) for ds in document_sentences}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Similarity Measures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Jaccard Coefficient Similarity Measure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\n",
    "\\large\n",
    "sim_{ jaccard } ( S_i, S_j ) = \n",
    "\\dfrac\n",
    "    { | S_i \\cap S_j | }\n",
    "    { | S_i \\cup S_j | }\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalized Google Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\n",
    "\\large\n",
    "\\text{NGD} ( t_k, t_l ) = \\dfrac \n",
    "    { \\text{max} \\big\\{\n",
    "        \\text{log} ( f_k ), \n",
    "        \\text{log} ( f_l )\n",
    "    \\big\\}\n",
    "    - \\text{log} ( f_{ lk } ) }\n",
    "    { \\text{log} ( n ) \n",
    "    - \\text{min} \\big\\{\n",
    "        \\text{log} ( f_k ), \n",
    "        \\text{log} ( f_l )\n",
    "    \\big\\} }\n",
    "$\n",
    "\n",
    "where:\n",
    "- $ t_k $ and $ t_l $ are terms \n",
    "- $ f_k $ is the number of sentences containing $ t_k $\n",
    "- $ f_{ kl } $ is the number of sentences containing both $ t_k $ and $ t_l $\n",
    "- $ n $ is the total number of sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\n",
    "\\large \n",
    "sim_{ \\text{NGD} } ( t_k, t_l ) = \\text{exp} \n",
    "    \\big( - \\text{NGD} ( t_k, t_l ) \\big)\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\n",
    "\\large \n",
    "sim_{ \\text{NGD} } ( S_i, S_j ) = \n",
    "\\dfrac\n",
    "    { \\sum\\limits\n",
    "        _{ \\small t_k \\in S_i } \n",
    "        \\sum\\limits\n",
    "            _{ \\small t_l \\in S_j } \n",
    "            sim_{ \\text{NGD} } ( t_k, t_l ) }\n",
    "    { m_i m_j }\n",
    "$  \n",
    "\n",
    "where:\n",
    "- $ S_i $ and $ S_j $ are sentences\n",
    "- $ m_i $ is the number of words in $ S_i $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NormalizedGoogle:\n",
    "    def __init__(self, document):\n",
    "        self.sentence_words = tuple(distinct_words(sent) for sent in tokenize.sent_tokenize(document))\n",
    "        \n",
    "    # double check scientific paper's handling of \"bad\" log values\n",
    "    def distance(self, term_k, term_l):\n",
    "        freq_k = sum(term_k in sent for sent in self.sentence_words)\n",
    "        freq_l = sum(term_l in sent for sent in self.sentence_words)\n",
    "        if not (freq_k and freq_l):\n",
    "            raise ValueError('terms must be in document')\n",
    "\n",
    "        freq_kl = sum((term_k in sent) and (term_l in sent) for sent in self.sentence_words)\n",
    "        if (freq_k > 0) and (freq_l > 0) and (freq_kl == 0):\n",
    "            return 1.0\n",
    "\n",
    "        logs_k_l = (math.log(freq_k), math.log(freq_l))\n",
    "        n = len(self.sentence_words)\n",
    "\n",
    "        numerator = max(logs_k_l) - math.log(freq_kl)\n",
    "        denominator = math.log(n) - min(logs_k_l)\n",
    "        return numerator / denominator\n",
    "    \n",
    "    def term_similarity(self, term_k, term_l):\n",
    "        dist = self.distance(term_k, term_l)\n",
    "        return math.exp(-dist)\n",
    "    \n",
    "    def sentence_similarity(self, sent_i, sent_j):\n",
    "        total = sum(self.term_similarity(term_k, term_l)\n",
    "                    for term_k, term_l in itertools.product(sent_i, sent_j))\n",
    "        return total / len(sent_i) / len(sent_j)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objective Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let $ C $ be a partition of $ D $ with $ k $ clusters.  \n",
    "$ C = \\{ C_1, C_2, ..., C_k \\} $  \n",
    "\n",
    "where:\n",
    "- $ C_p \\cap C_q = \\emptyset, \n",
    "        \\forall p \\ne q \\in \\{ 1, 2, ..., k \\} \n",
    "  $\n",
    "- $ \\bigcup\\limits\n",
    "    _{ p = 1 }\n",
    "    ^k C_p = D \n",
    "  $\n",
    "- $ C_p \\ne \\emptyset, \n",
    "        \\forall p \\in \\{ 1, 2, ..., k \\}\n",
    "  $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def verify_clusters(clusters, document):\n",
    "    disjoint = all(not cluster_i & cluster_j for cluster_i, cluster_j in itertools.combinations(clusters, r=2))\n",
    "    union = functools.reduce(operator.or_, clusters) == document\n",
    "    nonempty = all(cluster for cluster in clusters)\n",
    "    if not (disjoint and union and nonempty):\n",
    "        raise ValueError('clusters do not form a partition')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clusterize(chromosome):\n",
    "    partition = collections.defaultdict(list)\n",
    "    for i, cluster in enumerate(chromosome):\n",
    "        partition[cluster].append(i)\n",
    "    return partition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sigmoid Function\n",
    "$\n",
    "\\large\n",
    "sigm ( x ) = \n",
    "\\dfrac\n",
    "    { 1 }\n",
    "    { 1 + \\text{exp} ( -x ) }\n",
    "$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.special import expit as sigmoid\n",
    "# interesting this is 3x faster than my implimentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Intra-Cluster Similiarity (Cohesion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\n",
    "\\large\n",
    "F_1 = \n",
    "\\sum\\limits\n",
    "    _{ \\small p = 1 }\n",
    "    ^{ \\small k } \n",
    "\\sum\\limits\n",
    "    _{ \\small S_i, S_j \\in C_p } \n",
    "\\dfrac \n",
    "    { sim ( S_i, S_j ) } \n",
    "    { | C_p | } \n",
    "\\rightarrow \\text{max}\n",
    "$\n",
    "\n",
    "*(__Note:__ Evol Alg for Ext Txt Summ doesn't show division of $|C_p|$ but I believe this to be a typo since the paragraph detailing it says \"the average sum\". Additionally the Disc Diff Evol for Txt Summ shows it as such.)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inter-Cluster Dissimilarity (Separation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\n",
    "\\large\n",
    "F_2 = \n",
    "\\sum\\limits\n",
    "    _{ \\small p = 1 }\n",
    "    ^{ \\small k - 1 }\n",
    "\\sum\\limits\n",
    "    _{ \\small q = p + 1 }\n",
    "    ^{ \\small k } \n",
    "\\sum\\limits\n",
    "    _{ \\small S_i \\in C_p } \n",
    "\\sum\\limits\n",
    "    _{ \\small S_j \\in C_q } \n",
    "\\dfrac \n",
    "    { sim ( S_i, S_j ) } \n",
    "    { | C_p | \\cdot | C_q | }\n",
    "\\rightarrow \\text{min}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inter/Intra-Cluster Balance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\n",
    "\\large\n",
    "F = \\big( 1 + sigm ( F_1 ) \\big)\n",
    "    ^{ F_2 } \\rightarrow \\text{max}\n",
    "$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 431,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer()\n",
    "vec = cv.fit_transform(tokenize.sent_tokenize(text))\n",
    "\n",
    "doc = vec.toarray().astype(bool).astype(int)\n",
    "chrom = np.array([0, 2, 1, 1, 3, 2, 4, 1])\n",
    "\n",
    "\n",
    "def cohesion(chromosome, sim, doc):\n",
    "    total = 0\n",
    "    for p in np.unique(chromosome):\n",
    "        cluster_p = doc[np.where(chromosome == p)]\n",
    "        for sent_i, sent_j in itertools.combinations(cluster_p, r=2):\n",
    "            total += sim(sent_i, sent_j) / len(cluster_p)\n",
    "    return total\n",
    "\n",
    "\n",
    "def separation(chromosome, sim, doc):\n",
    "    total = 0\n",
    "    for p, q in itertools.combinations(np.unique(chromosome), r=2):\n",
    "        cluster_p = doc[np.where(chromosome == p)]\n",
    "        cluster_q = doc[np.where(chromosome == q)]\n",
    "        for sent_i, sent_j in itertools.product(cluster_p, cluster_q):\n",
    "            total += sim(sent_i, sent_j) / len(cluster_p) / len(cluster_q)\n",
    "    return total\n",
    "\n",
    "def cohesion_separation(chromosome, sim, doc):\n",
    "    coh = cohesion(chromosome, sim, doc)\n",
    "    sep = separation(chromosome, sim, doc)\n",
    "    return pow(1 + sigmoid(coh), sep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 432,
   "metadata": {},
   "outputs": [],
   "source": [
    "@numba.njit\n",
    "def jaccard_sim(a, b):\n",
    "    #: assume union is non-empty since each sentence >= 1 word\n",
    "    return np.sum(a & b) / np.sum(a | b)\n",
    "\n",
    "\n",
    "@numba.njit\n",
    "def sigmoid2(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "\n",
    "@numba.njit\n",
    "def cohesion2(chromosome, sim, doc):\n",
    "    total = 0\n",
    "    for p in np.unique(chromosome):\n",
    "        sents = doc[np.where(chromosome == p)]\n",
    "        k = len(sents)\n",
    "        #: combinations\n",
    "        for i in range(k-1):\n",
    "            for j in range(i+1, k):\n",
    "                total += sim(sents[i], sents[j]) / len(sents)  \n",
    "    return total\n",
    "\n",
    "\n",
    "\n",
    "@numba.njit\n",
    "def separation2(chromosome, sim, doc):\n",
    "    total = 0\n",
    "    k = len(np.unique(chromosome))\n",
    "    #: combinations\n",
    "    for p in range(k-1):\n",
    "        for q in range(p+1, k):\n",
    "            sents_p = doc[np.where(chromosome == p)]\n",
    "            sents_q = doc[np.where(chromosome == q)]\n",
    "            #: product\n",
    "            m, n = len(sents_p), len(sents_q)\n",
    "            for i in range(m):\n",
    "                for j in range(n):\n",
    "                    total += sim(sents_p[i], sents_q[j]) / m / n\n",
    "    return total\n",
    "\n",
    "\n",
    "@numba.njit\n",
    "def cohesion_separation2(chromosome, sim, doc):\n",
    "    coh = cohesion2(chromosome, sim, doc)\n",
    "    sep = separation2(chromosome, sim, doc)\n",
    "    return (1 + sigmoid2(coh)) ** sep\n",
    "\n",
    "\n",
    "\n",
    "cv = CountVectorizer()\n",
    "vec = cv.fit_transform(tokenize.sent_tokenize(text))\n",
    "\n",
    "doc = vec.toarray().astype(bool).astype(int)\n",
    "chrom = np.array([0, 2, 1, 1, 3, 2, 4, 1])\n",
    "\n",
    "assert cohesion2(chrom, jaccard_sim, doc) == cohesion(chrom, jaccard_sim, doc)\n",
    "assert separation2(chrom, jaccard_sim, doc) == separation(chrom, jaccard_sim, doc)\n",
    "assert cohesion_separation2(chrom, jaccard_sim, doc) == cohesion_separation(chrom, jaccard_sim, doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitness"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\n",
    "\\large\n",
    "fitness_1 \\big( X_a ( t ) \\big) = F_1 \\big( X_a ( t ) \\big)\n",
    "$\n",
    "\n",
    "$\n",
    "\\large\n",
    "fitness_2 \\big( X_a ( t ) \\big) = \\dfrac{ 1 }{ F_2 ( X_a ( t ) ) }\n",
    "$\n",
    "\n",
    "$\n",
    "\\large\n",
    "fitness \\big( X_a ( t ) \\big) = F \\big( X_a ( t ) \\big)\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modified Discrete Differential Evolution Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize the population with $ N $ chromosomes each composed of $ n $ random integers from \\[1, k\\].  \n",
    "\n",
    "$\n",
    "X_r ( t ) = [ x_{ r, 1 } ( t ), x_{ r, 2 } ( t ), ..., x_{ r, n } ( t ) ]\n",
    "$  \n",
    "\n",
    "where:\n",
    "- $ x_{ r, s } ( t ) \\in \\{ 1, 2, ..., k \\} $\n",
    "- $ r = 1, 2, ..., N $\n",
    "- $ s = 1, 2, ..., n $\n",
    "- $ N $ is the population size\n",
    "- $ n $ is the number of sentences _(in the document)_\n",
    "- $ k $ is the number of clusters _(number of sentences for summary)_\n",
    "- $ t $ is the iteration step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\n",
    "\\large\n",
    "y_{ r, s } ( t + 1 ) = \n",
    "\\begin{cases}\n",
    "    x_{ r1, s } ( t ) + \\lambda \\big( x_{ r2, s } ( t ) - x_{ r3, s } ( t ) \\big) \n",
    "        & \\text{if } rand_s < \\text{CR} \\\\\n",
    "    x_{ r, s } ( t ) & \\text{otherwise}\n",
    "\\end{cases}\n",
    "$\n",
    "\n",
    "where  \n",
    "- For each $ X_r $, randomly sample $ X_{ r1 } ( t ), X_{ r2 } ( t ), X_{ r3 } ( t ) $ \n",
    "  from the same generation _(each distinct)_\n",
    "- $ rand_s $ is uniformally distributed random numbers from $ [ 0, 1 ] $ \n",
    "  chosen once for each $ s \\in \\{ 1, 2, ..., n \\} $\n",
    "\n",
    "hyper-parameters \n",
    "- $ \\lambda $ is a scale factor from $ [ 0, 1 ] $\n",
    "- $ \\text{CR} $ is the crossover rate from $ [ 0, 1 ] $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 529,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_chromosome(choices, length):\n",
    "    chrom = np.full(length, -1)\n",
    "    #: ensure that each choice is accounted for at least once\n",
    "    idxs = np.random.choice(np.arange(length), len(choices), replace=False)\n",
    "    chrom[idxs] = np.random.permutation(choices)\n",
    "    idxs = np.where(chrom == -1)[0]\n",
    "    chrom[idxs] = np.random.choice(choices, len(idxs))\n",
    "    return chrom\n",
    "\n",
    "\n",
    "def init_population(pop_size, clust_amt, chrom_len):\n",
    "    clusts = np.arange(clust_amt)\n",
    "    chroms = [init_chromosome(clusts, chrom_len) for _ in range(pop_size)]\n",
    "    pop = np.vstack(chroms)\n",
    "    return pop\n",
    "    \n",
    "\n",
    "def get_offspring(pop, rnd, lam, cr):\n",
    "    #: For computation time, relax requirement that X_r, X_r1, X_r2, X_r3 are distinct. \n",
    "    #: With large population size, this is unlikely to occur, and if it does, it doesn't\n",
    "    #: seem that detrimental. Also is this mitigated with appropriate lam choice?\n",
    "    n = len(pop)\n",
    "    idxs = np.random.choice(np.arange(n), size=(n, 3))\n",
    "    chrom_1, chrom_2, chrom_3 = map(np.squeeze, np.split(pop[idxs], 3, axis=1))  # numba != map, split, squeeze\n",
    "    k = len(np.unique(pop))\n",
    "    offspr = (chrom_1 + lam * (chrom_2 - chrom_3)) % k\n",
    "    mask = rnd < cr\n",
    "    offspr[mask] = pop[mask]\n",
    "    return offspr\n",
    "\n",
    "\n",
    "@numba.jit(nopython=True)#, parallel=True)\n",
    "def next_generation(pop, offspr, func):\n",
    "    rnge = range(len(offspr))\n",
    "    fit_off = np.array([func(offspr[i]) for i in rnge])\n",
    "    fit_pop = np.array([func(pop[i]) for i in rnge])\n",
    "#     fit_off = np.array([func(chrom) for chrom in offspr])\n",
    "#     fit_pop = np.array([func(chrom) for chrom in pop])\n",
    "    mask = fit_off > fit_pop\n",
    "    pop[mask] = offspr[mask]\n",
    "    return\n",
    "\n",
    "\n",
    "def mutate(pop, rnd):\n",
    "    mask = rnd < sigmoid(pop)\n",
    "    idxs = np.nonzero(mask)\n",
    "    rev = np.vstack(idxs).T\n",
    "    rev = np.array(sorted(rev, key=lambda x: (x[0], -x[1]))) # numba != sorted key\n",
    "    rev = tuple(np.split(rev.T, 2))  # numba != split or tuple\n",
    "    pop[idxs] = pop[rev]\n",
    "    return\n",
    "\n",
    "\n",
    "#TODO: early stopping --> little fitness improvement over x generations, good enough fitness score\n",
    "def run_iterations(pop_size, summ_len, num_sents, func, lam, cr, iterations, \n",
    "                   random_state=None, verbose=False):\n",
    "    \n",
    "    pop = init_population(pop_size, summ_len, num_sents)\n",
    "    shape = pop.shape\n",
    "    for i in range(iterations):\n",
    "        if verbose:\n",
    "            print(i)  #TODO: logfile --> iteration number, best fitness score, avg fitness score\n",
    "        \n",
    "        rnd = np.random.random_sample(shape)\n",
    "        offspr = get_offspring(pop, rnd, lam, cr)\n",
    "        next_generation(pop, offspr, func)\n",
    "        mutate(pop, rnd)\n",
    "    return pop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 530,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypingError",
     "evalue": "Failed in nopython mode pipeline (step: nopython frontend)\nInternal error at <numba.typeinfer.ArgConstraint object at 0x1a1b7a4c18>:\n--%<----------------------------------------------------------------------------\nTraceback (most recent call last):\n  File \"/Users/matteding/miniconda3/lib/python3.6/site-packages/numba/errors.py\", line 617, in new_error_context\n    yield\n  File \"/Users/matteding/miniconda3/lib/python3.6/site-packages/numba/typeinfer.py\", line 198, in __call__\n    assert ty.is_precise()\nAssertionError\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/Users/matteding/miniconda3/lib/python3.6/site-packages/numba/typeinfer.py\", line 141, in propagate\n    constraint(typeinfer)\n  File \"/Users/matteding/miniconda3/lib/python3.6/site-packages/numba/typeinfer.py\", line 199, in __call__\n    typeinfer.add_type(self.dst, ty, loc=self.loc)\n  File \"/Users/matteding/miniconda3/lib/python3.6/contextlib.py\", line 99, in __exit__\n    self.gen.throw(type, value, traceback)\n  File \"/Users/matteding/miniconda3/lib/python3.6/site-packages/numba/errors.py\", line 625, in new_error_context\n    six.reraise(type(newerr), newerr, tb)\n  File \"/Users/matteding/miniconda3/lib/python3.6/site-packages/numba/six.py\", line 659, in reraise\n    raise value\nnumba.errors.InternalError: \n[1] During: typing of argument at <ipython-input-529-e7a0d4200752> (34)\n--%<----------------------------------------------------------------------------\n\n\nFile \"<ipython-input-529-e7a0d4200752>\", line 34:\ndef next_generation(pop, offspr, func):\n    rnge = range(len(offspr))\n    ^\n\nThis error may have been caused by the following argument(s):\n- argument 2: cannot determine Numba type of <class 'function'>\n\nThis is not usually a problem with Numba itself but instead often caused by\nthe use of unsupported features or an issue in resolving types.\n\nTo see Python/NumPy features supported by the latest release of Numba visit:\nhttp://numba.pydata.org/numba-doc/dev/reference/pysupported.html\nand\nhttp://numba.pydata.org/numba-doc/dev/reference/numpysupported.html\n\nFor more information about typing errors and how to debug them visit:\nhttp://numba.pydata.org/numba-doc/latest/user/troubleshoot.html#my-code-doesn-t-compile\n\nIf you think your code should work with Numba, please report the error message\nand traceback, along with a minimal reproducer at:\nhttps://github.com/numba/numba/issues/new\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypingError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-530-a12178ea05b1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mt0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m result = run_iterations(pop_size=100, summ_len=5, num_sents=len(doc), \n\u001b[0;32m---> 12\u001b[0;31m                         func=fitness, lam=0.9, cr=0.5, iterations=50, verbose=False)\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0mt1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt1\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mt0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-529-e7a0d4200752>\u001b[0m in \u001b[0;36mrun_iterations\u001b[0;34m(pop_size, summ_len, num_sents, func, lam, cr, iterations, random_state, verbose)\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0mrnd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom_sample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m         \u001b[0moffspr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_offspring\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrnd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m         \u001b[0mnext_generation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moffspr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m         \u001b[0mmutate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrnd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mpop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/numba/dispatcher.py\u001b[0m in \u001b[0;36m_compile_for_args\u001b[0;34m(self, *args, **kws)\u001b[0m\n\u001b[1;32m    346\u001b[0m                 \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpatch_message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 348\u001b[0;31m             \u001b[0merror_rewrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'typing'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    349\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUnsupportedError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    350\u001b[0m             \u001b[0;31m# Something unsupported is present in the user code, add help info\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/numba/dispatcher.py\u001b[0m in \u001b[0;36merror_rewrite\u001b[0;34m(e, issue_type)\u001b[0m\n\u001b[1;32m    313\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    314\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 315\u001b[0;31m                 \u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    316\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    317\u001b[0m         \u001b[0margtypes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/numba/six.py\u001b[0m in \u001b[0;36mreraise\u001b[0;34m(tp, value, tb)\u001b[0m\n\u001b[1;32m    656\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    657\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtb\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 658\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    659\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    660\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypingError\u001b[0m: Failed in nopython mode pipeline (step: nopython frontend)\nInternal error at <numba.typeinfer.ArgConstraint object at 0x1a1b7a4c18>:\n--%<----------------------------------------------------------------------------\nTraceback (most recent call last):\n  File \"/Users/matteding/miniconda3/lib/python3.6/site-packages/numba/errors.py\", line 617, in new_error_context\n    yield\n  File \"/Users/matteding/miniconda3/lib/python3.6/site-packages/numba/typeinfer.py\", line 198, in __call__\n    assert ty.is_precise()\nAssertionError\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/Users/matteding/miniconda3/lib/python3.6/site-packages/numba/typeinfer.py\", line 141, in propagate\n    constraint(typeinfer)\n  File \"/Users/matteding/miniconda3/lib/python3.6/site-packages/numba/typeinfer.py\", line 199, in __call__\n    typeinfer.add_type(self.dst, ty, loc=self.loc)\n  File \"/Users/matteding/miniconda3/lib/python3.6/contextlib.py\", line 99, in __exit__\n    self.gen.throw(type, value, traceback)\n  File \"/Users/matteding/miniconda3/lib/python3.6/site-packages/numba/errors.py\", line 625, in new_error_context\n    six.reraise(type(newerr), newerr, tb)\n  File \"/Users/matteding/miniconda3/lib/python3.6/site-packages/numba/six.py\", line 659, in reraise\n    raise value\nnumba.errors.InternalError: \n[1] During: typing of argument at <ipython-input-529-e7a0d4200752> (34)\n--%<----------------------------------------------------------------------------\n\n\nFile \"<ipython-input-529-e7a0d4200752>\", line 34:\ndef next_generation(pop, offspr, func):\n    rnge = range(len(offspr))\n    ^\n\nThis error may have been caused by the following argument(s):\n- argument 2: cannot determine Numba type of <class 'function'>\n\nThis is not usually a problem with Numba itself but instead often caused by\nthe use of unsupported features or an issue in resolving types.\n\nTo see Python/NumPy features supported by the latest release of Numba visit:\nhttp://numba.pydata.org/numba-doc/dev/reference/pysupported.html\nand\nhttp://numba.pydata.org/numba-doc/dev/reference/numpysupported.html\n\nFor more information about typing errors and how to debug them visit:\nhttp://numba.pydata.org/numba-doc/latest/user/troubleshoot.html#my-code-doesn-t-compile\n\nIf you think your code should work with Numba, please report the error message\nand traceback, along with a minimal reproducer at:\nhttps://github.com/numba/numba/issues/new\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "\n",
    "scaling_factor = 0.9\n",
    "crossover_rate = 0.5\n",
    "\n",
    "def fitness(chromosome):\n",
    "    return cohesion_separation2(chromosome, jaccard_sim, doc)\n",
    "\n",
    "t0 = time.time()\n",
    "result = run_iterations(pop_size=100, summ_len=5, num_sents=len(doc), \n",
    "                        func=fitness, lam=0.9, cr=0.5, iterations=50, verbose=False)\n",
    "t1 = time.time()\n",
    "print(t1-t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 512,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(38, 38, (100, 38))"
      ]
     },
     "execution_count": 512,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(doc), len(document_sentences), result.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\n",
    "\\large\n",
    "X_r(t+1) = \n",
    "\\begin{cases}\n",
    "    Y_r ( t + 1 ) & \\text{if } f \\big( Y_r ( t + 1 ) \\big) > f \\big( X_r ( t ) \\big) \\\\\n",
    "    X_r ( t ) & \\text{otherwise}\n",
    "\\end{cases}\n",
    "$\n",
    "\n",
    "where \n",
    "- $ f ( \\cdot ) $ is the objective function to be maximized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 496,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([], dtype=int64),\n",
       " array([0]),\n",
       " array([0, 1]),\n",
       " array([0, 1, 2]),\n",
       " array([0, 1, 2, 3])]"
      ]
     },
     "execution_count": 496,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@numba.njit\n",
    "def foo(x):\n",
    "    lst = []\n",
    "    for i in range(x):\n",
    "        lst.append(np.arange(i))\n",
    "    return lst\n",
    "\n",
    "foo(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mutation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At each iteration $ t + 1 $ for each $ X_r ( t ) $ creates\n",
    "$ m_r ( t + 1 ) = [ m_{ r, 1 } ( t ), m_{ r, 2 } ( t ), ..., m_{ r, n } ( t ) ] $.  \n",
    "For each gene, 1 indicates no mutation and 0 means mutate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\n",
    "\\large\n",
    "m_{ r, s } ( t + 1 ) = \n",
    "\\begin{cases}\n",
    "    1 & \\text{if } rand_s < sigm \\big( y_{ r, s } ( t + 1 ) \\big) \\\\\n",
    "    0 & \\text{otherwise}\n",
    "\\end{cases}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inversion Operator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Figure 1 - Psuedo-code\n",
    "<img src=\"./data/pngs/fig1_-_inversion_operator_psuedo_code.png\" alt=\"Fig 1. Inverse operator psuedo-code\" width=\"33%\" align=\"left\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Figure 2 - Example\n",
    "<img src=\"./data/pngs/fig2_-_inversion_operator_diagram.png\" alt=\"Fig 2. Inverse operator example\" width=\"33%\" align=\"left\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Iterations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ROUGE-N\n",
    "_(Recall Oriented Understudy for Gisting Evaluation)_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\n",
    "\\large\n",
    "\\text{ROUGE-N} = \n",
    "\\dfrac\n",
    "    { \\sum \\limits\n",
    "        _{ \\small S \\in Summ_{ ref }} \n",
    "        \\sum \\limits\n",
    "        _{ \\small \\text{N-gram} \\in S } \n",
    "            Count_{ \\small match } ( \\text{N-gram} ) }\n",
    "    { \\sum \\limits\n",
    "        _{ \\small S \\in Summ_{ ref } } \n",
    "        \\sum \\limits\n",
    "            _{ \\small \\text{N-gram} \\in S } \n",
    "            Count( \\text{N-gram} ) }\n",
    "$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rouge_n(n, y_pred, y_true):\n",
    "    n_gram_pred = set(ngrams(y_pred, n))\n",
    "    n_gram_true = set(ngrams(y_true, n))\n",
    "    return len(n_gram_pred & n_gram_true) / len(n_gram_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
