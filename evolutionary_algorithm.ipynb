{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import itertools\n",
    "import functools\n",
    "import math\n",
    "# import multiprocessing\n",
    "import operator\n",
    "import string\n",
    "# import random\n",
    "\n",
    "import numba\n",
    "import numpy as np\n",
    "# import pandas as pd\n",
    "\n",
    "from nltk import tokenize\n",
    "from nltk.util import ngrams\n",
    "\n",
    "# from scipy.spatial.distance import jaccard\n",
    "# from scipy.special import expit as sigmoid\n",
    "\n",
    "# from sklearn.metrics import jaccard_similarity_score\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import euclidean_distances, cosine_distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pathlib\n",
    "\n",
    "cwd = pathlib.Path.cwd()\n",
    "data = cwd / 'data'\n",
    "jsons = data / 'jsons'\n",
    "json_2018 = jsons / '2018'\n",
    "\n",
    "json_2018 = list(json_2018.iterdir())[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(json_2018) as fp:\n",
    "    articles_2018 = json.load(fp)['2018']\n",
    "\n",
    "article = articles_2018[4]\n",
    "text = article['story']\n",
    "summ_true = article['summary']\n",
    "# print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evolutionary Algorithms\n",
    "- [Discrete Differential Evolution for Text Summarization](https://www.researchgate.net/publication/281662415_Discrete_Differential_Evolution_for_Text_Summarization)\n",
    "- [Evolutionary Algorithm for Extractive Text Summarization](https://www.researchgate.net/profile/Ramiz_Aliguliyev/publication/220518077_Evolutionary_Algorithm_for_Extractive_Text_Summarization/links/09e4151356fc2caab6000000.pdf)\n",
    "- [An Improved Evolutionary Algorithm for Extractive Text Summarization](https://link.springer.com/chapter/10.1007/978-3-642-36543-0_9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Similarity Measures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Jaccard Coefficient Similarity Measure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\n",
    "\\large\n",
    "sim_{ jaccard } ( S_i, S_j ) = \n",
    "\\dfrac\n",
    "    { | S_i \\cap S_j | }\n",
    "    { | S_i \\cup S_j | }\n",
    "$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "@numba.njit\n",
    "def jaccard_similarity(a, b):\n",
    "    #: assume union is non-empty since each sentence >= 1 word\n",
    "    return np.sum(a & b) / np.sum(a | b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Objective Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let $ C $ be a partition of $ D $ with $ k $ clusters.  \n",
    "$ C = \\{ C_1, C_2, ..., C_k \\} $  \n",
    "\n",
    "where:\n",
    "- $ C_p \\cap C_q = \\emptyset, \n",
    "        \\forall p \\ne q \\in \\{ 1, 2, ..., k \\} \n",
    "  $\n",
    "- $ \\bigcup\\limits\n",
    "    _{ p = 1 }\n",
    "    ^k C_p = D \n",
    "  $\n",
    "- $ C_p \\ne \\emptyset, \n",
    "        \\forall p \\in \\{ 1, 2, ..., k \\}\n",
    "  $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sigmoid Function\n",
    "$\n",
    "\\large\n",
    "sigm ( x ) = \n",
    "\\dfrac\n",
    "    { 1 }\n",
    "    { 1 + \\text{exp} ( -x ) }\n",
    "$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [],
   "source": [
    "@numba.njit\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Intra-Cluster Similiarity (Cohesion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\n",
    "\\large\n",
    "F_1 = \n",
    "\\sum\\limits\n",
    "    _{ \\small p = 1 }\n",
    "    ^{ \\small k } \n",
    "\\sum\\limits\n",
    "    _{ \\small S_i, S_j \\in C_p } \n",
    "\\dfrac \n",
    "    { sim ( S_i, S_j ) } \n",
    "    { | C_p | } \n",
    "\\rightarrow \\text{max}\n",
    "$\n",
    "\n",
    "*(__Note:__ Evol Alg for Ext Txt Summ doesn't show division of $|C_p|$ but I believe this to be a typo since the paragraph detailing it says \"the average sum\". Additionally the Disc Diff Evol for Txt Summ shows it as such.)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @numba.njit\n",
    "# def cohesion(chromosome, similarity, document):\n",
    "#     total = 0\n",
    "#     for p in np.unique(chromosome):\n",
    "#         sents = document[np.where(chromosome == p)]\n",
    "#         k = len(sents)\n",
    "#         #: combinations choose 2\n",
    "#         for i in range(k-1):\n",
    "#             for j in range(i+1, k):\n",
    "#                 total += similarity(sents[i], sents[j]) / len(sents)  \n",
    "#     return total\n",
    "\n",
    "\n",
    "@numba.njit(parallel=True)\n",
    "def cohesion(chromosome, similarity, document):\n",
    "    total = 0\n",
    "    clusters = len(np.unique(chromosome))\n",
    "    for p in numba.prange(clusters):\n",
    "        sents = document[np.where(chromosome == p)]\n",
    "        k = len(sents)\n",
    "        #: combinations choose 2\n",
    "        for i in numba.prange(k-1):\n",
    "            for j in numba.prange(i+1, k):\n",
    "                total += similarity(sents[i], sents[j]) / len(sents)  \n",
    "    return total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inter-Cluster Dissimilarity (Separation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\n",
    "\\large\n",
    "F_2 = \n",
    "\\sum\\limits\n",
    "    _{ \\small p = 1 }\n",
    "    ^{ \\small k - 1 }\n",
    "\\sum\\limits\n",
    "    _{ \\small q = p + 1 }\n",
    "    ^{ \\small k } \n",
    "\\sum\\limits\n",
    "    _{ \\small S_i \\in C_p } \n",
    "\\sum\\limits\n",
    "    _{ \\small S_j \\in C_q } \n",
    "\\dfrac \n",
    "    { sim ( S_i, S_j ) } \n",
    "    { | C_p | \\cdot | C_q | }\n",
    "\\rightarrow \\text{min}\n",
    "$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @numba.njit\n",
    "# def separation(chromosome, similarity, document):\n",
    "#     total = 0\n",
    "#     k = len(np.unique(chromosome))\n",
    "#     #: combinations choose 2\n",
    "#     for p in range(k-1):\n",
    "#         for q in range(p+1, k):\n",
    "#             sents_p = document[np.where(chromosome == p)]\n",
    "#             sents_q = document[np.where(chromosome == q)]\n",
    "#             #: product\n",
    "#             m, n = len(sents_p), len(sents_q)\n",
    "#             for i in range(m):\n",
    "#                 for j in range(n):\n",
    "#                     total += similarity(sents_p[i], sents_q[j]) / m / n\n",
    "#     return total\n",
    "\n",
    "\n",
    "@numba.njit(parallel=True)\n",
    "def separation(chromosome, similarity, document):\n",
    "    total = 0\n",
    "    k = len(np.unique(chromosome))\n",
    "    #: combinations choose 2\n",
    "    for p in numba.prange(k-1):\n",
    "        for q in numba.prange(p+1, k):\n",
    "            sents_p = document[np.where(chromosome == p)]\n",
    "            sents_q = document[np.where(chromosome == q)]\n",
    "            #: product\n",
    "            m, n = len(sents_p), len(sents_q)\n",
    "            for i in numba.prange(m):\n",
    "                for j in numba.prange(n):\n",
    "                    total += similarity(sents_p[i], sents_q[j]) / m / n\n",
    "    return total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inter/Intra-Cluster Balance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\n",
    "\\large\n",
    "F = \\big( 1 + sigm ( F_1 ) \\big)\n",
    "    ^{ F_2 } \\rightarrow \\text{max}\n",
    "$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [],
   "source": [
    "@numba.njit\n",
    "def cohesion_separation(chromosome, similarity, document):\n",
    "    coh = cohesion(chromosome, similarity, document)\n",
    "    sep = separation(chromosome, similarity, document)\n",
    "    return (1 + sigmoid(coh)) ** sep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitness"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\n",
    "\\large\n",
    "fitness_1 \\big( X_a ( t ) \\big) = F_1 \\big( X_a ( t ) \\big)\n",
    "$\n",
    "\n",
    "$\n",
    "\\large\n",
    "fitness_2 \\big( X_a ( t ) \\big) = \\dfrac{ 1 }{ F_2 ( X_a ( t ) ) }\n",
    "$\n",
    "\n",
    "$\n",
    "\\large\n",
    "fitness \\big( X_a ( t ) \\big) = F \\big( X_a ( t ) \\big)\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modified Discrete Differential Evolution Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize the population with $ N $ chromosomes each composed of $ n $ random integers from \\[1, k\\].  \n",
    "\n",
    "$\n",
    "X_r ( t ) = [ x_{ r, 1 } ( t ), x_{ r, 2 } ( t ), ..., x_{ r, n } ( t ) ]\n",
    "$  \n",
    "\n",
    "where:\n",
    "- $ x_{ r, s } ( t ) \\in \\{ 1, 2, ..., k \\} $\n",
    "- $ r = 1, 2, ..., N $\n",
    "- $ s = 1, 2, ..., n $\n",
    "- $ N $ is the population size\n",
    "- $ n $ is the number of sentences _(in the document)_\n",
    "- $ k $ is the number of clusters _(number of sentences for summary)_\n",
    "- $ t $ is the iteration step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_chromosome(choices, length):\n",
    "    chrom = np.full(length, -1)\n",
    "    #: ensure that each choice is accounted for at least once\n",
    "    idxs = np.random.choice(np.arange(length), len(choices), replace=False)\n",
    "    chrom[idxs] = np.random.permutation(choices)\n",
    "    idxs = np.where(chrom == -1)[0]\n",
    "    chrom[idxs] = np.random.choice(choices, len(idxs))\n",
    "    return chrom\n",
    "\n",
    "\n",
    "def init_population(population_size, cluster_amount, chromosome_length):\n",
    "    clusts = np.arange(cluster_amount)\n",
    "    chroms = [init_chromosome(clusts, chromosome_length) for _ in range(population_size)]\n",
    "    pop = np.vstack(chroms)\n",
    "    return pop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\n",
    "\\large\n",
    "y_{ r, s } ( t + 1 ) = \n",
    "\\begin{cases}\n",
    "    x_{ r1, s } ( t ) + \\lambda \\big( x_{ r2, s } ( t ) - x_{ r3, s } ( t ) \\big) \n",
    "        & \\text{if } rand_s < \\text{CR} \\\\\n",
    "    x_{ r, s } ( t ) & \\text{otherwise}\n",
    "\\end{cases}\n",
    "$\n",
    "\n",
    "where  \n",
    "- For each $ X_r $, randomly sample $ X_{ r1 } ( t ), X_{ r2 } ( t ), X_{ r3 } ( t ) $ \n",
    "  from the same generation _(each distinct)_\n",
    "- $ rand_s $ is uniformally distributed random numbers from $ [ 0, 1 ] $ \n",
    "  chosen once for each $ s \\in \\{ 1, 2, ..., n \\} $\n",
    "\n",
    "hyper-parameters \n",
    "- $ \\lambda $ is a scale factor from $ [ 0, 1 ] $\n",
    "- $ \\text{CR} $ is the crossover rate from $ [ 0, 1 ] $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_offspring(population, randoms, lambda_, crossover_rate):\n",
    "    #: For computation time, relax requirement that X_r, X_r1, X_r2, X_r3 are distinct. \n",
    "    #: With large population size, this is unlikely to occur, and if it does, it doesn't\n",
    "    #: seem that detrimental. Also is this mitigated with appropriate lam choice?\n",
    "    n = len(population)\n",
    "    idxs = np.random.choice(np.arange(n), size=(n, 3))\n",
    "    chrom_1, chrom_2, chrom_3 = map(np.squeeze, np.split(population[idxs], 3, axis=1))\n",
    "    k = len(np.unique(population))\n",
    "    offspr = (chrom_1 + lambda_ * (chrom_2 - chrom_3)) % k\n",
    "    mask = randoms < crossover_rate\n",
    "    offspr[mask] = population[mask]\n",
    "    return offspr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: early stopping --> little fitness improvement over x generations, good enough fitness score\n",
    "def run_iterations(pop_size, summ_len, num_sents, func, lam, cr, iterations, *, mutate_after=True,\n",
    "                   seed=None, verbose=False, save_rate=np.nan, save_dir=None):\n",
    "    \n",
    "    if save_dir is not None:\n",
    "        save_dir = pathlib.Path(save_dir)\n",
    "        if not save_dir.is_dir():\n",
    "            msg = f'save_dir={save_dir} not a valid directory path'.format(save_dir=save_dir)\n",
    "            raise NotADirectoryError(msg)\n",
    "    \n",
    "    if seed is not None:\n",
    "        np.random.seed(seed)\n",
    "    \n",
    "    pop = init_population(pop_size, summ_len, num_sents)\n",
    "    shape = pop.shape\n",
    "    for i in range(iterations):\n",
    "        if i % save_rate == 0:\n",
    "            file = save_dir / 'generation_{i:0>pad}'.format(i=i, pad=len(str(iterations)))\n",
    "            np.save(file, pop)\n",
    "            \n",
    "        if verbose:\n",
    "            print(i)  #TODO: logfile --> iteration number, best fitness score, avg fitness score, hyper-params\n",
    "        \n",
    "        rand = np.random.random_sample(shape)\n",
    "        \n",
    "        t0 = time.time()\n",
    "        offspr = get_offspring(pop, rand, lam, cr)\n",
    "        t1 = time.time()\n",
    "        PROFILER['offspring'] += t1 - t0\n",
    "        \n",
    "        #: option since papers unclear if mutate at offspring or survivors stage\n",
    "        if not mutate_after:\n",
    "            mutate(offspr, rand)\n",
    "            \n",
    "        t0 = time.time()\n",
    "        next_generation(pop, offspr, func)\n",
    "        t1 = time.time()\n",
    "        PROFILER['generation'] += t1 - t0\n",
    "        \n",
    "        if mutate_after:\n",
    "            t0 = time.time()\n",
    "            mutate(pop, rand)\n",
    "            t1 = time.time()\n",
    "            PROFILER['mutate'] += t1 - t0\n",
    "    \n",
    "    return pop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [],
   "source": [
    "def best_chromosome(population):\n",
    "    #TODO: make sure it picks one with all k-clusters\n",
    "    fits = np.argmax([fitness(chrom) for chrom in population])\n",
    "    chrom = population[fits]\n",
    "    return chrom\n",
    "    \n",
    "\n",
    "def central_sentences(chromosome, document, metric=cosine_distances):\n",
    "    central_sents = []\n",
    "    for cluster in np.unique(chromosome):\n",
    "        idxs = np.where(chromosome == cluster)[0]\n",
    "        sents = document[idxs]\n",
    "        centroid = sents.mean(axis=0)[np.newaxis,:]\n",
    "        dists = metric(sents, centroid)\n",
    "        cent_sent = idxs[np.argmin(dists)]\n",
    "        central_sents.append(cent_sent)\n",
    "    return central_sents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\n",
    "\\large\n",
    "X_r(t+1) = \n",
    "\\begin{cases}\n",
    "    Y_r ( t + 1 ) & \\text{if } f \\big( Y_r ( t + 1 ) \\big) > f \\big( X_r ( t ) \\big) \\\\\n",
    "    X_r ( t ) & \\text{otherwise}\n",
    "\\end{cases}\n",
    "$\n",
    "\n",
    "where \n",
    "- $ f ( \\cdot ) $ is the objective function to be maximized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [],
   "source": [
    "def next_generation(population, offspring, func):\n",
    "    fit_off = np.array([func(chrom) for chrom in offspring])\n",
    "    fit_pop = np.array([func(chrom) for chrom in population])\n",
    "    mask = fit_off > fit_pop\n",
    "    population[mask] = offspring[mask]\n",
    "    return\n",
    "\n",
    "# @numba.njit(parallel=True)\n",
    "# def _get_mask(pop, offspr, func):\n",
    "#     fit_pop = np.empty_like(pop)\n",
    "#     fit_off = np.empty_like(offspr)\n",
    "#     for i in numba.prange(len(pop)):\n",
    "#         fit_pop[i] = func(pop[i])\n",
    "#         fit_off[i] = func(offspr[i])\n",
    "#     mask = fit_off > fit_pop\n",
    "#     return mask\n",
    "\n",
    "# def next_generation(pop, offspr, func):\n",
    "#     mask = _get_mask(pop, offspr, func)\n",
    "#     pop[mask] = offspr[mask]\n",
    "#     return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mutation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At each iteration $ t + 1 $ for each $ X_r ( t ) $ creates\n",
    "$ m_r ( t + 1 ) = [ m_{ r, 1 } ( t ), m_{ r, 2 } ( t ), ..., m_{ r, n } ( t ) ] $.  \n",
    "For each gene, 1 indicates no mutation and 0 means mutate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\n",
    "\\large\n",
    "m_{ r, s } ( t + 1 ) = \n",
    "\\begin{cases}\n",
    "    1 & \\text{if } rand_s < sigm \\big( y_{ r, s } ( t + 1 ) \\big) \\\\\n",
    "    0 & \\text{otherwise}\n",
    "\\end{cases}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inversion Operator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Figure 1 - Psuedo-code\n",
    "<img src=\"./data/pngs/fig1_-_inversion_operator_psuedo_code.png\" alt=\"Fig 1. Inverse operator psuedo-code\" width=\"33%\" align=\"left\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Figure 2 - Example\n",
    "<img src=\"./data/pngs/fig2_-_inversion_operator_diagram.png\" alt=\"Fig 2. Inverse operator example\" width=\"33%\" align=\"left\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mutate(population, randoms):\n",
    "    mask = randoms < sigmoid(population)\n",
    "    #: inversion operator\n",
    "    idxs = np.nonzero(mask)\n",
    "    arr = np.array(idxs)\n",
    "    sorter = np.lexsort((-arr[1], arr[0]))\n",
    "    rev = arr.T[sorter].T\n",
    "    population[idxs] = population[(rev[0], rev[1])]\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ROUGE-N\n",
    "_(Recall Oriented Understudy for Gisting Evaluation)_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\n",
    "\\large\n",
    "\\text{ROUGE-N} = \n",
    "\\dfrac\n",
    "    { \\sum \\limits\n",
    "        _{ \\small S \\in Summ_{ ref }} \n",
    "        \\sum \\limits\n",
    "        _{ \\small \\text{N-gram} \\in S } \n",
    "            Count_{ \\small match } ( \\text{N-gram} ) }\n",
    "    { \\sum \\limits\n",
    "        _{ \\small S \\in Summ_{ ref } } \n",
    "        \\sum \\limits\n",
    "            _{ \\small \\text{N-gram} \\in S } \n",
    "            Count( \\text{N-gram} ) }\n",
    "$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rouge_n(n, y_pred, y_true):\n",
    "    n_gram_pred = set(ngrams(y_pred, n))\n",
    "    n_gram_true = set(ngrams(y_true, n))\n",
    "    return len(n_gram_pred & n_gram_true) / len(n_gram_true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35.624605894088745\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Counter({'offspring': 0.15188932418823242,\n",
       "         'generation': 35.24577760696411,\n",
       "         'mutate': 0.18845772743225098})"
      ]
     },
     "execution_count": 307,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "PROFILER = collections.Counter()\n",
    "\n",
    "\n",
    "def fitness(chromosome):\n",
    "    return cohesion_separation(chromosome, jaccard_similarity, doc)\n",
    "\n",
    "cv = CountVectorizer(stop_words='english')\n",
    "sents = tokenize.sent_tokenize(text.lower())\n",
    "vec = cv.fit_transform(sents)\n",
    "doc = vec.toarray().astype(bool).astype(int)\n",
    "\n",
    "\n",
    "t0 = time.time()\n",
    "pop = run_iterations(pop_size=100, summ_len=5, num_sents=len(doc), \n",
    "                     func=fitness, lam=0.9, cr=0.5, iterations=500, verbose=False, seed=0)\n",
    "t1 = time.time()\n",
    "print(t1-t0)\n",
    "\n",
    "\n",
    "chrom_best = best_chromosome(pop)\n",
    "idxs = central_sentences(chrom_best, doc)\n",
    "summ_evol = '\\n'.join(np.array(tokenize.sent_tokenize(text))[idxs])\n",
    "\n",
    "PROFILER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Antos and Corlette both say they don't expect the availability of short-term policies to have a major impact on the ACA exchanges.\n",
      "Many of those people have been priced out of the health insurance market since the ACA took effect, says Joseph Antos, an economist at the American Enterprise Institute.\n",
      "Short-term plans don't have to meet the Affordable Care Act's consumer protection and coverage requirements, so many will not cover services such as mental health care or prescription drugs.\n",
      "People who don't get insurance through their jobs will now be able to buy short-term policies that may be cheaper than Affordable Care Act coverage.\n",
      "The plans are aimed at people who earn more than four times the federal poverty threshold and therefore don't qualify for federal subsidies to help pay premiums for policies sold on the ACA exchanges.\n"
     ]
    }
   ],
   "source": [
    "print(summ_evol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "And insurance companies can deny customers coverage on these plans if they have a pre-existing medical condition and charge people more if they are likely to need medical care.\n",
      "Many of those people have been priced out of the health insurance market since the ACA took effect, says Joseph Antos, an economist at the American Enterprise Institute.\n",
      "Short-term plans don't have to meet the Affordable Care Act's consumer protection and coverage requirements, so many will not cover services such as mental health care or prescription drugs.\n",
      "And if a short-term policy ends before the open enrollment period for Affordable Care Act insurance, consumers will not have the right to buy ACA insurance right away, says Randy Pate, the deputy administrator of the Centers for Medicare and Medicaid Services at HHS.\n",
      "According to a report by the National Association of Insurance Commissioners, the policies paid out an average 55 percent of their premiums in actual health care last year.\n"
     ]
    }
   ],
   "source": [
    "print(summ_evol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scores(n, y_pred):\n",
    "    for i in range(1, n+1):\n",
    "        score = rouge_n(i, y_pred, summ_true)\n",
    "        print(i, score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evolutionary\n",
      "1 0.9230769230769231\n",
      "2 0.8529411764705882\n",
      "3 0.5827338129496403\n",
      "4 0.46938775510204084\n",
      "5 0.3959731543624161\n"
     ]
    }
   ],
   "source": [
    "print('evolutionary')\n",
    "scores(5, summ_evol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gensim\n",
      "1 0.9615384615384616\n",
      "2 0.9411764705882353\n",
      "3 0.7410071942446043\n",
      "4 0.6258503401360545\n",
      "5 0.5100671140939598\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "\n",
    "summ_gensim = gensim.summarization.summarize(text)\n",
    "print('gensim')\n",
    "scores(5, summ_gensim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sumy - text rank\n",
      "1 0.9615384615384616\n",
      "2 0.8529411764705882\n",
      "3 0.6330935251798561\n",
      "4 0.5034013605442177\n",
      "5 0.4228187919463087\n",
      "\n",
      "sumy - lex rank\n",
      "1 0.9230769230769231\n",
      "2 0.7254901960784313\n",
      "3 0.48201438848920863\n",
      "4 0.35374149659863946\n",
      "5 0.26174496644295303\n"
     ]
    }
   ],
   "source": [
    "from sumy.parsers.plaintext import PlaintextParser\n",
    "from sumy.summarizers.lex_rank import LexRankSummarizer\n",
    "from sumy.summarizers.text_rank import TextRankSummarizer\n",
    "from sumy.nlp.tokenizers import Tokenizer\n",
    "\n",
    "\n",
    "def format_sumy(sumy_summary):\n",
    "    return ''.join(char for sent in sumy_summary for char in str(sent) if char in string.printable)\n",
    "\n",
    "parser = PlaintextParser(text, Tokenizer('english'))\n",
    "\n",
    "text_rank = TextRankSummarizer()\n",
    "summ_text_rank = format_sumy(text_rank(parser.document, 5))\n",
    "print('sumy - text rank')\n",
    "scores(5, summ_text_rank)\n",
    "\n",
    "print()\n",
    "\n",
    "lex_rank = LexRankSummarizer()\n",
    "summ_lex_rank = format_sumy(lex_rank(parser.document, 2))\n",
    "print('sumy - lex rank')\n",
    "scores(5, summ_lex_rank)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
