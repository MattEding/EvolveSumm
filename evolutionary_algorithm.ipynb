{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import itertools\n",
    "import functools\n",
    "import math\n",
    "import operator\n",
    "import string\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from nltk import tokenize\n",
    "from nltk.util import ngrams\n",
    "\n",
    "from scipy.special import expit as sigmoid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import contextlib\n",
    "import io\n",
    "\n",
    "with io.StringIO() as str_io, contextlib.redirect_stdout(str_io):\n",
    "    import this\n",
    "    zen = str_io.getvalue()\n",
    "\n",
    "del str_io, this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"the zen of python, by tim peters\\n\\nbeautiful is better than ugly.\\nexplicit is better than implicit.\\nsimple is better than complex.\\ncomplex is better than complicated.\\nflat is better than nested.\\nsparse is better than dense.\\nreadability counts.\\nspecial cases aren't special enough to break the rules.\\nalthough practicality beats purity.\\nerrors should never pass silently.\\nunless explicitly silenced.\\nin the face of ambiguity, refuse the temptation to guess.\\nthere should be one-- and preferably only one --obvious way to do it.\\nalthough that way may not be obvious at first unless you're dutch.\\nnow is better than never.\\nalthough never is often better than *right* now.\\nif the implementation is hard to explain, it's a bad idea.\\nif the implementation is easy to explain, it may be a good idea.\\nnamespaces are one honking great idea -- let's do more of those!\\n\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def distinct_words(text):\n",
    "    no_punctuation = ''.join(t for t in text if t not in string.punctuation)\n",
    "    return frozenset(tokenize.word_tokenize(no_punctuation))\n",
    "\n",
    "text = zen.lower()\n",
    "text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evolutionary Algorithms\n",
    "- [Discrete Differential Evolution for Text Summarization](https://www.researchgate.net/publication/281662415_Discrete_Differential_Evolution_for_Text_Summarization)\n",
    "- [Evolutionary Algorithm for Extractive Text Summarization](https://www.researchgate.net/profile/Ramiz_Aliguliyev/publication/220518077_Evolutionary_Algorithm_for_Extractive_Text_Summarization/links/09e4151356fc2caab6000000.pdf)\n",
    "- [An Improved Evolutionary Algorithm for Extractive Text Summarization](https://link.springer.com/chapter/10.1007/978-3-642-36543-0_9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentence Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let document $D$ be decomposed into a set of $n$ sentences.  \n",
    "$\n",
    "D = \\{S_1, S_2, ..., S_n\\}\n",
    "$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "D = document_sentences = set(tokenize.sent_tokenize(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let terms $T$ be the set of all $m$ distinct words in D.  \n",
    "$\n",
    "T = \\{t_1, t_2, ..., t_m\\}\n",
    "$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "T = document_distinct_words = distinct_words(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let $S_i$ represent the set of distinct terms in sentence $S_i$ with \n",
    "$m_i$ distinct terms.  \n",
    "$\n",
    "S_i = \\{t_1, t_2, ..., t_{m_i}\\}\n",
    "$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "S = sentence_distinct_words = {distinct_words(ds) for ds in document_sentences}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seed = None  # need reproducibility option\n",
    "\n",
    "# N = len(population)\n",
    "# n = len(sentences)\n",
    "# k = len(clusters)\n",
    "\n",
    "# r = range(N)\n",
    "# s = range(n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Similarity Measures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Jaccard Coefficient Similarity Measure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\n",
    "\\large\n",
    "sim_{jaccard}(S_i, S_j) = \\frac{|S_i \\cap S_j|}{|S_i \\cup S_j|}\n",
    "$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jaccard_similarity(a, b):\n",
    "    a, b = set(a), set(b)\n",
    "    if not a and not b:\n",
    "        return 1.0\n",
    "    return len(a & b) / len(a | b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = \"however doctors learned that long inactivity did more harm than good\".split()\n",
    "b = \"patients got out of shape developed blood clots and became demoralized\".split()\n",
    "\n",
    "jaccard_similarity(a, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalized Google Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\n",
    "\\large\n",
    "NGD(t_k, t_l) = \\dfrac \n",
    "{ max\\big\\{log(f_k), log(f_l)\\big\\} - log(f_{lk}) }\n",
    "{ log(n) - min\\big\\{log(f_k), log(f_l)\\big\\} }\n",
    "$  \n",
    "\n",
    "where:\n",
    "- $t_k$ and $t_l$ are terms \n",
    "- $f_k$ is the number of sentences containing $t_k$\n",
    "- $f_{kl}$ is the number of sentences containing both $t_k$ and $t_l$\n",
    "- $n$ is the total number of sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\n",
    "\\large \n",
    "sim_{NGD}(t_k, t_l) = e^{-NGD(t_k, t_l)}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\n",
    "\\large \n",
    "sim_{NGD}(S_i, S_j) = \\dfrac\n",
    "{ \\sum\\limits_{ \\small t_k \\in S_i} \\sum\\limits_{ \\small t_l \\in S_j} sim_{NGD}(t_k, t_l) }\n",
    "{ m_i m_j }\n",
    "$  \n",
    "\n",
    "where:\n",
    "- $S_i$ and $S_j$ are sentences\n",
    "- $m_i$ is the number of words in $S_i$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NormalizedGoogle:\n",
    "    def __init__(self, document):\n",
    "        self.sentence_words = tuple(distinct_words(sent) for sent in tokenize.sent_tokenize(document))\n",
    "        \n",
    "    # double check scientific paper's handling of \"bad\" log values\n",
    "    def distance(self, term_k, term_l):\n",
    "        freq_k = sum(term_k in sent for sent in self.sentence_words)\n",
    "        freq_l = sum(term_l in sent for sent in self.sentence_words)\n",
    "        if not (freq_k and freq_l):\n",
    "            raise ValueError('terms must be in document')\n",
    "\n",
    "        freq_kl = sum((term_k in sent) and (term_l in sent) for sent in self.sentence_words)\n",
    "        if (freq_k > 0) and (freq_l > 0) and (freq_kl == 0):\n",
    "            return 1.0\n",
    "\n",
    "        logs_k_l = (math.log(freq_k), math.log(freq_l))\n",
    "        n = len(self.sentence_words)\n",
    "\n",
    "        numerator = max(logs_k_l) - math.log(freq_kl)\n",
    "        denominator = math.log(n) - min(logs_k_l)\n",
    "        return numerator / denominator\n",
    "    \n",
    "    def term_similarity(self, term_k, term_l):\n",
    "        dist = self.distance(term_k, term_l)\n",
    "        return math.exp(-dist)\n",
    "    \n",
    "    def sentence_similarity(self, sent_i, sent_j):\n",
    "        total = sum(self.term_similarity(term_k, term_l)\n",
    "                    for term_k, term_l in itertools.product(sent_i, sent_j))\n",
    "        return total / len(sent_i) / len(sent_j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_google = NormalizedGoogle(text)\n",
    "\n",
    "# if tₖ == tₗ --> 1\n",
    "assert norm_google.term_similarity('better', 'better') == 1\n",
    "\n",
    "# if (tₖ != tₗ) and (fₖ == fₗ == fₖₗ > 0) --> 1\n",
    "assert norm_google.term_similarity('explicit', 'implicit') == 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objective Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let $C$ be a partition of D with $k$ clusters.  \n",
    "$\n",
    "C = \\{C_1, C_2, ..., C_k\\}\n",
    "$  \n",
    "\n",
    "where:\n",
    "- $ C_p \\cap C_q = \\emptyset, \\forall p \\ne q \\in \\{1, 2, ..., k\\} $\n",
    "- $ \\bigcup\\limits_{p=1}^k C_p = D $\n",
    "- $ C_p \\ne \\emptyset, \\forall p \\in \\{1, 2, ..., k\\} $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def verify_cluster(C, D):\n",
    "    disjoint = all(not C_i & C_j for C_i, C_j in itertools.combinations(C, C))\n",
    "    union = functools.reduce(operator.or_, C) == D\n",
    "    nonempty = all(C_p for C_p in C)\n",
    "    if not (disjoint and union and nonempty):\n",
    "        raise ValueError('cluster is not a partition')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sigmoid Function\n",
    "$\n",
    "\\large\n",
    "sigm(x) = \\frac{ 1 }{ 1 + e^{-x} }\n",
    "$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.special import expit as sigmoid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Intra-Cluster Similiarity (Cohesion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\n",
    "\\large\n",
    "F_1 = \n",
    "\\sum\\limits_{ \\small p=1}^{ \\small k} \n",
    "\\sum\\limits_{ \\small S_i, S_j \\in C_p} \n",
    "\\frac {sim(S_i, S_j)} {|C_p|} \n",
    "\\rightarrow max\n",
    "$\n",
    "\n",
    "*(__Note:__ Evol Alg for Ext Txt Summ doesn't show division of $|C_p|$ but I believe this to be a typo since the paragraph detailing it says \"the average sum\". Additionally the Disc Diff Evol for Txt Summ shows it as such.)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clusterize(chromosome):\n",
    "    partition = collections.defaultdict(list)\n",
    "    for i, cluster in enumerate(chromosome):\n",
    "        partition[cluster].append(i)\n",
    "    return partition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(list, {0: [0, 2], 4: [1, 8], 2: [3, 10], 1: [4, 5, 6], 3: [7, 9]})"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences = np.array(['to the moon', 'moon shine is good', 'wolves howl to the moon', 'sunny day as ever', \n",
    "                      'it is windy', 'go to the store now', 'testing testing testing', 'python good js bad', \n",
    "                      'time to leave', 'the time is now', 'finally the wolves are here'])\n",
    "chromosome = np.array([0, 4, 0, 2, 1, 1, 1, 3, 4, 3, 2])\n",
    "clusterize(chromosome)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cohesion(chromosome, sentences, sim):\n",
    "    total = 0\n",
    "    clusters = clusterize(chromosome)\n",
    "    for cluster in clusters.values():\n",
    "        for i, j in itertools.combinations(cluster, r=2):\n",
    "            sent_i, sent_j = sentences[[i,j]]\n",
    "            total += sim(sent_i, sent_j) / len(cluster)\n",
    "    return total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.3852079293255766"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cohesion(chromosome, sentences, jaccard_similarity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inter-Cluster Dissimilarity (Separation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\n",
    "\\large\n",
    "F_2 = \n",
    "\\sum\\limits_{ \\small p=1}^{ \\small k-1}\n",
    "\\sum\\limits_{ \\small q=p+1}^{ \\small k} \n",
    "\\sum\\limits_{ \\small S_i \\in C_p} \n",
    "\\sum\\limits_{ \\small S_j \\in C_q} \n",
    "\\dfrac {sim(S_i, S_j)} {|C_p| \\cdot |C_q|}\n",
    "\\rightarrow min\n",
    "$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def separation(chromosome, sentences, sim):\n",
    "    total = 0\n",
    "    clusters = clusterize(chromosome)\n",
    "    for cluster_p, cluster_q in itertools.combinations(clusters.values(), r=2):\n",
    "        for i, j in itertools.product(cluster_p, cluster_q):\n",
    "            sent_i, sent_j = sentences[[i,j]]\n",
    "            total += sim(sent_i, sent_j) / len(cluster_p) / len(cluster_q)\n",
    "    return total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.317544526551878"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "separation(chromosome, sentences, jaccard_similarity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inter/Intra-Cluster Balance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\n",
    "\\large\n",
    "F = \\big(1 + sigm(F_1) \\big)^{F_2} \\rightarrow max\n",
    "$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cohesion_separation(chromosome, sentences, sim):\n",
    "    coh = cohesion(chromosome, sentences, sim)\n",
    "    sep = separation(chromosome, sentences, sim)\n",
    "    return pow(1 + sigmoid(coh), sep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12.646468283276466"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cohesion_separation(chromosome, sentences, jaccard_similarity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitness"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\n",
    "\\large\n",
    "fitness_1\\big(X_a(t)\\big) = F_1\\big(X_a(t)\\big)\n",
    "$\n",
    "\n",
    "$\n",
    "\\large\n",
    "fitness_2\\big(X_a(t)\\big) = \\dfrac{1}{F_2(X_a(t))}\n",
    "$\n",
    "\n",
    "$\n",
    "\\large\n",
    "fitness\\big(X_a(t)\\big) = F\\big(X_a(t)\\big)\n",
    "$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fitness_1(chromosome, sentences, sim):\n",
    "    return cohesion(chromosome, sentences, sim)\n",
    "\n",
    "def fitness_2(chromosome, sentences, sim):\n",
    "    return 1 / separation(chromosome, sentences, sim)\n",
    "\n",
    "def fitness(chromosome, sentences, sim):\n",
    "    return cohesion_separation(chromosome, sentences, sim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modified Discrete Differential Evolution Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize the population with $N$ chromosomes each composed of $n$ random integers from \\[1, k\\]. __NOTE:__ $t$ is the iteration step.\n",
    "\n",
    "$\n",
    "X_r(t) = [x_{r,1}(t), x_{r,2}(t), ... x_{r,n}(t)]\n",
    "$  \n",
    "\n",
    "where:\n",
    "- $ x_{r,s}(t) \\in \\{1, 2, ..., k\\} $\n",
    "- $ r = 1, 2, ..., N $\n",
    "- $ s = 1, 2, ..., n $\n",
    "- $N$ is the population size\n",
    "- $n$ is the number of sentences (in the document)\n",
    "- $k$ is the number of clusters (number of sentences for summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: create more robust algorithm; however this is only run once for initialization\n",
    "def assign_clusters(num_of_clusters, num_of_sentences):\n",
    "    chromosome = []\n",
    "    while not all(num in chromosome for num in range(num_of_clusters)):\n",
    "        chromosome = np.random.choice(range(num_of_clusters), num_of_sentences)\n",
    "    return chromosome\n",
    "    \n",
    "\n",
    "def initialize_population(population_size, num_of_clusters, num_of_sentences):\n",
    "    if num_of_clusters > num_of_sentences:\n",
    "        raise ValueError('num_of_clusters cannot be greater than num_of_sentences')\n",
    "    population = np.array([assign_clusters(num_of_clusters, num_of_sentences) for _ in range(population_size)])\n",
    "    return population"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\n",
    "\\large\n",
    "y_{r, s}(t+1) = \n",
    "\\begin{cases}\n",
    "    x_{r1,s}(t) + \\lambda\\big(x_{r2,s}(t) - x_{r3,s}(t)\\big) & \\text{if}\\ rand_s < CR \\\\\n",
    "    x_{r,s}(t) & \\text{otherwise}\n",
    "\\end{cases}\n",
    "$\n",
    "\n",
    "where  \n",
    "- For each $X_r$, randomly sample $X_{r1}(t), X_{r2}(t), X_{r3}(t)$ from the same generation (each distinct)\n",
    "- $rand_s$ is uniformally distributed random numbers from \\[0, 1\\] chosen once for each $s \\in \\{1, 2, ..., n\\}$\n",
    "\n",
    "hyper-parameters \n",
    "- $\\lambda$ is a scale factor from \\[0, 1\\]\n",
    "- $CR$ is the crossover rate from \\[0, 1\\]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "###: __NOTES__\n",
    "#: X - population\n",
    "#: Y - offspring\n",
    "#: M - mutation\n",
    "#: X_r - chromosome\n",
    "#: X_r_s - gene\n",
    "#: (t) - current generation\n",
    "#: (t+1) - next generation\n",
    "#: f(*) - fitness function for determining whether X_r(t) or X_r(t+1) survives\n",
    "\n",
    "\n",
    "#TODO: what if new chromosome does not include all clusters? do these just fall out from fitness func?\n",
    "def generate_offspring(population, scaling_factor, crossover_rate):\n",
    "    population_size, num_of_sentences = population.shape\n",
    "    population_idxs = frozenset(range(population_size))\n",
    "    num_of_clusters = np.max(population)\n",
    "    offspring = population.copy()\n",
    "    \n",
    "    for i, chrom in enumerate(population):\n",
    "        #: must select chrom != chrom_l != chrom_m != chrom_n from population\n",
    "        choices = tuple(population_idxs - {i})\n",
    "        idxs = np.random.choice(choices, size=3, replace=False)\n",
    "        chrom_l, chrom_m, chrom_n = population[idxs]\n",
    "        \n",
    "        #TODO: pull out `rand` so it can be used in `mutate()`\n",
    "        rand = np.random.random_sample(num_of_sentences)\n",
    "        chromosomes = zip(chrom, chrom_l, chrom_m, chrom_n, rand)\n",
    "        for j, (gene, gene_l, gene_m, gene_n, rand_s) in enumerate(chromosomes):\n",
    "            if rand_s < crossover_rate:\n",
    "                new_gene = (gene_l + scaling_factor * (gene_m - gene_n)) % num_of_clusters\n",
    "                offspring[i,j] = new_gene\n",
    "\n",
    "    return offspring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 200\n",
    "n = len(sentences)\n",
    "k = n // 3\n",
    "λ = 0.6\n",
    "cr = 0.3\n",
    "criterion = functools.partial(fitness, sentences=sentences, sim=jaccard_similarity)\n",
    "\n",
    "X = initialize_population(N, k, n)\n",
    "Y = generate_offspring(X, scaling_factor=λ, crossover_rate=cr)\n",
    "\n",
    "#: check genes in {1,2,...,k} -> % handles this\n",
    "assert all(gene in range(k) for gene in Y.ravel())\n",
    "\n",
    "#: check genes are integers -> np.array(dtype=int) handles this\n",
    "assert all(gene == int(gene) for gene in Y.ravel())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\n",
    "\\large\n",
    "X_r(t+1) = \n",
    "\\begin{cases}\n",
    "    Y_r(t+1) & \\text{if}\\ f\\big(Y_r(t+1)\\big) > f\\big(X_r(t)\\big) \\\\\n",
    "    X_{r}(t) & \\text{otherwise}\n",
    "\\end{cases}\n",
    "$\n",
    "\n",
    "where \n",
    "- $f(\\cdot)$ is the objective function to be maximized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def next_generation(population, offspring, fitness, scaling_factor, crossover_rate):\n",
    "    next_gen = population.copy()\n",
    "    offspring = generate_offspring(population, scaling_factor, crossover_rate)\n",
    "    for i, (chrom_pop, chrom_off) in enumerate(zip(population, offspring)):\n",
    "        if fitness(chrom_off) > fitness(chrom_pop):\n",
    "            offspring[i] = chrom_off\n",
    "    return next_gen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 2, 0, ..., 2, 1, 1],\n",
       "       [1, 1, 0, ..., 0, 0, 1],\n",
       "       [1, 2, 2, ..., 2, 1, 0],\n",
       "       ...,\n",
       "       [1, 2, 2, ..., 1, 1, 0],\n",
       "       [0, 1, 1, ..., 1, 1, 1],\n",
       "       [2, 1, 1, ..., 2, 1, 1]])"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next_generation(X, Y, criterion, scaling_factor=λ, crossover_rate=cr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mutation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At each iteration $t+1$ for each $X_r(t)$ creates\n",
    "$ m_r(t+1) = [m_{r,1}(t), m_{r,2}(t), ..., m_{r,n}(t)] $.  \n",
    "For each gene, 1 indicates no mutation and 0 means mutate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\n",
    "\\large\n",
    "m_{r,s}(t+1) = \n",
    "\\begin{cases}\n",
    "    1 & \\text{if}\\ rand_s < sigm\\big(y_{r,s}(t+1)\\big) \\\\\n",
    "    0 & \\text{otherwise}\n",
    "\\end{cases}\n",
    "$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mutate(Y, rand):\n",
    "    M = np.empty_like(Y)\n",
    "    for i, (y, rand_s) in enumerate(zip(Y.ravel(), rand.ravel())):\n",
    "        M.ravel()[i] = rand_s < sigmoid(y)\n",
    "\n",
    "mutate(Y, rand)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inversion Operator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Figure 1 - Psuedo-code\n",
    "<img src=\"./data/pngs/fig1_-_inversion_operator_psuedo_code.png\" alt=\"Fig 1. Inverse operator psuedo-code\" width=\"33%\" align=\"left\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inversion_operator(X_r, m_r_p1):\n",
    "    X_r_p1 = X_r.copy()\n",
    "    S = set(i for i, mutate in enumerate(m_r_p1) if not mutate)\n",
    "    \n",
    "    while len(S):\n",
    "        s_min, s_max = min(S), max(S)\n",
    "        X_r_p1[s_min] = X_r[s_max]\n",
    "        X_r_p1[s_max] = X_r[s_min]\n",
    "        S -= {s_max, s_min}\n",
    "    \n",
    "    return X_r_p1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Figure 2 - Example\n",
    "<img src=\"./data/pngs/fig2_-_inversion_operator_diagram.png\" alt=\"Fig 2. Inverse operator example\" width=\"33%\" align=\"left\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_r = [3, 2, 4, 2, 3, 1, 4, 1]\n",
    "m_r_p1 = [0, 1, 1, 0, 1, 0, 0, 1]\n",
    "X_r_p1 = [4, 2, 4, 1, 3, 2, 3, 1]\n",
    "\n",
    "assert inversion_operator(X_r, m_r_p1) == X_r_p1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ROUGE-N\n",
    "(Recall Oriented Understudy for Gisting Evaluation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\n",
    "\\large\n",
    "\\text{ ROUGE-N } = \\dfrac\n",
    "    { \\sum\\limits_{ \\small S \\in Summ_{ref}} \n",
    "        \\sum\\limits_{ \\small \\text{N-gram} \\in S} \n",
    "            Count_{ \\small match}(\\text{N-gram}) }\n",
    "    { \\sum\\limits_{ \\small S \\in Summ_{ref}} \n",
    "        \\sum\\limits_{ \\small \\text{N-gram} \\in S} \n",
    "            Count( \\text{N-gram} ) }\n",
    "$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rouge_n(n, y_pred, y_true):\n",
    "    n_gram_pred = set(ngrams(y_pred, n))\n",
    "    n_gram_true = set(ngrams(y_true, n))\n",
    "    return len(n_gram_pred & n_gram_true) / len(n_gram_true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### testing rouge_n\n",
    "https://rare-technologies.com/text-summarization-in-python-extractive-vs-abstractive-techniques-revisited/#how_to_evaluate_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = 'a good diet must have apples and bananas'.split()\n",
    "y_pred = 'apples and bananas are must for a good diet'.split()\n",
    "\n",
    "assert rouge_n(1, y_pred, y_true) == 7 / 8\n",
    "assert rouge_n(2, y_pred, y_true) == 4 / 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocess Steps:  \n",
    " - stop words\n",
    " - lemmatize"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
