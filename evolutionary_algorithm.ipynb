{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import functools\n",
    "import math\n",
    "import operator\n",
    "import string\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from nltk import tokenize\n",
    "from nltk.util import ngrams\n",
    "\n",
    "from scipy.special import expit as sigmoid\n",
    "# from scipy.stats import wasserstein_distance as earth_movers_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import contextlib\n",
    "import io\n",
    "\n",
    "with io.StringIO() as str_io, contextlib.redirect_stdout(str_io):\n",
    "    import this\n",
    "    zen = str_io.getvalue()\n",
    "\n",
    "del str_io, this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distinct_words(text):\n",
    "    no_punctuation = ''.join(t for t in text if t not in string.punctuation)\n",
    "    return frozenset(tokenize.word_tokenize(no_punctuation))\n",
    "\n",
    "text = zen.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evolutionary Algorithms\n",
    "- [Discrete Differential Evolution for Text Summarization](https://www.researchgate.net/publication/281662415_Discrete_Differential_Evolution_for_Text_Summarization)\n",
    "- [Evolutionary Algorithm for Extractive Text Summarization](https://www.researchgate.net/profile/Ramiz_Aliguliyev/publication/220518077_Evolutionary_Algorithm_for_Extractive_Text_Summarization/links/09e4151356fc2caab6000000.pdf)\n",
    "- [An Improved Evolutionary Algorithm for Extractive Text Summarization](https://link.springer.com/chapter/10.1007/978-3-642-36543-0_9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentence Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let document $D$ be decomposed into a set of $n$ sentences.  \n",
    "$\n",
    "D = \\{S_1, S_2, ..., S_n\\}\n",
    "$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "D = document_sentences = set(tokenize.sent_tokenize(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let terms $T$ be the set of all $m$ distinct words in D.  \n",
    "$\n",
    "T = \\{t_1, t_2, ..., t_m\\}\n",
    "$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "T = document_distinct_words = distinct_words(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let $S_i$ represent the set of distinct terms in sentence $S_i$ with \n",
    "$m_i$ distinct terms.  \n",
    "$\n",
    "S_i = \\{t_1, t_2, ..., t_{m_i}\\}\n",
    "$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "S = sentence_distinct_words = {distinct_words(ds) for ds in document_sentences}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"the zen of python, by tim peters\\n\\nbeautiful is better than ugly.\\nexplicit is better than implicit.\\nsimple is better than complex.\\ncomplex is better than complicated.\\nflat is better than nested.\\nsparse is better than dense.\\nreadability counts.\\nspecial cases aren't special enough to break the rules.\\nalthough practicality beats purity.\\nerrors should never pass silently.\\nunless explicitly silenced.\\nin the face of ambiguity, refuse the temptation to guess.\\nthere should be one-- and preferably only one --obvious way to do it.\\nalthough that way may not be obvious at first unless you're dutch.\\nnow is better than never.\\nalthough never is often better than *right* now.\\nif the implementation is hard to explain, it's a bad idea.\\nif the implementation is easy to explain, it may be a good idea.\\nnamespaces are one honking great idea -- let's do more of those!\\n\""
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.7820114830995408, 0.7820114830995408)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ngd = normalized_google_distance(text)\n",
    "ngd('explicit', 'is'), norm_google_distance('explicit', 'is', D, S)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seed = None  # need reproducibility option\n",
    "\n",
    "# N = len(population)\n",
    "# n = len(sentences)\n",
    "# k = len(clusters)\n",
    "\n",
    "# r = range(N)\n",
    "# s = range(n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Similarity Measures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Jaccard Coefficient Similarity Measure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\n",
    "\\large\n",
    "sim_{jaccard}(S_i, S_j) = \\frac{|S_i \\cap S_j|}{|S_i \\cup S_j|}\n",
    "$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jaccard_similarity(a, b):\n",
    "    a, b = set(a), set(b)\n",
    "    if not a and not b:\n",
    "        return 1.0\n",
    "    return len(a & b) / len(a | b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalized Google Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\n",
    "\\large\n",
    "NGD(t_k, t_l) = \\frac{max\\{log(f_k), log(f_l)\\} - log(f_{lk})} {log(n) - min\\{log(f_k), log(f_l)\\}}\n",
    "$  \n",
    "\n",
    "where:\n",
    "- $t_k$ and $t_l$ are terms \n",
    "- $f_k$ is the number of sentences containing $t_k$\n",
    "- $f_{kl}$ is the number of sentences containing both $t_k$ and $t_l$\n",
    "- $n$ is the total number of sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# double check scientific paper's handling of \"bad\" log values\n",
    "\n",
    "def normalized_google_distance(document):\n",
    "    def ngd(t_k, t_l):\n",
    "        f_k = sum(t_k in sent for sent in sentence_words)\n",
    "        f_l = sum(t_l in sent for sent in sentence_words)\n",
    "        if not (f_k and f_l):\n",
    "            raise ValueError('terms must be in document')\n",
    "\n",
    "        f_kl = sum((t_k in sent) and (t_l in sent) for sent in sentence_words)\n",
    "        if (f_k > 0) and (f_l > 0) and (f_kl == 0):\n",
    "            return 1.0\n",
    "\n",
    "        log_kl = (math.log(f_k), math.log(f_l))\n",
    "\n",
    "        numerator = max(log_kl) - math.log(f_kl)\n",
    "        denominator = math.log(n) - min(log_kl)\n",
    "        return numerator / denominator\n",
    "    \n",
    "    sentence_words = tuple(frozenset(sent.split()) for sent in tokenize.sent_tokenize(document))\n",
    "    n = len(sentence_words)\n",
    "    return ngd\n",
    "\n",
    "\n",
    "# def norm_google_distance(t_k, t_l, D, S):\n",
    "#     \"\"\"Metric for distance between two terms-- tₖ, tₗ\"\"\"\n",
    "    \n",
    "#     f_k = sum(t_k in sent for sent in S)\n",
    "#     f_l = sum(t_l in sent for sent in S)\n",
    "#     if not (f_k and f_l):\n",
    "#         raise ValueError('terms must be in document')\n",
    "    \n",
    "#     f_kl = sum((t_k in sent) and (t_l in sent) for sent in S)\n",
    "#     if (f_k > 0) and (f_l > 0) and (f_kl == 0):\n",
    "#         return 1.0\n",
    "    \n",
    "#     log_kl = (math.log(f_k), math.log(f_l))\n",
    "#     n = len(D)\n",
    "    \n",
    "#     numerator = max(log_kl) - math.log(f_kl)\n",
    "#     denominator = math.log(n) - min(log_kl)\n",
    "#     return numerator / denominator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\n",
    "\\large \n",
    "sim_{NGD}(t_k, t_l) = e^{-NGD(t_k, t_l)}\n",
    "$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def norm_google_similarity_term(t_k, t_l, D, S):\n",
    "    \"\"\"Metric for similarity between two terms-- tₖ, tₗ\"\"\"\n",
    "    \n",
    "    ngd = normalized_google_distance(t_k, t_l, D, S)\n",
    "    return math.exp(-ngd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\n",
    "\\large \n",
    "sim_{NGD}(S_i, S_j) = \\frac{ \\sum\\limits_{t_k \\in S_i} \\sum\\limits_{t_l \\in S_j} sim_{NGD}(t_k, t_l) }{ m_i m_j }\n",
    "$  \n",
    "\n",
    "where:\n",
    "- $S_i$ and $S_j$ are sentences\n",
    "- $m_i$ is the number of words in $S_i$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "def norm_google_similarity_sent(S_i, S_j, D, S):\n",
    "    total = sum(sum(norm_google_similarity_term(t_k, t_l, D, S) for t_l in S_j) for t_k in S_i)\n",
    "    return total / len(S_i) / len(S_j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if tₖ == tₗ --> 1\n",
    "assert norm_google_similarity_term('python', 'python', D, S) == 1\n",
    "\n",
    "# if (tₖ != tₗ) and (fₖ == fₗ == fₖₗ > 0) --> 1\n",
    "assert norm_google_similarity_term('explicit', 'implicit', D, S) == 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objective Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let $C$ be a partition of D with $k$ clusters.  \n",
    "$\n",
    "C = \\{C_1, C_2, ..., C_k\\}\n",
    "$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C = k_clusters = {frozenset(), ...}\n",
    "#: 1) Two different clusters should have no sentences in common\n",
    "# assert all(not C_i & C_j for C_i, C_j in itertools.combinations(C, C))\n",
    "\n",
    "#: 2) Each sentence should definitely be attached to a cluster\n",
    "# assert functools.reduce(operator.or_, C) == D\n",
    "\n",
    "#: 3) Each cluster should have at least one sentence assigned\n",
    "# assert all(C_p for C_p in C)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sigmoid Function\n",
    "$\n",
    "\\large\n",
    "sigm(x) = \\frac{ 1 }{ 1 + e^{-x} }\n",
    "$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.special import expit as sigmoid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Intra-Cluster Similiarity (Cohesion)\n",
    "$\n",
    "\\large\n",
    "F_1 = \\sum\\limits_{p=1}^{k} |C_p| \\sum\\limits_{S_i, S_j \\in C_p} sim(S_i, S_j) \\rightarrow max\n",
    "$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one paper shows 1/|Cp| and another shows 1/|Cp|^2. Both that show just |Cp| are the two orig ones i found\n",
    "\n",
    "def cohesion(C, sim):\n",
    "    total = 0\n",
    "    for C_p in C:\n",
    "        for S_i, S_j in itertools.product(C_p, repeat=2):  # should i use itertools.combinations? sim is symmetric\n",
    "            total += sim(S_i, S_j) / len(C_p)\n",
    "    return total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inter-Cluster Dissimilarity (Separation)\n",
    "$\n",
    "\\large\n",
    "F_2 = \n",
    "\\sum\\limits_{p=1}^{k-1} \\frac{1}{|C_p|} \n",
    "\\sum\\limits_{q=p+1}^{k} \\frac{1}{|C_q|} \n",
    "\\sum\\limits_{S_i \\in C_p} \n",
    "\\sum\\limits_{S_l \\in C_q} sim(S_i, S_l) \n",
    "\\rightarrow min\n",
    "$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "def separation(C, sim):\n",
    "    total = 0\n",
    "    for C_p, C_q in itertools.combinations(C, r=2):  # this is Σ(p=1 to k-1) and Σ(q=p+1 to k)\n",
    "        for S_i, S_j in itertools.product(C_p, C_q):    # should i use itertools.combinations?\n",
    "            total += sim(S_i, S_j) / len(C_p) / len(C_q)  # need D, S args to sim unless partial it in algorithm\n",
    "    return total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Criterion Function\n",
    "$\n",
    "\\large\n",
    "F = (1 + sigm(F_1))^{F_2} \\rightarrow max\n",
    "$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def criterion_function(C, sim):\n",
    "    coh = cohesion(C, sim)\n",
    "    sep = separation(C, sim)\n",
    "    return pow(1 + sigmoid(coh), sep)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modified Discrete Differential Evolution Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize the population with $N$ chromosomes each composed of $n$ random integers from \\[1, k\\]. __NOTE:__ $t$ is the iteration step.\n",
    "\n",
    "$\n",
    "X_r(t) = [x_{r,1}(t), x_{r,2}(t), ... x_{r,n}(t)]\n",
    "$  \n",
    "\n",
    "where:\n",
    "- $ x_{r,s}(t) \\in \\{1, 2, ..., k\\} $\n",
    "- $ r = 1, 2, ..., N $\n",
    "- $ s = 1, 2, ..., n $\n",
    "- $N$ is the population size\n",
    "- $n$ is the number of sentences (in the document)\n",
    "- $k$ is the number of clusters (number of sentences for summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[7, 1, 2, 2, 5, 3, 0, 0, 1, 5]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.choices(range(8), k=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2, 0, 3, 1, 2, 2, 2, 3],\n",
       "       [0, 0, 2, 2, 3, 3, 2, 1],\n",
       "       [3, 0, 1, 0, 3, 0, 2, 0],\n",
       "       [1, 2, 1, 3, 0, 2, 0, 0],\n",
       "       [0, 3, 1, 3, 2, 2, 1, 1],\n",
       "       [0, 0, 2, 0, 1, 3, 0, 0]])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def partition(k, n):  #TODO: create more robust algorithm; however this is only run once for initialization\n",
    "    X_r = []\n",
    "    while not all(i in X_r for i in range(k)):\n",
    "        X_r = np.random.choice(range(k), n)\n",
    "    return X_r\n",
    "    \n",
    "\n",
    "def initialize_population(N, k, n):\n",
    "    assert k <= n\n",
    "    X = np.array([partition(k, n) for _ in range(N)])\n",
    "    return X\n",
    "\n",
    "\n",
    "X = initialize_population(6, 4, 8)\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\n",
    "\\large\n",
    "y_{r, s}(t+1) = \n",
    "\\begin{cases}\n",
    "    x_{r1,s}(t) + \\lambda[x_{r2,s}(t) - x_{r3,s}(t)] & \\text{if}\\ rand_s < CR \\\\\n",
    "    x_{r,s}(t) & \\text{otherwise}\n",
    "\\end{cases}\n",
    "$\n",
    "\n",
    "where  \n",
    "- For each $X_r$, randomly sample $X_{r1}(t), X_{r2}(t), X_{r3}(t)$ from the same generation (each distinct)\n",
    "- $rand_s$ is uniformally distributed random numbers from \\[0, 1\\] chosen once for each $s \\in \\{1, 2, ..., n\\}$\n",
    "\n",
    "hyper-parameters \n",
    "- $\\lambda$ is a scale factor from \\[0, 1\\]\n",
    "- $CR$ is the crossover rate from \\[0, 1\\]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2,  0,  1,  1,  4,  2,  2,  3],\n",
       "       [ 0,  0,  2,  2,  3,  0,  2,  1],\n",
       "       [ 3,  0,  1,  0,  3,  0,  2,  0],\n",
       "       [ 1,  2,  1, -1,  0,  1,  0,  0],\n",
       "       [ 0,  3,  1,  3,  2,  2,  1,  1],\n",
       "       [ 2,  0,  2,  0,  1,  3,  0,  0]])"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# how to handle new X_r(t+1) clusters to be in [1,...,k]?\n",
    "def offspring(X, scaling_factor, crossover_rate):\n",
    "    r, s = X.shape\n",
    "    Y = np.empty_like(X)\n",
    "    for i, X_r in enumerate(X):\n",
    "        choices = tuple(set(range(r)) - {i})\n",
    "        l, m, n = np.random.choice(choices, size=3, replace=False)\n",
    "        X_l, X_m,X_n = X[l], X[m], X[n]\n",
    "        rand = np.random.random_sample(s)  # pull out so it can be used in mutate()\n",
    "        for j, (x, x_l, x_m, x_n, rand_s) in enumerate(zip(X_r, X_l, X_m, X_n, rand)):\n",
    "            if rand_s < crossover_rate:\n",
    "                y = x_l + scaling_factor * (x_m - x_n)\n",
    "            else:\n",
    "                y = x\n",
    "            Y[i, j] = y\n",
    "    return Y\n",
    "\n",
    "Y = offspring(X, scaling_factor=0.6, crossover_rate=0.2)\n",
    "Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\n",
    "\\large\n",
    "X_r(t+1) = \n",
    "\\begin{cases}\n",
    "    Y_r(t+1) & \\text{if}\\ f(Y_r(t+1)) > f(X_r(t)) \\\\\n",
    "    X_{r}(t) & \\text{otherwise}\n",
    "\\end{cases}\n",
    "$\n",
    "\n",
    "where \n",
    "- $f(\\cdot)$ is the objective function to be maximized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "def next_generation(X, Y, func):\n",
    "    next_gen = X.copy()\n",
    "    Y = offspring(X, scaling_factor, crossover_rate)\n",
    "    for i, X_r, Y_r in enumerate(zip(X, Y)):\n",
    "        if func(Y_r) > func(X_r):\n",
    "            Y[i] = Y_r\n",
    "    return next_gen\n",
    "\n",
    "next_generation(X, Y, func=fitness(...))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitness\n",
    "<img src=\"./data/pngs/fitness.png\" alt=\"inverse operator psuedo-code\" width=\"33%\" align=\"left\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fitness_1(X_a):\n",
    "    return cohesion(X_a)\n",
    "\n",
    "def fitness_2(X_a):\n",
    "    return 1 / separation(X_a)\n",
    "\n",
    "def fitness(X_a):\n",
    "    return criterion_function(X_a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mutation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At each iteration $t+1$ for each $X_r(t)$ creates\n",
    "$ m_r(t+1) = [m_{r,1}(t), m_{r,2}(t), ..., m_{r,n}(t)] $.  \n",
    "For each gene, 1 indicates no mutation and 0 means mutate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\n",
    "\\large\n",
    "m_{r,s}(t+1) = \n",
    "\\begin{cases}\n",
    "    1 & \\text{if}\\ rand_s < sigm(y_{r,s}(t+1)) \\\\\n",
    "    0 & \\text{otherwise}\n",
    "\\end{cases}\n",
    "$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mutate(Y, rand):\n",
    "    M = np.empty_like(Y)\n",
    "    for i, (y, rand_s) in enumerate(zip(Y.ravel(), rand.ravel())):\n",
    "        M.ravel()[i] = rand_s < sigmoid(y)\n",
    "\n",
    "mutate(Y, rand)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inversion Operator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Figure 1 - Psuedo-code\n",
    "<img src=\"./data/pngs/fig1_-_inversion_operator_psuedo_code.png\" alt=\"inverse operator psuedo-code\" width=\"33%\" align=\"left\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inversion_operator(X_r, m_r_p1):\n",
    "    X_r_p1 = X_r.copy()\n",
    "    S = set(i for i, mutate in enumerate(m_r_p1) if not mutate)\n",
    "    \n",
    "    while len(S):\n",
    "        s_min, s_max = min(S), max(S)\n",
    "        X_r_p1[s_min] = X_r[s_max]\n",
    "        X_r_p1[s_max] = X_r[s_min]\n",
    "        S -= {s_max, s_min}\n",
    "    \n",
    "    return X_r_p1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Figure 2 - Example\n",
    "<img src=\"./data/pngs/fig2_-_inversion_operator_diagram.png\" alt=\"inverse operator example\" width=\"33%\" align=\"left\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_r = [3, 2, 4, 2, 3, 1, 4, 1]\n",
    "m_r_p1 = [0, 1, 1, 0, 1, 0, 0, 1]\n",
    "X_r_p1 = [4, 2, 4, 1, 3, 2, 3, 1]\n",
    "\n",
    "assert inversion_operator(X_r, m_r_p1) == X_r_p1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ROUGE-N\n",
    "(Recall Oriented Understudy for Gisting Evaluation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\n",
    "\\large\n",
    "ROUGE-N = \\frac\n",
    "{ \\sum\\limits_{S \\in Summ_{ref}} \\sum\\limits_{N-gram \\in S} Count_{match}(N-gram) }\n",
    "{ \\sum\\limits_{S \\in Summ_{ref}} \\sum\\limits_{N-gram \\in S} Count(N-gram) }\n",
    "$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rouge_n(n, y_pred, y_true):\n",
    "    n_gram_pred = set(ngrams(y_pred, n))\n",
    "    n_gram_true = set(ngrams(y_true, n))\n",
    "    return len(n_gram_pred & n_gram_true) / len(n_gram_true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### testing rouge_n\n",
    "https://rare-technologies.com/text-summarization-in-python-extractive-vs-abstractive-techniques-revisited/#how_to_evaluate_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = 'a good diet must have apples and bananas'.split()\n",
    "y_pred = 'apples and bananas are must for a good diet'.split()\n",
    "\n",
    "assert rouge_n(1, y_pred, y_true) == 7 / 8\n",
    "assert rouge_n(2, y_pred, y_true) == 4 / 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "set()"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = \"however doctors learned that long inactivity did more harm than good\".split()\n",
    "b = \"patients got out of shape developed blood clots and became demoralized\".split()\n",
    "\n",
    "set(a) & set(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jaccard_similarity(a, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocess Steps:  \n",
    " - stop words\n",
    " - lemmatize"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
