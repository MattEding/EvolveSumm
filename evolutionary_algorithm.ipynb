{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import itertools\n",
    "import functools\n",
    "import math\n",
    "# import multiprocessing\n",
    "import operator\n",
    "import string\n",
    "# import random\n",
    "\n",
    "import numba\n",
    "import numpy as np\n",
    "# import pandas as pd\n",
    "\n",
    "from nltk import tokenize\n",
    "from nltk.util import ngrams\n",
    "\n",
    "# from scipy.spatial.distance import jaccard\n",
    "# from scipy.special import expit as sigmoid\n",
    "\n",
    "# from sklearn.metrics import jaccard_similarity_score\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import euclidean_distances, cosine_distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pathlib\n",
    "\n",
    "cwd = pathlib.Path.cwd()\n",
    "data = cwd / 'data'\n",
    "jsons = data / 'jsons'\n",
    "json_2018 = jsons / '2018'\n",
    "\n",
    "json_2018 = list(json_2018.iterdir())[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(json_2018) as fp:\n",
    "    articles_2018 = json.load(fp)['2018']\n",
    "\n",
    "article = articles_2018[4]\n",
    "text = article['story']\n",
    "summ_true = article['summary']\n",
    "# print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evolutionary Algorithms\n",
    "- [Discrete Differential Evolution for Text Summarization](https://www.researchgate.net/publication/281662415_Discrete_Differential_Evolution_for_Text_Summarization)\n",
    "- [Evolutionary Algorithm for Extractive Text Summarization](https://www.researchgate.net/profile/Ramiz_Aliguliyev/publication/220518077_Evolutionary_Algorithm_for_Extractive_Text_Summarization/links/09e4151356fc2caab6000000.pdf)\n",
    "- [An Improved Evolutionary Algorithm for Extractive Text Summarization](https://link.springer.com/chapter/10.1007/978-3-642-36543-0_9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentence Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distinct_words(text):\n",
    "    no_punctuation = ''.join(t for t in text if t not in string.punctuation)\n",
    "    return frozenset(tokenize.word_tokenize(no_punctuation))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let document $ D $ be decomposed into a set of $ n $ sentences.  \n",
    "$\n",
    "D = \\{ S_1, S_2, ..., S_n \\}\n",
    "$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "D = document_sentences = set(tokenize.sent_tokenize(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let terms $ T $ be the set of all $ m $ distinct words in $ D $.  \n",
    "$\n",
    "T = \\{ t_1, t_2, ..., t_m \\}\n",
    "$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "T = document_distinct_words = distinct_words(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let $ S_i $ represent the set of distinct terms in sentence $ S_i $ with \n",
    "$ m_i $ distinct terms.  \n",
    "$\n",
    "S_i = \\{ t_1, t_2, ..., t_{ m_i } \\}\n",
    "$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "S = sentence_distinct_words = {distinct_words(ds) for ds in document_sentences}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Similarity Measures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Jaccard Coefficient Similarity Measure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\n",
    "\\large\n",
    "sim_{ jaccard } ( S_i, S_j ) = \n",
    "\\dfrac\n",
    "    { | S_i \\cap S_j | }\n",
    "    { | S_i \\cup S_j | }\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalized Google Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\n",
    "\\large\n",
    "\\text{NGD} ( t_k, t_l ) = \\dfrac \n",
    "    { \\text{max} \\big\\{\n",
    "        \\text{log} ( f_k ), \n",
    "        \\text{log} ( f_l )\n",
    "    \\big\\}\n",
    "    - \\text{log} ( f_{ lk } ) }\n",
    "    { \\text{log} ( n ) \n",
    "    - \\text{min} \\big\\{\n",
    "        \\text{log} ( f_k ), \n",
    "        \\text{log} ( f_l )\n",
    "    \\big\\} }\n",
    "$\n",
    "\n",
    "where:\n",
    "- $ t_k $ and $ t_l $ are terms \n",
    "- $ f_k $ is the number of sentences containing $ t_k $\n",
    "- $ f_{ kl } $ is the number of sentences containing both $ t_k $ and $ t_l $\n",
    "- $ n $ is the total number of sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\n",
    "\\large \n",
    "sim_{ \\text{NGD} } ( t_k, t_l ) = \\text{exp} \n",
    "    \\big( - \\text{NGD} ( t_k, t_l ) \\big)\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\n",
    "\\large \n",
    "sim_{ \\text{NGD} } ( S_i, S_j ) = \n",
    "\\dfrac\n",
    "    { \\sum\\limits\n",
    "        _{ \\small t_k \\in S_i } \n",
    "        \\sum\\limits\n",
    "            _{ \\small t_l \\in S_j } \n",
    "            sim_{ \\text{NGD} } ( t_k, t_l ) }\n",
    "    { m_i m_j }\n",
    "$  \n",
    "\n",
    "where:\n",
    "- $ S_i $ and $ S_j $ are sentences\n",
    "- $ m_i $ is the number of words in $ S_i $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NormalizedGoogle:\n",
    "    def __init__(self, document):\n",
    "        self.sentence_words = tuple(distinct_words(sent) for sent in tokenize.sent_tokenize(document))\n",
    "        \n",
    "    # double check scientific paper's handling of \"bad\" log values\n",
    "    def distance(self, term_k, term_l):\n",
    "        freq_k = sum(term_k in sent for sent in self.sentence_words)\n",
    "        freq_l = sum(term_l in sent for sent in self.sentence_words)\n",
    "        if not (freq_k and freq_l):\n",
    "            raise ValueError('terms must be in document')\n",
    "\n",
    "        freq_kl = sum((term_k in sent) and (term_l in sent) for sent in self.sentence_words)\n",
    "        if (freq_k > 0) and (freq_l > 0) and (freq_kl == 0):\n",
    "            return 1.0\n",
    "\n",
    "        logs_k_l = (math.log(freq_k), math.log(freq_l))\n",
    "        n = len(self.sentence_words)\n",
    "\n",
    "        numerator = max(logs_k_l) - math.log(freq_kl)\n",
    "        denominator = math.log(n) - min(logs_k_l)\n",
    "        return numerator / denominator\n",
    "    \n",
    "    def term_similarity(self, term_k, term_l):\n",
    "        dist = self.distance(term_k, term_l)\n",
    "        return math.exp(-dist)\n",
    "    \n",
    "    def sentence_similarity(self, sent_i, sent_j):\n",
    "        total = sum(self.term_similarity(term_k, term_l)\n",
    "                    for term_k, term_l in itertools.product(sent_i, sent_j))\n",
    "        return total / len(sent_i) / len(sent_j)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objective Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let $ C $ be a partition of $ D $ with $ k $ clusters.  \n",
    "$ C = \\{ C_1, C_2, ..., C_k \\} $  \n",
    "\n",
    "where:\n",
    "- $ C_p \\cap C_q = \\emptyset, \n",
    "        \\forall p \\ne q \\in \\{ 1, 2, ..., k \\} \n",
    "  $\n",
    "- $ \\bigcup\\limits\n",
    "    _{ p = 1 }\n",
    "    ^k C_p = D \n",
    "  $\n",
    "- $ C_p \\ne \\emptyset, \n",
    "        \\forall p \\in \\{ 1, 2, ..., k \\}\n",
    "  $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def verify_clusters(clusters, document):\n",
    "    disjoint = all(not cluster_i & cluster_j for cluster_i, cluster_j in itertools.combinations(clusters, r=2))\n",
    "    union = functools.reduce(operator.or_, clusters) == document\n",
    "    nonempty = all(cluster for cluster in clusters)\n",
    "    if not (disjoint and union and nonempty):\n",
    "        raise ValueError('clusters do not form a partition')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clusterize(chromosome):\n",
    "    partition = collections.defaultdict(list)\n",
    "    for i, cluster in enumerate(chromosome):\n",
    "        partition[cluster].append(i)\n",
    "    return partition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sigmoid Function\n",
    "$\n",
    "\\large\n",
    "sigm ( x ) = \n",
    "\\dfrac\n",
    "    { 1 }\n",
    "    { 1 + \\text{exp} ( -x ) }\n",
    "$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.special import expit as sigmoid\n",
    "# interesting this is 3x faster than my implimentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Intra-Cluster Similiarity (Cohesion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\n",
    "\\large\n",
    "F_1 = \n",
    "\\sum\\limits\n",
    "    _{ \\small p = 1 }\n",
    "    ^{ \\small k } \n",
    "\\sum\\limits\n",
    "    _{ \\small S_i, S_j \\in C_p } \n",
    "\\dfrac \n",
    "    { sim ( S_i, S_j ) } \n",
    "    { | C_p | } \n",
    "\\rightarrow \\text{max}\n",
    "$\n",
    "\n",
    "*(__Note:__ Evol Alg for Ext Txt Summ doesn't show division of $|C_p|$ but I believe this to be a typo since the paragraph detailing it says \"the average sum\". Additionally the Disc Diff Evol for Txt Summ shows it as such.)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inter-Cluster Dissimilarity (Separation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\n",
    "\\large\n",
    "F_2 = \n",
    "\\sum\\limits\n",
    "    _{ \\small p = 1 }\n",
    "    ^{ \\small k - 1 }\n",
    "\\sum\\limits\n",
    "    _{ \\small q = p + 1 }\n",
    "    ^{ \\small k } \n",
    "\\sum\\limits\n",
    "    _{ \\small S_i \\in C_p } \n",
    "\\sum\\limits\n",
    "    _{ \\small S_j \\in C_q } \n",
    "\\dfrac \n",
    "    { sim ( S_i, S_j ) } \n",
    "    { | C_p | \\cdot | C_q | }\n",
    "\\rightarrow \\text{min}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inter/Intra-Cluster Balance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\n",
    "\\large\n",
    "F = \\big( 1 + sigm ( F_1 ) \\big)\n",
    "    ^{ F_2 } \\rightarrow \\text{max}\n",
    "$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer()\n",
    "vec = cv.fit_transform(tokenize.sent_tokenize(text))\n",
    "\n",
    "doc = vec.toarray().astype(bool).astype(int)\n",
    "chrom = np.array([0, 2, 1, 1, 3, 2, 4, 1])\n",
    "\n",
    "\n",
    "def cohesion(chromosome, sim, doc):\n",
    "    total = 0\n",
    "    for p in np.unique(chromosome):\n",
    "        cluster_p = doc[np.where(chromosome == p)]\n",
    "        for sent_i, sent_j in itertools.combinations(cluster_p, r=2):\n",
    "            total += sim(sent_i, sent_j) / len(cluster_p)\n",
    "    return total\n",
    "\n",
    "\n",
    "def separation(chromosome, sim, doc):\n",
    "    total = 0\n",
    "    for p, q in itertools.combinations(np.unique(chromosome), r=2):\n",
    "        cluster_p = doc[np.where(chromosome == p)]\n",
    "        cluster_q = doc[np.where(chromosome == q)]\n",
    "        for sent_i, sent_j in itertools.product(cluster_p, cluster_q):\n",
    "            total += sim(sent_i, sent_j) / len(cluster_p) / len(cluster_q)\n",
    "    return total\n",
    "\n",
    "def cohesion_separation(chromosome, sim, doc):\n",
    "    coh = cohesion(chromosome, sim, doc)\n",
    "    sep = separation(chromosome, sim, doc)\n",
    "    return pow(1 + sigmoid(coh), sep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "@numba.njit\n",
    "def jaccard_sim(a, b):\n",
    "    #: assume union is non-empty since each sentence >= 1 word\n",
    "    return np.sum(a & b) / np.sum(a | b)\n",
    "\n",
    "\n",
    "@numba.njit\n",
    "def sigmoid2(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "\n",
    "@numba.njit\n",
    "def cohesion2(chromosome, sim, doc):\n",
    "    total = 0\n",
    "    for p in np.unique(chromosome):\n",
    "        sents = doc[np.where(chromosome == p)]\n",
    "        k = len(sents)\n",
    "        #: combinations choose 2\n",
    "        for i in range(k-1):\n",
    "            for j in range(i+1, k):\n",
    "                total += sim(sents[i], sents[j]) / len(sents)  \n",
    "    return total\n",
    "\n",
    "\n",
    "\n",
    "@numba.njit\n",
    "def separation2(chromosome, sim, doc):\n",
    "    total = 0\n",
    "    k = len(np.unique(chromosome))\n",
    "    #: combinations choose 2\n",
    "    for p in range(k-1):\n",
    "        for q in range(p+1, k):\n",
    "            sents_p = doc[np.where(chromosome == p)]\n",
    "            sents_q = doc[np.where(chromosome == q)]\n",
    "            #: product\n",
    "            m, n = len(sents_p), len(sents_q)\n",
    "            for i in range(m):\n",
    "                for j in range(n):\n",
    "                    total += sim(sents_p[i], sents_q[j]) / m / n\n",
    "    return total\n",
    "\n",
    "\n",
    "@numba.njit\n",
    "def cohesion_separation2(chromosome, sim, doc):\n",
    "    coh = cohesion2(chromosome, sim, doc)\n",
    "    sep = separation2(chromosome, sim, doc)\n",
    "    return (1 + sigmoid2(coh)) ** sep\n",
    "\n",
    "\n",
    "\n",
    "cv = CountVectorizer(stop_words='english')\n",
    "sents = tokenize.sent_tokenize(text.lower())\n",
    "vec = cv.fit_transform(sents)\n",
    "\n",
    "doc = vec.toarray().astype(bool).astype(int)\n",
    "chrom = np.array([0, 2, 1, 1, 3, 2, 4, 1])\n",
    "\n",
    "assert cohesion2(chrom, jaccard_sim, doc) == cohesion(chrom, jaccard_sim, doc)\n",
    "assert separation2(chrom, jaccard_sim, doc) == separation(chrom, jaccard_sim, doc)\n",
    "assert cohesion_separation2(chrom, jaccard_sim, doc) == cohesion_separation(chrom, jaccard_sim, doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitness"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\n",
    "\\large\n",
    "fitness_1 \\big( X_a ( t ) \\big) = F_1 \\big( X_a ( t ) \\big)\n",
    "$\n",
    "\n",
    "$\n",
    "\\large\n",
    "fitness_2 \\big( X_a ( t ) \\big) = \\dfrac{ 1 }{ F_2 ( X_a ( t ) ) }\n",
    "$\n",
    "\n",
    "$\n",
    "\\large\n",
    "fitness \\big( X_a ( t ) \\big) = F \\big( X_a ( t ) \\big)\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modified Discrete Differential Evolution Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize the population with $ N $ chromosomes each composed of $ n $ random integers from \\[1, k\\].  \n",
    "\n",
    "$\n",
    "X_r ( t ) = [ x_{ r, 1 } ( t ), x_{ r, 2 } ( t ), ..., x_{ r, n } ( t ) ]\n",
    "$  \n",
    "\n",
    "where:\n",
    "- $ x_{ r, s } ( t ) \\in \\{ 1, 2, ..., k \\} $\n",
    "- $ r = 1, 2, ..., N $\n",
    "- $ s = 1, 2, ..., n $\n",
    "- $ N $ is the population size\n",
    "- $ n $ is the number of sentences _(in the document)_\n",
    "- $ k $ is the number of clusters _(number of sentences for summary)_\n",
    "- $ t $ is the iteration step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\n",
    "\\large\n",
    "y_{ r, s } ( t + 1 ) = \n",
    "\\begin{cases}\n",
    "    x_{ r1, s } ( t ) + \\lambda \\big( x_{ r2, s } ( t ) - x_{ r3, s } ( t ) \\big) \n",
    "        & \\text{if } rand_s < \\text{CR} \\\\\n",
    "    x_{ r, s } ( t ) & \\text{otherwise}\n",
    "\\end{cases}\n",
    "$\n",
    "\n",
    "where  \n",
    "- For each $ X_r $, randomly sample $ X_{ r1 } ( t ), X_{ r2 } ( t ), X_{ r3 } ( t ) $ \n",
    "  from the same generation _(each distinct)_\n",
    "- $ rand_s $ is uniformally distributed random numbers from $ [ 0, 1 ] $ \n",
    "  chosen once for each $ s \\in \\{ 1, 2, ..., n \\} $\n",
    "\n",
    "hyper-parameters \n",
    "- $ \\lambda $ is a scale factor from $ [ 0, 1 ] $\n",
    "- $ \\text{CR} $ is the crossover rate from $ [ 0, 1 ] $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_chromosome(choices, length):\n",
    "    chrom = np.full(length, -1)\n",
    "    #: ensure that each choice is accounted for at least once\n",
    "    idxs = np.random.choice(np.arange(length), len(choices), replace=False)\n",
    "    chrom[idxs] = np.random.permutation(choices)\n",
    "    idxs = np.where(chrom == -1)[0]\n",
    "    chrom[idxs] = np.random.choice(choices, len(idxs))\n",
    "    return chrom\n",
    "\n",
    "\n",
    "def init_population(pop_size, clust_amt, chrom_len):\n",
    "    clusts = np.arange(clust_amt)\n",
    "    chroms = [init_chromosome(clusts, chrom_len) for _ in range(pop_size)]\n",
    "    pop = np.vstack(chroms)\n",
    "    return pop\n",
    "    \n",
    "\n",
    "def get_offspring(pop, rand, lam, cr):\n",
    "    #: For computation time, relax requirement that X_r, X_r1, X_r2, X_r3 are distinct. \n",
    "    #: With large population size, this is unlikely to occur, and if it does, it doesn't\n",
    "    #: seem that detrimental. Also is this mitigated with appropriate lam choice?\n",
    "    n = len(pop)\n",
    "    idxs = np.random.choice(np.arange(n), size=(n, 3))\n",
    "    chrom_1, chrom_2, chrom_3 = map(np.squeeze, np.split(pop[idxs], 3, axis=1))\n",
    "    k = len(np.unique(pop))\n",
    "    offspr = (chrom_1 + lam * (chrom_2 - chrom_3)) % k\n",
    "    mask = rand < cr\n",
    "    offspr[mask] = pop[mask]\n",
    "    return offspr\n",
    "\n",
    "\n",
    "# @numba.njit(parallel=True)\n",
    "# def get_mask(pop, offspr, func):\n",
    "#     fit_pop = np.empty_like(pop)\n",
    "#     fit_off = np.empty_like(offspr)\n",
    "#     for i in numba.prange(len(pop)):\n",
    "#         fit_pop[i] = func(pop[i])\n",
    "#         fit_off[i] = func(offspr[i])\n",
    "\n",
    "#     mask = fit_off > fit_pop\n",
    "#     return mask\n",
    "\n",
    "# def next_generation(pop, offspr, func):\n",
    "#     mask = get_mask(pop, offspr, func)\n",
    "#     pop[mask] = offspr[mask]\n",
    "#     return\n",
    "\n",
    "\n",
    "def next_generation(pop, offspr, func):\n",
    "    fit_off = np.array([func(chrom) for chrom in offspr])\n",
    "    fit_pop = np.array([func(chrom) for chrom in pop])\n",
    "    mask = fit_off > fit_pop\n",
    "    pop[mask] = offspr[mask]\n",
    "    return\n",
    "\n",
    "\n",
    "# def next_generation(pop, offspr, func):\n",
    "#     fit_off = func(offspr)\n",
    "#     fit_pop = func(pop)\n",
    "#     mask = fit_off > fit_pop\n",
    "#     pop[mask] = offspr[mask]\n",
    "#     return\n",
    "\n",
    "\n",
    "def mutate(pop, rand):\n",
    "    mask = rand < sigmoid(pop)\n",
    "    idxs = np.nonzero(mask)\n",
    "    #: reverse indices for each row\n",
    "    rev = np.vstack(idxs).T\n",
    "    rev = np.array(sorted(rev, key=lambda x: (x[0], -x[1])))\n",
    "    rev = tuple(np.split(rev.T, 2))\n",
    "    pop[idxs] = pop[rev]\n",
    "    return\n",
    "\n",
    "\n",
    "#TODO: early stopping --> little fitness improvement over x generations, good enough fitness score\n",
    "def run_iterations(pop_size, summ_len, num_sents, func, lam, cr, iterations, *, mutate_after=True,\n",
    "                   seed=None, verbose=False, save_rate=np.nan, save_dir=None):\n",
    "    \n",
    "    if save_dir is not None:\n",
    "        save_dir = pathlib.Path(save_dir)\n",
    "        if not save_dir.is_dir():\n",
    "            msg = f'save_dir={save_dir} not a valid directory path'.format(save_dir=save_dir)\n",
    "            raise NotADirectoryError(msg)\n",
    "    \n",
    "    if seed is not None:\n",
    "        np.random.seed(seed)\n",
    "    \n",
    "    pop = init_population(pop_size, summ_len, num_sents)\n",
    "    shape = pop.shape\n",
    "    for i in range(iterations):\n",
    "        if i % save_rate == 0:\n",
    "            file = save_dir / 'generation_{i:0>pad}'.format(i=i, pad=len(str(iterations)))\n",
    "            np.save(file, pop)\n",
    "            \n",
    "        if verbose:\n",
    "            print(i)  #TODO: logfile --> iteration number, best fitness score, avg fitness score, hyper-params\n",
    "        \n",
    "        rand = np.random.random_sample(shape)\n",
    "        \n",
    "        t0 = time.time()\n",
    "        offspr = get_offspring(pop, rand, lam, cr)\n",
    "        t1 = time.time()\n",
    "        PROFILER['offspring'] += t1 - t0\n",
    "        \n",
    "        #: option since papers unclear if mutate at offspring or survivors stage\n",
    "        if not mutate_after:\n",
    "            mutate(offspr, rand)\n",
    "            \n",
    "        t0 = time.time()\n",
    "        next_generation(pop, offspr, func)\n",
    "        t1 = time.time()\n",
    "        PROFILER['generation'] += t1 - t0\n",
    "        \n",
    "        if mutate_after:\n",
    "            t0 = time.time()\n",
    "            mutate(pop, rand)\n",
    "            t1 = time.time()\n",
    "            PROFILER['mutate'] += t1 - t0\n",
    "    \n",
    "    return pop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14.08823299407959\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "\n",
    "scaling_factor = 0.9\n",
    "crossover_rate = 0.5\n",
    "\n",
    "PROFILER = collections.Counter()\n",
    "\n",
    "@numba.njit\n",
    "def fitness(chromosome):\n",
    "    return cohesion_separation2(chromosome, jaccard_sim, doc)\n",
    "\n",
    "t0 = time.time()\n",
    "pop = run_iterations(pop_size=100, summ_len=5, num_sents=len(doc), \n",
    "                     func=fitness, lam=0.9, cr=0.5, iterations=200, verbose=False, seed=0)\n",
    "t1 = time.time()\n",
    "print(t1-t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'offspring': 0.053069114685058594,\n",
       "         'generation': 13.132251739501953,\n",
       "         'mutate': 0.8798696994781494})"
      ]
     },
     "execution_count": 231,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PROFILER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "def best_chromosome(population):\n",
    "    #TODO: make sure it picks one with all k-clusters\n",
    "    fits = np.argmax([fitness(chrom) for chrom in population])\n",
    "    chrom = population[fits]\n",
    "    return chrom\n",
    "    \n",
    "\n",
    "def central_sentences(chromosome, document, metric=cosine_distances):\n",
    "    central_sents = []\n",
    "    for cluster in np.unique(chromosome):\n",
    "        idxs = np.where(chromosome == cluster)[0]\n",
    "        sents = document[idxs]\n",
    "        centroid = sents.mean(axis=0)[np.newaxis,:]\n",
    "        dists = metric(sents, centroid)\n",
    "        cent_sent = idxs[np.argmin(dists)]\n",
    "        central_sents.append(cent_sent)\n",
    "    return central_sents\n",
    "\n",
    "\n",
    "chrom_best = best_chromosome(pop)\n",
    "idxs = central_sentences(chrom_best, doc)\n",
    "summ_evol = '\\n'.join(np.array(tokenize.sent_tokenize(text))[idxs])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\n",
    "\\large\n",
    "X_r(t+1) = \n",
    "\\begin{cases}\n",
    "    Y_r ( t + 1 ) & \\text{if } f \\big( Y_r ( t + 1 ) \\big) > f \\big( X_r ( t ) \\big) \\\\\n",
    "    X_r ( t ) & \\text{otherwise}\n",
    "\\end{cases}\n",
    "$\n",
    "\n",
    "where \n",
    "- $ f ( \\cdot ) $ is the objective function to be maximized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mutation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At each iteration $ t + 1 $ for each $ X_r ( t ) $ creates\n",
    "$ m_r ( t + 1 ) = [ m_{ r, 1 } ( t ), m_{ r, 2 } ( t ), ..., m_{ r, n } ( t ) ] $.  \n",
    "For each gene, 1 indicates no mutation and 0 means mutate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\n",
    "\\large\n",
    "m_{ r, s } ( t + 1 ) = \n",
    "\\begin{cases}\n",
    "    1 & \\text{if } rand_s < sigm \\big( y_{ r, s } ( t + 1 ) \\big) \\\\\n",
    "    0 & \\text{otherwise}\n",
    "\\end{cases}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inversion Operator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Figure 1 - Psuedo-code\n",
    "<img src=\"./data/pngs/fig1_-_inversion_operator_psuedo_code.png\" alt=\"Fig 1. Inverse operator psuedo-code\" width=\"33%\" align=\"left\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Figure 2 - Example\n",
    "<img src=\"./data/pngs/fig2_-_inversion_operator_diagram.png\" alt=\"Fig 2. Inverse operator example\" width=\"33%\" align=\"left\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ROUGE-N\n",
    "_(Recall Oriented Understudy for Gisting Evaluation)_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\n",
    "\\large\n",
    "\\text{ROUGE-N} = \n",
    "\\dfrac\n",
    "    { \\sum \\limits\n",
    "        _{ \\small S \\in Summ_{ ref }} \n",
    "        \\sum \\limits\n",
    "        _{ \\small \\text{N-gram} \\in S } \n",
    "            Count_{ \\small match } ( \\text{N-gram} ) }\n",
    "    { \\sum \\limits\n",
    "        _{ \\small S \\in Summ_{ ref } } \n",
    "        \\sum \\limits\n",
    "            _{ \\small \\text{N-gram} \\in S } \n",
    "            Count( \\text{N-gram} ) }\n",
    "$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rouge_n(n, y_pred, y_true):\n",
    "    n_gram_pred = set(ngrams(y_pred, n))\n",
    "    n_gram_true = set(ngrams(y_true, n))\n",
    "    return len(n_gram_pred & n_gram_true) / len(n_gram_true)\n",
    "\n",
    "\n",
    "rouge_n = functools.partial(rouge_n, y_true=summ_true)\n",
    "\n",
    "\n",
    "def scores(n, y_pred):\n",
    "    for i in range(1, n+1):\n",
    "        score = rouge_n(i, y_pred)\n",
    "        print(i, score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evolutionary\n",
      "1 0.9230769230769231\n",
      "2 0.8529411764705882\n",
      "3 0.5827338129496403\n",
      "4 0.46938775510204084\n",
      "5 0.3959731543624161\n"
     ]
    }
   ],
   "source": [
    "print('evolutionary')\n",
    "scores(5, summ_evol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gensim\n",
      "1 0.9615384615384616\n",
      "2 0.9411764705882353\n",
      "3 0.7410071942446043\n",
      "4 0.6258503401360545\n",
      "5 0.5100671140939598\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "\n",
    "summ_gensim = gensim.summarization.summarize(text)\n",
    "print('gensim')\n",
    "scores(5, summ_gensim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sumy - text rank\n",
      "1 0.9615384615384616\n",
      "2 0.8529411764705882\n",
      "3 0.6330935251798561\n",
      "4 0.5034013605442177\n",
      "5 0.4228187919463087\n",
      "\n",
      "sumy - lex rank\n",
      "1 0.9230769230769231\n",
      "2 0.7254901960784313\n",
      "3 0.48201438848920863\n",
      "4 0.35374149659863946\n",
      "5 0.26174496644295303\n"
     ]
    }
   ],
   "source": [
    "from sumy.parsers.plaintext import PlaintextParser\n",
    "from sumy.summarizers.lex_rank import LexRankSummarizer\n",
    "from sumy.summarizers.text_rank import TextRankSummarizer\n",
    "from sumy.nlp.tokenizers import Tokenizer\n",
    "\n",
    "\n",
    "def format_sumy(sumy_summary):\n",
    "    return ''.join(char for sent in sumy_summary for char in str(sent) if char in string.printable)\n",
    "\n",
    "parser = PlaintextParser(text, Tokenizer('english'))\n",
    "\n",
    "text_rank = TextRankSummarizer()\n",
    "summ_text_rank = format_sumy(text_rank(parser.document, 5))\n",
    "print('sumy - text rank')\n",
    "scores(5, summ_text_rank)\n",
    "\n",
    "print()\n",
    "\n",
    "lex_rank = LexRankSummarizer()\n",
    "summ_lex_rank = format_sumy(lex_rank(parser.document, 2))\n",
    "print('sumy - lex rank')\n",
    "scores(5, summ_lex_rank)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
