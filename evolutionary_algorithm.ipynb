{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import functools\n",
    "import math\n",
    "import operator\n",
    "import string\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from nltk import tokenize\n",
    "from nltk.util import ngrams\n",
    "\n",
    "from scipy.special import expit as sigmoid\n",
    "from scipy.stats import wasserstein_distance as earth_movers_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import contextlib\n",
    "import io\n",
    "\n",
    "with io.StringIO() as str_io, contextlib.redirect_stdout(str_io):\n",
    "    import this\n",
    "    zen = str_io.getvalue()\n",
    "\n",
    "del str_io, this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distinct_words(text):\n",
    "    no_punctuation = ''.join(t for t in text if t not in string.punctuation)\n",
    "    return frozenset(tokenize.word_tokenize(no_punctuation))\n",
    "\n",
    "text = zen.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evolutionary Algorithms\n",
    "- [Evolutionary Algorithm for Extractive Text Summarization](https://www.researchgate.net/profile/Ramiz_Aliguliyev/publication/220518077_Evolutionary_Algorithm_for_Extractive_Text_Summarization/links/09e4151356fc2caab6000000.pdf)\n",
    "- [An Improved Evolutionary Algorithm for Extractive Text Summarization](https://link.springer.com/chapter/10.1007/978-3-642-36543-0_9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentence Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let document $D$ be decomposed into a set of $n$ sentences.  \n",
    "$\n",
    "D = \\{S_1, S_2, ..., S_n\\}\n",
    "$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D = document_sentences = set(tokenize.sent_tokenize(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let terms $T$ be the set of all $m$ distinct words in D.  \n",
    "$\n",
    "T = \\{t_1, t_2, ..., t_m\\}\n",
    "$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "T = document_distinct_words = distinct_words(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let $S_i$ represent the set of distinct terms in sentence $S_i$ with \n",
    "$m_i$ distinct terms.  \n",
    "$\n",
    "S_i = \\{t_1, t_2, ..., t_{m_i}\\}\n",
    "$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S = sentence_distinct_words = {distinct_words(ds) for ds in document_sentences}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Similarity Measures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Jaccard Coefficient Similarity Measure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\n",
    "\\large\n",
    "sim_{jaccard}(S_i, S_j) = \\frac{|S_i \\cap S_j|}{|S_i \\cup S_j|}\n",
    "$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jaccard_similarity(a, b):\n",
    "    a, b = set(a), set(b)\n",
    "    if not a and not b:\n",
    "        return 1.0\n",
    "    return len(a & b) / len(a | b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalized Google Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\n",
    "\\large\n",
    "NGD(t_k, t_l) = \\frac{max\\{log(f_k), log(f_l)\\} - log(f_{lk})} {log(n) - min\\{log(f_k), log(f_l)\\}}\n",
    "$  \n",
    "\n",
    "where:\n",
    "- $t_k$ and $t_l$ are terms \n",
    "- $f_k$ is the number of sentences containing $t_k$\n",
    "- $f_{kl}$ is the number of sentences containing both $t_k$ and $t_l$\n",
    "- $n$ is the total number of sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# double check scientific paper's handling of \"bad\" log values\n",
    "def norm_google_distance(t_k, t_l, D, S):\n",
    "    \"\"\"Metric for distance between two terms-- tₖ, tₗ\"\"\"\n",
    "    \n",
    "    f_k = sum(t_k in sent for sent in S)\n",
    "    f_l = sum(t_l in sent for sent in S)\n",
    "    if not (f_k and f_l):\n",
    "        raise ValueError('terms must be in document')\n",
    "    \n",
    "    f_kl = sum((t_k in sent) and (t_l in sent) for sent in S)\n",
    "    if (f_k > 0) and (f_l > 0) and (f_kl == 0):\n",
    "        return 1.0\n",
    "    \n",
    "    log_kl = (math.log(f_k), math.log(f_l))\n",
    "    n = len(D)\n",
    "    \n",
    "    numerator = max(log_kl) - math.log(f_kl)\n",
    "    denominator = math.log(n) - min(log_kl)\n",
    "    return numerator / denominator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\n",
    "\\large \n",
    "sim_{NGD}(t_k, t_l) = e^{-NGD(t_k, t_l)}\n",
    "$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#: (1)\n",
    "def norm_google_similarity_term(t_k, t_l, D, S):\n",
    "    \"\"\"Metric for similarity between two terms-- tₖ, tₗ\"\"\"\n",
    "    \n",
    "    ngd = normalized_google_distance(t_k, t_l, D, S)\n",
    "    return math.exp(-ngd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\n",
    "\\large \n",
    "sim_{NGD}(S_i, S_j) = \\frac{ \\sum\\limits_{t_k \\in S_i} \\sum\\limits_{t_l \\in S_j} sim_{NGD}(t_k, t_l) }{ m_i m_j }\n",
    "$  \n",
    "\n",
    "where:\n",
    "- $S_i$ and $S_j$ are sentences\n",
    "- $m_i$ is the number of words in $S_i$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "#: (3)\n",
    "def norm_google_similarity_sent(S_i, S_j, D, S):\n",
    "    total = sum(sum(norm_google_similarity_term(t_k, t_l, D, S) for t_l in S_j) for t_k in S_i)\n",
    "    return total / len(S_i) / len(S_j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if tₖ == tₗ --> 1\n",
    "assert norm_google_similarity_term('python', 'python', D, S) == 1\n",
    "\n",
    "# if (tₖ != tₗ) and (fₖ == fₗ == fₖₗ > 0) --> 1\n",
    "assert norm_google_similarity_term('explicit', 'implicit', D, S) == 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objective Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let $C$ be a partition of D with $k$ clusters.  \n",
    "$\n",
    "C = \\{C_1, C_2, ..., C_k\\}\n",
    "$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C = k_clusters = {frozenset(), ...}\n",
    "#: 1) Two different clusters should have no sentences in common\n",
    "# assert all(not C_i & C_j for C_i, C_j in itertools.combinations(C, C))\n",
    "\n",
    "#: 2) Each sentence should definitely be attached to a cluster\n",
    "# assert functools.reduce(operator.or_, C) == D\n",
    "\n",
    "#: 3) Each cluster should have at least one sentence assigned\n",
    "# assert all(C_p for C_p in C)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sigmoid Function\n",
    "$\n",
    "\\large\n",
    "sigm(x) = \\frac{ 1 }{ 1 + e^{-x} }\n",
    "$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.special import expit as sigmoid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Criterion Function\n",
    "$\n",
    "\\large\n",
    "F = (1 + sigm(F_1))^{F_2} \\rightarrow max\n",
    "$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def criterion_function(C):\n",
    "    return pow(1 + sigmoid(F_1(C)), F_2(C))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Intra-Cluster Similiarity\n",
    "$\n",
    "\\large\n",
    "F_1 = \\sum\\limits_{p=1}^{k} |C_p| \\sum\\limits_{S_i, S_j \\in C_p} sim(S_i, S_j) \\rightarrow max\n",
    "$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def intra_cluster_imilarity(C):\n",
    "    outer = 0\n",
    "    for C_p in C:\n",
    "        inner = 0\n",
    "        for S_i, S_j in itertools.combinations(C_p):\n",
    "            sim_ngd = norm_google_similarity_sent(S_i, S_i, D, S)           # D, S args\n",
    "            inner += sim_ngd\n",
    "        outer += len(C_p) * inner\n",
    "    return outer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inter-Cluster Dissimilarity\n",
    "$\n",
    "\\large\n",
    "F_2 = \\sum\\limits_{p=1}^{k-1} \\frac{1}{|C_p|} \\sum\\limits_{q=p+1}^{k} \\sum\\limits_{S_i \\in C_p} \\sum\\limits_{S_l \\in C_q} sim(S_i, S_l) \\rightarrow min\n",
    "$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inter_cluster_dissimilarity(C):\n",
    "    C = tuple(C)    # may need to use 'ordered sets'; unclear how to handle p = 1 to k-1\n",
    "    k = len(C)\n",
    "    \n",
    "    sum_0 = 0\n",
    "    for p in range(k - 1):\n",
    "        sum_1 = 0\n",
    "        for q in range(p + 1, k):\n",
    "            sum_2 = 0\n",
    "            for S_i in C[p]:\n",
    "                sum_3 = 0\n",
    "                for S_j in C[q]:\n",
    "                    sum_3 += sum(norm_google_similarity_sent(S_i, S_j))\n",
    "                sum_2 += sum_3\n",
    "            sum_1 += sum_2 / len(C[q])\n",
    "        sum_0 += sum_1 / len(C[p])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = None  # need reproducibility option\n",
    "\n",
    "N = len(population)\n",
    "n = len(sentences)\n",
    "k = len(clusters)\n",
    "\n",
    "r = range(N)\n",
    "s = range(n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modified Discrete Differential Evolution Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize the population with $N$ chromosomes each composed of $n$ random integers from \\[1, k\\]. __NOTE:__ $t$ is the iteration step.\n",
    "\n",
    "$\n",
    "X_r(t) = [x_{r,1}(t), x_{r,2}(t), ... x_{r,n}(t)]\n",
    "$  \n",
    "\n",
    "where:\n",
    "- $ x_{r,s}(t) \\in \\{1, 2, ..., k\\} $\n",
    "- $ r = 1, 2, ..., N $\n",
    "- $ s = 1, 2, ..., n $\n",
    "- $N$ is the population size\n",
    "- $k$ is the number of clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def X_r(t):\n",
    "    return [x_r_1(t), x_r_2(t),..., x_r_n(t)]\n",
    "\n",
    "def x_r_s(t):\n",
    "    return integer in range(k)\n",
    "\n",
    "def Y_r(t):\n",
    "    return [y_r_1(t), y_r_2(t),..., y_r_n(t)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\n",
    "\\large\n",
    "y_{r, s}(t+1) = \n",
    "\\begin{cases}\n",
    "    x_{r1,s}(t) + \\lambda[x_{r2,s}(t) - x_{r3,s}(t)] & \\text{if}\\ rand_s < CR \\\\\n",
    "    x_{r,s}(t) & \\text{otherwise}\n",
    "\\end{cases}\n",
    "$\n",
    "\n",
    "where  \n",
    "- For each $X_r$, randomly sample $X_{r1}(t), X_{r2}(t), X_{r3}(t)$ from the same generation (each distinct)\n",
    "- $rand_s$ is uniformally distributed random numbers from \\[0, 1\\] chosen once for each $s \\in \\{1, 2, ..., n\\}$\n",
    "\n",
    "hyper-parameters \n",
    "- $\\lambda$ is a scale factor from \\[0, 1\\]\n",
    "- $CR$ is the crossover rate from \\[0, 1\\]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def y_r_s(t, lam, crossover_rate):\n",
    "    x_r1_s = random.choice(...)\n",
    "    x_r2_s = random.choice(...)\n",
    "    x_r3_s = random.choice(...)\n",
    "    \n",
    "    rand_s = random.random(0, 1)\n",
    "    if rand_s < crossover_rate:\n",
    "        return x_r1_s(t-1) + lam * (x_r2_s(t-1) - x_r3_s(t-1))\n",
    "    else:\n",
    "        return x_r_s(t-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\n",
    "\\large\n",
    "X_r(t+1) = \n",
    "\\begin{cases}\n",
    "    Y_r(t+1) & \\text{if}\\ f(Y_r(t+1)) > f(X_r(t)) \\\\\n",
    "    x_{r}(t) & \\text{otherwise}\n",
    "\\end{cases}\n",
    "$\n",
    "\n",
    "where \n",
    "- $f(\\cdot)$ is the objective function to be maximized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def X_r(t, f):\n",
    "    y = y_r(t-1)\n",
    "    x = x_r(t-1)\n",
    "    if f(y) > f(x):\n",
    "        return y\n",
    "    else:\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitness\n",
    "<img src=\"./data/pngs/fitness.png\" alt=\"inverse operator psuedo-code\" width=\"33%\" align=\"left\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: rename fitness funcs to their corresponding objective function\n",
    "\n",
    "#: (9)\n",
    "def fitness_1(X_a):\n",
    "    return F_1(X_a)\n",
    "\n",
    "#: (10)\n",
    "def fitness_2(X_a):\n",
    "    return 1 / F_2(X_a)\n",
    "\n",
    "#: (11)\n",
    "def fitness(X_a):\n",
    "    return F(X_a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mutation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\n",
    "\\large\n",
    "m_{r,s}(t+1) = \n",
    "\\begin{cases}\n",
    "    1 & \\text{if}\\ rand_s < sigm(y_{r,s}(t+1)) \\\\\n",
    "    0 & \\text{otherwise}\n",
    "\\end{cases}\n",
    "$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "def m_r_s(t):\n",
    "    # do I need same rand_s from (7)?\n",
    "    rand_s = random.uniform(0, 1)\n",
    "    sigm = sigmoid(y_r_s(t))\n",
    "    return rand_s < sigm\n",
    "\n",
    "# need mutation_rate parameter (MR in paper)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inversion Operator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Figure 1 - Psuedo-code\n",
    "<img src=\"./data/pngs/fig1_-_inversion_operator_psuedo_code.png\" alt=\"inverse operator psuedo-code\" width=\"33%\" align=\"left\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inversion_operator(X_r, m_r_p1):\n",
    "    X_r_p1 = X_r.copy()\n",
    "    S = set(i for i, mutate in enumerate(m_r_p1) if not mutate)\n",
    "    \n",
    "    while len(S):\n",
    "        s_min, s_max = min(S), max(S)\n",
    "        X_r_p1[s_min] = X_r[s_max]\n",
    "        X_r_p1[s_max] = X_r[s_min]\n",
    "        S -= {s_max, s_min}\n",
    "    \n",
    "    return X_r_p1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Figure 2 - Example\n",
    "<img src=\"./data/pngs/fig2_-_inversion_operator_diagram.png\" alt=\"inverse operator example\" width=\"33%\" align=\"left\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_r = [3, 2, 4, 2, 3, 1, 4, 1]\n",
    "m_r_p1 = [0, 1, 1, 0, 1, 0, 0, 1]\n",
    "X_r_p1 = [4, 2, 4, 1, 3, 2, 3, 1]\n",
    "\n",
    "assert inversion_operator(X_r, m_r_p1) == X_r_p1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ROUGE-N\n",
    "(Recall Oriented Understudy for Gisting Evaluation)  \n",
    "<img src=\"./data/pngs/rouge-n.png\" alt=\"inverse operator example\" width=\"33%\" align=\"left\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#: (14)\n",
    "def rouge_n(n, y_pred, y_true):\n",
    "    n_gram_pred = set(ngrams(y_pred, n))\n",
    "    n_gram_true = set(ngrams(y_true, n))\n",
    "    return len(n_gram_pred & n_gram_true) / len(n_gram_true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### testing rouge_n\n",
    "https://rare-technologies.com/text-summarization-in-python-extractive-vs-abstractive-techniques-revisited/#how_to_evaluate_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = 'a good diet must have apples and bananas'.split()\n",
    "y_pred = 'apples and bananas are must for a good diet'.split()\n",
    "\n",
    "assert rouge_n(1, y_pred, y_true) == 7 / 8\n",
    "assert rouge_n(2, y_pred, y_true) == 4 / 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocess Steps:  \n",
    " - stop words\n",
    " - lemmatize"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
